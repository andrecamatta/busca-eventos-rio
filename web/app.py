"""AplicaÃ§Ã£o web FastAPI para visualizaÃ§Ã£o de eventos em calendÃ¡rio."""

import asyncio
import json
import logging
import os
import re
import shutil
import threading
import time
from datetime import datetime
from pathlib import Path
from typing import Optional

from apscheduler.schedulers.background import BackgroundScheduler
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from starlette.requests import Request

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Paths
BASE_DIR = Path(__file__).parent.parent
OUTPUT_DIR = BASE_DIR / "output"
LATEST_OUTPUT = OUTPUT_DIR / "latest"
LOG_FILE = BASE_DIR / "busca_eventos.log"

# FastAPI app
app = FastAPI(
    title="Eventos Culturais Rio",
    description="CalendÃ¡rio de eventos culturais no Rio de Janeiro",
    version="1.0.0"
)

# Mount static files and templates
app.mount("/static", StaticFiles(directory=Path(__file__).parent / "static"), name="static")
templates = Jinja2Templates(directory=Path(__file__).parent / "templates")

# Scheduler para atualizaÃ§Ã£o automÃ¡tica
scheduler = BackgroundScheduler()

# Status do job de atualizaÃ§Ã£o (rastreamento global)
job_status = {
    "is_running": False,
    "last_started": None,
    "last_completed": None,
    "last_result": None,  # "success", "error", ou None
    "last_error": None,
    "last_duration_seconds": None,
    "last_stdout": None,
    "last_stderr": None,
}

# Status do job de julgamento (rastreamento global)
judge_status = {
    "is_running": False,
    "last_started": None,
    "last_completed": None,
    "last_result": None,  # "success", "error", ou None
    "last_error": None,
    "total_events": 0,
    "judged_count": 0,
    "current_batch": 0,
    "total_batches": 0,
    "average_score": 0.0,
}


def ensure_output_directory():
    """Garante que o diretÃ³rio base de output existe."""
    try:
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        # LATEST_OUTPUT serÃ¡ criado como symlink por EventFileManager
        logger.info(f"âœ“ DiretÃ³rio de output verificado: {OUTPUT_DIR}")
    except Exception as e:
        logger.warning(f"NÃ£o foi possÃ­vel criar diretÃ³rio de output: {e}")


def load_latest_events() -> list[dict]:
    """Carrega os eventos mais recentes do output/latest."""
    try:
        # Garantir que diretÃ³rio existe
        if not LATEST_OUTPUT.exists():
            logger.info(f"ğŸ“‚ DiretÃ³rio {LATEST_OUTPUT} nÃ£o existe. Criando...")
            ensure_output_directory()
            logger.info("â„¹ï¸  Nenhum evento carregado ainda. Execute a busca ou use /api/refresh")
            return []

        # Tentar vÃ¡rios arquivos possÃ­veis (em ordem de prioridade)
        possible_files = [
            LATEST_OUTPUT / "judged_events.json",              # PRIORIDADE 1: Eventos com notas de qualidade (GPT-5)
            LATEST_OUTPUT / "formatted_output.json",           # PRIORIDADE 2: Eventos formatados para WhatsApp
            LATEST_OUTPUT / "verified_events.json",            # PRIORIDADE 3: Eventos verificados
            LATEST_OUTPUT / "enriched_events_initial.json",    # PRIORIDADE 4: Fallback (eventos enriquecidos)
        ]

        eventos = []
        for file_path in possible_files:
            if file_path.exists():
                logger.info(f"ğŸ“ Carregando eventos de: {file_path.name}")
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

                # Extrair eventos (pode ser dict ou list)
                if isinstance(data, list):
                    eventos = data
                elif isinstance(data, dict):
                    # Tentar vÃ¡rias chaves possÃ­veis
                    eventos = (
                        data.get("eventos") or
                        data.get("verified_events") or
                        data.get("enriched_events") or
                        []
                    )

                logger.info(f"âœ“ Carregados {len(eventos)} eventos de {file_path.name}")
                return eventos

        logger.info(f"â„¹ï¸  Nenhum arquivo de eventos encontrado em {LATEST_OUTPUT}")
        logger.info(f"ğŸ’¡ Execute 'python main.py' ou use /api/refresh para buscar eventos")
        return []

    except Exception as e:
        logger.error(f"âŒ Erro ao carregar eventos: {e}")
        return []


def extract_venue_from_local(local: str) -> str:
    """
    Extrai o nome do venue do campo 'local'.

    Formato esperado: "Nome do Venue - EndereÃ§o completo"
    Retorna: "Nome do Venue" ou string vazia se nÃ£o conseguir extrair
    """
    if not local:
        return ""

    # Se contÃ©m " - ", extrair a parte antes do primeiro hÃ­fen
    if " - " in local:
        venue_name = local.split(" - ")[0].strip()
        return venue_name

    # Se nÃ£o tem hÃ­fen, retornar vazio (local genÃ©rico)
    return ""


def parse_event_to_fullcalendar(event: dict) -> dict:
    """Converte evento do formato interno para FullCalendar."""
    try:
        # Parsear data e horÃ¡rio
        data_str = event.get("data", "")
        horario_str = event.get("horario", "00:00")

        # Formato: DD/MM/YYYY
        data_parts = data_str.split("/")
        if len(data_parts) != 3:
            return None

        day, month, year = data_parts
        hora, minuto = horario_str.split(":")

        # ISO format para FullCalendar
        start_datetime = f"{year}-{month}-{day}T{hora}:{minuto}:00"

        # Determinar cor baseado na categoria
        categoria = event.get("categoria", "Geral")

        # Extrair venue do campo 'local' se nÃ£o existir campo 'venue' direto
        venue = event.get("venue", "")
        if not venue:
            local = event.get("local", "")
            venue = extract_venue_from_local(local)

        # Cores por categoria (granular)
        color_map = {
            "Jazz": "#3498db",  # Azul
            "MÃºsica ClÃ¡ssica": "#9b59b6",  # Roxo
            "Teatro": "#e67e22",  # Laranja
            "ComÃ©dia": "#e74c3c",  # Vermelho
            "Cinema": "#34495e",  # Cinza escuro
            "Feira GastronÃ´mica": "#f39c12",  # Amarelo/ouro
            "Feira de Artesanato": "#16a085",  # Verde-azulado
            "Outdoor/Parques": "#2ecc71",  # Verde
            "Cursos de CafÃ©": "#795548",  # Marrom
            "Geral": "#95a5a6",  # Cinza claro
        }

        # Prioridade: CATEGORIA primeiro, venue como fallback
        if categoria and categoria in color_map:
            color = color_map[categoria]  # Usar cor da categoria
        elif venue:
            color = "#7f8c8d"  # Cinza mÃ©dio para venues sem categoria
        else:
            color = "#95a5a6"  # Cinza claro default

        # Validar link antes de incluir (nÃ£o mostrar links 404 ou invÃ¡lidos)
        link_ingresso = event.get("link_ingresso")
        link_valid = event.get("link_valid")
        link_status_code = event.get("link_status_code")

        # SÃ³ incluir link se for vÃ¡lido (nÃ£o Ã© False e nÃ£o Ã© 404)
        if link_valid is False or link_status_code == 404:
            link_ingresso = None

        # Badge visual para eventos contÃ­nuos (exposiÃ§Ãµes/temporadas)
        title = event.get("titulo", "Sem tÃ­tulo")
        is_temporada = event.get("is_temporada", False)
        tipo_temporada = event.get("tipo_temporada")

        if is_temporada and tipo_temporada:
            # Adicionar Ã­cone de calendÃ¡rio ao tÃ­tulo
            title = f"ğŸ“… {title}"

        return {
            "id": hash(event.get("titulo", "") + data_str + horario_str),
            "title": title,
            "start": start_datetime,
            "end": start_datetime,  # Eventos pontuais
            "extendedProps": {
                "local": event.get("local", ""),
                "preco": event.get("preco", "Consultar"),
                "link_ingresso": link_ingresso,  # None se invÃ¡lido
                "link_type": event.get("link_type", "info") if link_ingresso else None,  # purchase, info, ou venue
                "descricao": event.get("descricao", ""),
                "categoria": categoria,
                "venue": venue,
                "is_temporada": is_temporada,
                "tipo_temporada": tipo_temporada,
            },
            "color": color,
            "textColor": "#ffffff",
        }

    except Exception as e:
        logger.error(f"Erro ao parsear evento: {e} - {event}")
        return None


def parse_log_line(line: str) -> Optional[dict]:
    """
    Parseia uma linha de log no formato:
    2025-11-08 15:36:54,176 - module.name - LEVEL - message

    Returns:
        Dict com timestamp, module, level, message ou None se nÃ£o conseguir parsear
    """
    # Regex para parsear formato de log
    # Formato: YYYY-MM-DD HH:MM:SS,mmm - module - LEVEL - message
    pattern = r'^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) - ([^ ]+) - (\w+) - (.+)$'
    match = re.match(pattern, line.strip())

    if match:
        return {
            "timestamp": match.group(1),
            "module": match.group(2),
            "level": match.group(3),
            "message": match.group(4)
        }

    # Se nÃ£o conseguir parsear, retornar linha raw
    return {
        "timestamp": "",
        "module": "",
        "level": "RAW",
        "message": line.strip()
    }


def run_event_search():
    """Executa a busca de eventos (main.py) com logging detalhado e rastreamento de status."""
    import subprocess
    import shutil
    import time
    import traceback

    # Marcar inÃ­cio da execuÃ§Ã£o
    start_time = time.time()
    job_status["is_running"] = True
    job_status["last_started"] = datetime.now().isoformat()
    job_status["last_result"] = None
    job_status["last_error"] = None

    logger.info("=" * 80)
    logger.info("ğŸš€ INICIANDO BUSCA DE EVENTOS (MANUAL/SCHEDULED)")
    logger.info(f"ğŸ“… HorÃ¡rio de inÃ­cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 80)

    try:
        # Verificar se API key estÃ¡ configurada
        if not os.getenv("OPENROUTER_API_KEY"):
            error_msg = "OPENROUTER_API_KEY nÃ£o configurada. Busca cancelada."
            logger.error(f"âŒ {error_msg}")
            job_status["last_result"] = "error"
            job_status["last_error"] = error_msg
            return

        # Determinar comando para executar main.py
        # No container Docker, venv estÃ¡ no PATH via ENV VIRTUAL_ENV=/app/.venv
        # Usar 'python' diretamente aproveita o venv ativo
        venv_python = BASE_DIR / ".venv" / "bin" / "python"

        if venv_python.exists():
            # Venv existe (container Docker ou dev local) - usar diretamente
            cmd = ["python", "main.py"]
            logger.info(f"âœ“ Comando selecionado: python main.py (venv em {venv_python})")
        elif shutil.which("uv"):
            # Ambiente dev com uv mas sem venv
            cmd = ["uv", "run", "python", "main.py"]
            logger.info(f"âœ“ Comando selecionado: uv run python main.py")
        else:
            # Fallback: python do sistema
            logger.warning("âš ï¸  Nem venv nem uv encontrados. Usando python do sistema...")
            cmd = ["python3", "main.py"]
            logger.info(f"âœ“ Comando selecionado: python3 main.py")

        logger.info(f"ğŸ“‚ DiretÃ³rio de execuÃ§Ã£o: {BASE_DIR}")
        logger.info(f"â±ï¸  Timeout configurado: 600s (10 minutos)")
        logger.info("ğŸ”„ Executando subprocess...")

        # Executar comando
        result = subprocess.run(
            cmd,
            cwd=BASE_DIR,
            capture_output=True,
            text=True,
            timeout=600  # 10 minutos
        )

        duration = time.time() - start_time
        job_status["last_duration_seconds"] = round(duration, 2)

        logger.info("-" * 80)
        logger.info(f"â±ï¸  DuraÃ§Ã£o total: {duration:.2f}s")
        logger.info(f"ğŸ“Š Return code: {result.returncode}")

        if result.returncode == 0:
            logger.info("=" * 80)
            logger.info("âœ… BUSCA DE EVENTOS CONCLUÃDA COM SUCESSO!")
            logger.info("=" * 80)

            # Logar stdout completo (primeiros 5000 chars)
            if result.stdout:
                logger.info("ğŸ“ STDOUT (primeiros 5000 chars):")
                logger.info(result.stdout[:5000])

            job_status["last_result"] = "success"
            job_status["last_error"] = None
            job_status["last_stdout"] = result.stdout[:5000] if result.stdout else None
            job_status["last_stderr"] = result.stderr[:5000] if result.stderr else None
        else:
            error_msg = f"Subprocess retornou cÃ³digo {result.returncode}"
            logger.error("=" * 80)
            logger.error(f"âŒ ERRO NA BUSCA DE EVENTOS")
            logger.error("=" * 80)
            logger.error(f"Return code: {result.returncode}")

            # Logar stderr completo (primeiros 5000 chars)
            if result.stderr:
                logger.error("ğŸ“ STDERR (primeiros 5000 chars):")
                logger.error(result.stderr[:5000])

            # Logar stdout tambÃ©m (pode conter mensagens Ãºteis)
            if result.stdout:
                logger.error("ğŸ“ STDOUT (primeiros 5000 chars):")
                logger.error(result.stdout[:5000])

            job_status["last_result"] = "error"
            job_status["last_error"] = error_msg
            job_status["last_stdout"] = result.stdout[:5000] if result.stdout else None
            job_status["last_stderr"] = result.stderr[:5000] if result.stderr else None

    except subprocess.TimeoutExpired as e:
        duration = time.time() - start_time
        job_status["last_duration_seconds"] = round(duration, 2)

        error_msg = f"Timeout de 10 minutos excedido (executou por {duration:.2f}s)"
        logger.error("=" * 80)
        logger.error("âŒ TIMEOUT NA BUSCA DE EVENTOS")
        logger.error("=" * 80)
        logger.error(error_msg)

        # Tentar capturar output parcial
        if hasattr(e, 'stdout') and e.stdout:
            logger.error("ğŸ“ STDOUT parcial (primeiros 5000 chars):")
            logger.error(e.stdout[:5000])
        if hasattr(e, 'stderr') and e.stderr:
            logger.error("ğŸ“ STDERR parcial (primeiros 5000 chars):")
            logger.error(e.stderr[:5000])

        job_status["last_result"] = "error"
        job_status["last_error"] = error_msg

    except Exception as e:
        duration = time.time() - start_time
        job_status["last_duration_seconds"] = round(duration, 2)

        error_msg = f"{type(e).__name__}: {str(e)}"
        logger.error("=" * 80)
        logger.error("âŒ EXCEÃ‡ÃƒO NÃƒO TRATADA NA BUSCA DE EVENTOS")
        logger.error("=" * 80)
        logger.error(f"Tipo: {type(e).__name__}")
        logger.error(f"Mensagem: {str(e)}")
        logger.error("ğŸ“ Traceback completo:")
        logger.error(traceback.format_exc())

        job_status["last_result"] = "error"
        job_status["last_error"] = error_msg

    finally:
        # Marcar fim da execuÃ§Ã£o
        job_status["is_running"] = False
        job_status["last_completed"] = datetime.now().isoformat()

        logger.info("=" * 80)
        logger.info(f"ğŸ EXECUÃ‡ÃƒO FINALIZADA - Status: {job_status['last_result']}")
        logger.info(f"ğŸ“… HorÃ¡rio de tÃ©rmino: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 80)


@app.on_event("startup")
async def startup_event():
    """Inicializa o scheduler na startup."""
    logger.info("=" * 60)
    logger.info("ğŸš€ INICIANDO APLICAÃ‡ÃƒO EVENTOS CULTURAIS RIO")
    logger.info("=" * 60)

    # Log environment info
    port = os.getenv("PORT", "NOT SET")
    logger.info(f"ğŸ“Œ PORT configurado: {port}")
    logger.info(f"ğŸ“ BASE_DIR: {BASE_DIR}")
    logger.info(f"ğŸ“‚ OUTPUT_DIR: {OUTPUT_DIR}")

    # Garantir que diretÃ³rios existem
    logger.info("ğŸ“‚ Criando diretÃ³rios de output...")
    ensure_output_directory()
    logger.info("âœ“ DiretÃ³rios verificados")

    # Verificar se API key estÃ¡ configurada
    api_key = os.getenv("OPENROUTER_API_KEY")
    logger.info(f"ğŸ”‘ API Key configurada: {bool(api_key)}")

    if api_key:
        # Agendar busca diÃ¡ria Ã s 6h da manhÃ£
        logger.info("â° Configurando scheduler...")
        scheduler.add_job(
            run_event_search,
            trigger="cron",
            hour=6,
            minute=0,
            id="daily_event_search",
            replace_existing=True
        )
        scheduler.start()
        logger.info("âœ“ Scheduler iniciado - busca automÃ¡tica Ã s 6h")
    else:
        logger.warning("âš ï¸  OPENROUTER_API_KEY nÃ£o configurada - scheduler desabilitado")
        logger.info("ğŸ’¡ Configure a variÃ¡vel para habilitar atualizaÃ§Ã£o automÃ¡tica")

    logger.info("=" * 60)
    logger.info("âœ… APLICAÃ‡ÃƒO PRONTA PARA RECEBER REQUISIÃ‡Ã•ES")
    logger.info("=" * 60)


@app.on_event("shutdown")
async def shutdown_event():
    """Para o scheduler no shutdown."""
    scheduler.shutdown()
    logger.info("âœ“ Scheduler parado")


@app.get("/health")
async def health_check():
    """
    Health check endpoint RÃPIDO para Railway e monitoramento.

    Retorna apenas status bÃ¡sico sem operaÃ§Ãµes pesadas de I/O.
    """
    return JSONResponse(content={
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "app": "eventos-culturais-rio"
    })


@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    """PÃ¡gina principal com calendÃ¡rio."""
    return templates.TemplateResponse("index.html", {"request": request})


@app.get("/api/events")
async def get_events(
    categoria: Optional[str] = None,
    venue: Optional[str] = None
):
    """
    Retorna eventos em formato FullCalendar.

    Query params:
    - categoria: filtrar por categoria (Jazz, Teatro-ComÃ©dia, Outdoor-FimDeSemana)
    - venue: filtrar por venue especÃ­fico
    """
    eventos = load_latest_events()

    # Aplicar filtros
    if categoria:
        eventos = [e for e in eventos if e.get("categoria") == categoria]

    if venue:
        eventos = [e for e in eventos if e.get("venue") == venue]

    # FILTRO TEMPORAL: Eventos de hoje sÃ³ aparecem se faltam pelo menos 3 horas
    from datetime import datetime, timedelta
    now = datetime.now()
    hora_minima = now + timedelta(hours=3)

    eventos_filtrados = []
    for evento in eventos:
        data_str = evento.get("data", "")
        horario_str = evento.get("horario", "00:00")

        try:
            # Parse data (formato DD/MM/YYYY)
            event_date = datetime.strptime(data_str, "%d/%m/%Y").date()

            # Se nÃ£o Ã© hoje, verificar se nÃ£o Ã© data passada
            if event_date != now.date():
                # Rejeitar eventos passados
                if event_date < now.date():
                    continue  # Filtrar silenciosamente
                # Aceitar eventos futuros
                eventos_filtrados.append(evento)
                continue

            # Se Ã© hoje, verificar horÃ¡rio
            hora_partes = horario_str.split(":")
            if len(hora_partes) >= 2:
                hora = int(hora_partes[0])
                minuto = int(hora_partes[1])
                event_datetime = datetime.combine(event_date, datetime.min.time()).replace(hour=hora, minute=minuto)

                # SÃ³ adicionar se faltam pelo menos 3 horas
                if event_datetime >= hora_minima:
                    eventos_filtrados.append(evento)
                # SenÃ£o, evento muito prÃ³ximo/passado - filtrar silenciosamente
            else:
                # HorÃ¡rio invÃ¡lido - manter por seguranÃ§a (modo permissivo)
                eventos_filtrados.append(evento)

        except (ValueError, IndexError):
            # Erro de parsing - manter evento por seguranÃ§a (modo permissivo)
            eventos_filtrados.append(evento)

    eventos = eventos_filtrados

    # Converter para formato FullCalendar
    calendar_events = []
    for evento in eventos:
        parsed = parse_event_to_fullcalendar(evento)
        if parsed:
            calendar_events.append(parsed)

    return JSONResponse(content=calendar_events)


@app.get("/api/categories")
async def get_categories():
    """Retorna lista de categorias disponÃ­veis."""
    eventos = load_latest_events()
    categorias = set()

    for evento in eventos:
        cat = evento.get("categoria")
        if cat:
            categorias.add(cat)

    return JSONResponse(content=sorted(list(categorias)))


@app.get("/api/venues")
async def get_venues():
    """Retorna lista de venues disponÃ­veis."""
    eventos = load_latest_events()
    venues = set()

    for evento in eventos:
        venue = evento.get("venue")
        if venue:
            venues.add(venue)

    return JSONResponse(content=sorted(list(venues)))


@app.post("/api/refresh")
async def trigger_refresh():
    """ForÃ§a atualizaÃ§Ã£o manual dos eventos."""
    try:
        # Verificar se API key estÃ¡ configurada antes de aceitar a requisiÃ§Ã£o
        if not os.getenv("OPENROUTER_API_KEY"):
            logger.warning("âš ï¸  Tentativa de atualizaÃ§Ã£o sem API key configurada")
            raise HTTPException(
                status_code=503,
                detail="AtualizaÃ§Ã£o indisponÃ­vel: OPENROUTER_API_KEY nÃ£o configurada. Configure a variÃ¡vel de ambiente para habilitar buscas automÃ¡ticas."
            )

        # Executar busca em background
        logger.info("ğŸ“¨ RequisiÃ§Ã£o de atualizaÃ§Ã£o manual recebida")
        scheduler.add_job(
            run_event_search,
            id="manual_refresh",
            replace_existing=True
        )
        return JSONResponse(content={"status": "success", "message": "AtualizaÃ§Ã£o iniciada"})

    except HTTPException:
        # Re-raise HTTP exceptions (como o 503 acima)
        raise
    except Exception as e:
        logger.error(f"âŒ Erro ao agendar atualizaÃ§Ã£o: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/refresh/status")
async def get_refresh_status():
    """
    Retorna o status atual do job de atualizaÃ§Ã£o de eventos.

    Response:
        {
            "is_running": bool,
            "last_started": str | null,
            "last_completed": str | null,
            "last_result": "success" | "error" | null,
            "last_error": str | null,
            "last_duration_seconds": float | null
        }
    """
    return JSONResponse(content=job_status)


@app.get("/api/stats")
async def get_stats():
    """Retorna estatÃ­sticas dos eventos."""
    eventos = load_latest_events()

    # Contagens por categoria
    categorias = {}
    venues = {}

    for evento in eventos:
        cat = evento.get("categoria", "Geral")
        categorias[cat] = categorias.get(cat, 0) + 1

        venue = evento.get("venue")
        if venue:
            venues[venue] = venues.get(venue, 0) + 1

    return JSONResponse(content={
        "total_eventos": len(eventos),
        "por_categoria": categorias,
        "por_venue": venues,
        "ultima_atualizacao": datetime.now().isoformat()
    })


@app.get("/api/logs")
async def get_logs(
    lines: int = 100,
    level: Optional[str] = None,
    search: Optional[str] = None,
    reverse: bool = True
):
    """
    Retorna Ãºltimas linhas do log de execuÃ§Ã£o com filtros.

    Query params:
    - lines: nÃºmero de linhas a retornar (padrÃ£o: 100, mÃ¡x: 1000)
    - level: filtrar por nÃ­vel (INFO, ERROR, WARNING, DEBUG) - aceita mÃºltiplos separados por vÃ­rgula
    - search: buscar texto especÃ­fico (case-insensitive)
    - reverse: ordem cronolÃ³gica reversa (padrÃ£o: True = mais recentes primeiro)

    Examples:
        /api/logs?lines=50
        /api/logs?level=ERROR,WARNING
        /api/logs?search=blue%20note
        /api/logs?level=ERROR&search=timeout
    """
    try:
        # Validar parÃ¢metros
        lines = min(max(1, lines), 1000)  # Entre 1 e 1000

        # Verificar se arquivo existe
        if not LOG_FILE.exists():
            return JSONResponse(content={
                "logs": [],
                "total_lines": 0,
                "filtered_lines": 0,
                "file_size_mb": 0,
                "message": "Arquivo de log ainda nÃ£o existe. Execute uma busca primeiro."
            })

        # Obter tamanho do arquivo
        file_size_bytes = LOG_FILE.stat().st_size
        file_size_mb = round(file_size_bytes / (1024 * 1024), 2)

        # Ler arquivo (otimizado para arquivos grandes)
        with open(LOG_FILE, 'r', encoding='utf-8', errors='ignore') as f:
            all_lines = f.readlines()

        total_lines = len(all_lines)

        # Parsear nÃ­veis de filtro se fornecidos
        level_filters = None
        if level:
            level_filters = [l.strip().upper() for l in level.split(',')]

        # Processar linhas
        parsed_logs = []
        for line in all_lines:
            if not line.strip():
                continue

            log_entry = parse_log_line(line)
            if not log_entry:
                continue

            # Filtrar por nÃ­vel
            if level_filters and log_entry["level"] not in level_filters:
                continue

            # Filtrar por busca de texto
            if search:
                search_lower = search.lower()
                # Buscar em todos os campos
                searchable = f"{log_entry['timestamp']} {log_entry['module']} {log_entry['level']} {log_entry['message']}".lower()
                if search_lower not in searchable:
                    continue

            parsed_logs.append(log_entry)

        # Aplicar ordem reversa se solicitado
        if reverse:
            parsed_logs = parsed_logs[::-1]

        # Limitar nÃºmero de linhas
        filtered_count = len(parsed_logs)
        parsed_logs = parsed_logs[:lines]

        return JSONResponse(content={
            "logs": parsed_logs,
            "total_lines": total_lines,
            "filtered_lines": filtered_count,
            "returned_lines": len(parsed_logs),
            "file_size_mb": file_size_mb,
            "filters": {
                "level": level_filters,
                "search": search,
                "reverse": reverse
            }
        })

    except Exception as e:
        logger.error(f"âŒ Erro ao ler logs: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao ler logs: {str(e)}")


@app.get("/api/logs/download")
async def download_logs():
    """
    Download do arquivo completo de logs.

    Retorna o arquivo busca_eventos.log para download direto.
    Ãštil para anÃ¡lise offline ou arquivamento.
    """
    try:
        if not LOG_FILE.exists():
            raise HTTPException(
                status_code=404,
                detail="Arquivo de log nÃ£o encontrado. Execute uma busca primeiro."
            )

        # Retornar arquivo para download
        return FileResponse(
            path=LOG_FILE,
            media_type="text/plain",
            filename="busca_eventos.log",
            headers={
                "Content-Disposition": f"attachment; filename=busca_eventos_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Erro ao fazer download de logs: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao fazer download: {str(e)}")


@app.post("/api/logs/clear")
async def clear_logs():
    """
    Limpa o arquivo de log (rotaÃ§Ã£o).

    Cria backup do arquivo atual antes de limpar.
    SÃ³ permite limpeza se arquivo > 5MB.

    Returns:
        JSON com status da operaÃ§Ã£o e informaÃ§Ãµes do backup
    """
    try:
        if not LOG_FILE.exists():
            return JSONResponse(content={
                "status": "success",
                "message": "Arquivo de log nÃ£o existe, nada para limpar."
            })

        # Verificar tamanho do arquivo
        file_size_bytes = LOG_FILE.stat().st_size
        file_size_mb = round(file_size_bytes / (1024 * 1024), 2)

        # SÃ³ permitir limpeza se arquivo > 5MB
        MIN_SIZE_MB = 5
        if file_size_mb < MIN_SIZE_MB:
            return JSONResponse(content={
                "status": "skipped",
                "message": f"Arquivo muito pequeno ({file_size_mb}MB). SÃ³ Ã© possÃ­vel limpar arquivos > {MIN_SIZE_MB}MB.",
                "file_size_mb": file_size_mb
            })

        # Criar backup
        backup_dir = OUTPUT_DIR / "log_backups"
        backup_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = backup_dir / f"busca_eventos_{timestamp}.log"

        shutil.copy2(LOG_FILE, backup_file)
        logger.info(f"âœ“ Backup criado: {backup_file}")

        # Limpar arquivo original (mantÃ©m arquivo vazio)
        with open(LOG_FILE, 'w') as f:
            f.write(f"# Log rotacionado em {datetime.now().isoformat()}\n")
            f.write(f"# Backup salvo em: {backup_file}\n\n")

        logger.info(f"âœ“ Arquivo de log limpo. Tamanho anterior: {file_size_mb}MB")

        return JSONResponse(content={
            "status": "success",
            "message": f"Log rotacionado com sucesso. Backup criado.",
            "backup_file": str(backup_file),
            "previous_size_mb": file_size_mb,
            "timestamp": timestamp
        })

    except Exception as e:
        logger.error(f"âŒ Erro ao limpar logs: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao limpar logs: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# JULGAMENTO DE QUALIDADE DE EVENTOS (GPT-5)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_judge_quality():
    """Executa julgamento de qualidade dos eventos em background thread."""
    import sys
    sys.path.insert(0, str(BASE_DIR))

    from agents.judge_agent import QualityJudgeAgent

    start_time = time.time()
    judge_status["is_running"] = True
    judge_status["last_started"] = datetime.now().isoformat()
    judge_status["last_result"] = None
    judge_status["last_error"] = None

    logger.info("=" * 80)
    logger.info("âš–ï¸  INICIANDO JULGAMENTO DE QUALIDADE DE EVENTOS")
    logger.info(f"ğŸ“… HorÃ¡rio de inÃ­cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 80)

    try:
        # Verificar API key
        if not os.getenv("OPENROUTER_API_KEY"):
            error_msg = "OPENROUTER_API_KEY nÃ£o configurada. Julgamento cancelado."
            logger.error(f"âŒ {error_msg}")
            judge_status["last_result"] = "error"
            judge_status["last_error"] = error_msg
            judge_status["is_running"] = False
            return

        # Carregar eventos
        logger.info("ğŸ“‚ Carregando eventos para julgar...")
        eventos = load_latest_events()

        if not eventos:
            error_msg = "Nenhum evento encontrado para julgar"
            logger.warning(f"âš ï¸  {error_msg}")
            judge_status["last_result"] = "error"
            judge_status["last_error"] = error_msg
            judge_status["is_running"] = False
            return

        judge_status["total_events"] = len(eventos)
        logger.info(f"âœ“ {len(eventos)} eventos carregados")

        # Criar agent
        logger.info("ğŸ¤– Inicializando QualityJudgeAgent...")
        judge = QualityJudgeAgent()

        # Callback de progresso
        def progress_callback(batch_num, total_batches):
            judge_status["current_batch"] = batch_num
            judge_status["total_batches"] = total_batches
            judge_status["judged_count"] = min(batch_num * 5, len(eventos))
            logger.info(
                f"âš–ï¸  Progresso: batch {batch_num}/{total_batches} "
                f"({judge_status['judged_count']}/{len(eventos)} eventos)"
            )

        # Executar julgamento (assÃ­ncrono)
        logger.info("âš–ï¸  Iniciando julgamento...")

        # Criar event loop para executar cÃ³digo assÃ­ncrono
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            judged_events = loop.run_until_complete(
                judge.judge_all_events(eventos, progress_callback)
            )
        finally:
            loop.close()

        # Calcular estatÃ­sticas
        scores = [e.get("quality_score", 0) for e in judged_events]
        avg_score = sum(scores) / len(scores) if scores else 0
        judge_status["average_score"] = round(avg_score, 2)

        # Salvar eventos julgados
        judged_file = LATEST_OUTPUT / "judged_events.json"
        logger.info(f"ğŸ’¾ Salvando eventos julgados em: {judged_file}")

        with open(judged_file, "w", encoding="utf-8") as f:
            json.dump(judged_events, f, ensure_ascii=False, indent=2)

        duration = time.time() - start_time

        logger.info("=" * 80)
        logger.info("âœ… JULGAMENTO CONCLUÃDO COM SUCESSO!")
        logger.info(f"â±ï¸  DuraÃ§Ã£o: {duration:.2f}s")
        logger.info(f"ğŸ“Š Eventos julgados: {len(judged_events)}")
        logger.info(f"â­ Nota mÃ©dia: {avg_score:.2f}/10")
        logger.info("=" * 80)

        judge_status["last_result"] = "success"
        judge_status["last_completed"] = datetime.now().isoformat()
        judge_status["is_running"] = False

    except Exception as e:
        duration = time.time() - start_time
        error_msg = f"Erro no julgamento: {str(e)}"

        logger.error("=" * 80)
        logger.error("âŒ ERRO NO JULGAMENTO DE QUALIDADE")
        logger.error("=" * 80)
        logger.error(f"â±ï¸  DuraÃ§Ã£o atÃ© erro: {duration:.2f}s")
        logger.error(f"Erro: {e}")
        logger.exception(e)

        judge_status["last_result"] = "error"
        judge_status["last_error"] = error_msg
        judge_status["last_completed"] = datetime.now().isoformat()
        judge_status["is_running"] = False


@app.post("/api/judge")
async def start_judge():
    """
    Inicia julgamento de qualidade dos eventos em background.

    Returns:
        JSON com status de inÃ­cio do job
    """
    # Verificar se jÃ¡ estÃ¡ rodando
    if judge_status["is_running"]:
        return JSONResponse(
            status_code=409,
            content={
                "detail": "Julgamento jÃ¡ em andamento",
                "status": judge_status
            }
        )

    # Verificar API key
    if not os.getenv("OPENROUTER_API_KEY"):
        raise HTTPException(
            status_code=503,
            detail="OPENROUTER_API_KEY nÃ£o configurada"
        )

    # Iniciar job em background thread
    thread = threading.Thread(target=run_judge_quality, daemon=True)
    thread.start()

    return JSONResponse(
        status_code=202,
        content={
            "message": "Julgamento iniciado com sucesso",
            "status": "started",
            "started_at": judge_status["last_started"]
        }
    )


@app.get("/api/judge/status")
async def judge_status_endpoint():
    """
    Retorna status atual do julgamento.

    Returns:
        JSON com informaÃ§Ãµes de progresso
    """
    return JSONResponse(content=judge_status)


@app.get("/api/judge/results")
async def judge_results():
    """
    Retorna eventos julgados (com notas de qualidade).

    Returns:
        JSON com lista de eventos e suas avaliaÃ§Ãµes
    """
    try:
        judged_file = LATEST_OUTPUT / "judged_events.json"

        if not judged_file.exists():
            return JSONResponse(
                status_code=404,
                content={
                    "detail": "Nenhum julgamento disponÃ­vel ainda. Execute /api/judge primeiro.",
                    "events": []
                }
            )

        with open(judged_file, "r", encoding="utf-8") as f:
            events = json.load(f)

        # EstatÃ­sticas
        scores = [e.get("quality_score", 0) for e in events if e.get("quality_score")]
        stats = {
            "total": len(events),
            "average_score": round(sum(scores) / len(scores), 2) if scores else 0,
            "min_score": round(min(scores), 2) if scores else 0,
            "max_score": round(max(scores), 2) if scores else 0,
        }

        return JSONResponse(content={
            "events": events,
            "stats": stats
        })

    except Exception as e:
        logger.error(f"âŒ Erro ao carregar resultados do julgamento: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
