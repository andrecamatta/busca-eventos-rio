"""Agente de verifica√ß√£o e valida√ß√£o de eventos."""

import asyncio
import difflib
import json
import logging
import re
from datetime import datetime, timedelta
from typing import Any
from urllib.parse import urlparse

from bs4 import BeautifulSoup

from agents.base_agent import BaseAgent
from config import (
    HTTP_TIMEOUT,
    LINK_VALIDATION_MAX_CONCURRENT,
    MAX_RETRIES,
    SEARCH_CONFIG,
)
from utils.http_client import HttpClientWrapper
from utils.link_validator import LinkValidator

logger = logging.getLogger(__name__)

# Sites SPAs que sempre retornam 200 OK (requerem valida√ß√£o de conte√∫do)
SPA_DOMAINS = [
    'eleventickets.com',
    'eventbrite.com.br',
    'eventbrite.com',
]

# Padr√µes de URL v√°lidos por dom√≠nio
URL_PATTERNS = {
    'eleventickets.com': r'!/apresentacao/[a-f0-9]{40}$',  # Hash SHA1 de 40 chars hex (fragment sem #)
}

# SOLU√á√ÉO 2: Dom√≠nios de venues com scrapers dedicados - links gen√©ricos N√ÉO permitidos
SCRAPER_VENUE_DOMAINS = [
    'bluenoterio.com.br',
    'salaceciliameireles.rj.gov.br',
    'theatromunicipal.rj.gov.br',
    'teatromunicipal.rj.gov.br',
    'ccbb.com.br',
    'osb.org.br',
]


class VerifyAgent(BaseAgent):
    """Agente respons√°vel por verificar e validar informa√ß√µes de eventos."""

    def __init__(self, http_client=None):
        """Inicializa VerifyAgent.

        Args:
            http_client: HttpClientWrapper opcional para dependency injection (√∫til para testes)
        """
        super().__init__(
            agent_name="VerifyAgent",
            log_emoji="‚úîÔ∏è",
            model_type="important",  # GPT-5 - tarefa cr√≠tica (verifica√ß√£o rigorosa)
            description="Agente especializado em verificar e validar informa√ß√µes de eventos",
            instructions=[
                "Verificar se as datas dos eventos est√£o no per√≠odo correto "
                f"({SEARCH_CONFIG['start_date'].strftime('%d/%m/%Y')} a {SEARCH_CONFIG['end_date'].strftime('%d/%m/%Y')})",
                "Validar se os links de compra s√£o v√°lidos e acess√≠veis",
                "Identificar e remover eventos duplicados",
                "Confirmar se eventos de com√©dia n√£o s√£o infantis",
                "Verificar consist√™ncia de informa√ß√µes (data/hora/local/pre√ßo)",
                "Enriquecer descri√ß√µes quando necess√°rio",
                "Validar se eventos ao ar livre s√£o realmente em fim de semana",
                "Marcar eventos com baixa confiabilidade para revis√£o",
            ],
            markdown=True,
            http_client=http_client,
        )

    def _initialize_dependencies(self, http_client=None, **kwargs):
        """Inicializa HTTP client e link validator.

        Args:
            http_client: HttpClientWrapper opcional para dependency injection (√∫til para testes)
            **kwargs: Argumentos adicionais
        """
        self.http_client = http_client or HttpClientWrapper()
        self.link_validator = LinkValidator()

    def _is_generic_link(self, url: str) -> bool:
        """Detecta se um link √© gen√©rico (p√°gina de busca/categoria/listagem).

        Args:
            url: URL a verificar

        Returns:
            True se o link for gen√©rico (n√£o espec√≠fico de um evento)
        """
        return self.link_validator.is_generic_link(url)

    def _find_consensus(self, links: list[str], event: dict) -> dict[str, Any] | None:
        """Encontra consenso entre m√∫ltiplos links retornados por diferentes buscas.

        Args:
            links: Lista de links encontrados (pode conter duplicatas e None)
            event: Dados do evento (para logging)

        Returns:
            dict com link consensual e metadados, ou None se n√£o houver consenso
        """
        from collections import Counter
        from config import LINK_CONSENSUS_THRESHOLD, LINK_CONSENSUS_USE_GPT5_TIEBREAKER

        titulo = event.get("titulo", "")

        # Filtrar links v√°lidos (remover None, vazios, "NONE")
        valid_links = [
            link.strip()
            for link in links
            if link and isinstance(link, str) and link.strip() and link.strip().upper() != "NONE"
        ]

        if not valid_links:
            logger.warning(f"{self.log_prefix} Nenhum link v√°lido para consenso: {titulo}")
            return None

        # Normalizar URLs (remover trailing slashes, query params opcionais)
        def normalize_url(url: str) -> str:
            """Normaliza URL para compara√ß√£o (mant√©m path e dom√≠nio)."""
            # Remove trailing slash
            normalized = url.rstrip('/')
            # Remove fragmentos (#)
            normalized = normalized.split('#')[0]
            return normalized

        normalized_links = [normalize_url(link) for link in valid_links]

        # Contar ocorr√™ncias
        link_counts = Counter(normalized_links)
        total_searches = len(links)  # Total de buscas (incluindo falhas)
        threshold = LINK_CONSENSUS_THRESHOLD

        logger.info(f"{self.log_prefix} Consenso de links para '{titulo}':")
        logger.info(f"{self.log_prefix}   Total de buscas: {total_searches}")
        logger.info(f"{self.log_prefix}   Links v√°lidos: {len(valid_links)}")
        logger.info(f"{self.log_prefix}   Links √∫nicos: {len(link_counts)}")

        # Encontrar link com maior frequ√™ncia
        most_common_link, count = link_counts.most_common(1)[0]
        consensus_ratio = count / total_searches

        logger.info(f"{self.log_prefix}   Link mais comum: {most_common_link}")
        logger.info(f"{self.log_prefix}   Apari√ß√µes: {count}/{total_searches} ({consensus_ratio:.1%})")

        # Verificar se atingiu threshold
        if consensus_ratio >= threshold:
            logger.info(f"{self.log_prefix} ‚úÖ CONSENSO ATINGIDO ({consensus_ratio:.1%} >= {threshold:.1%})")
            return {
                "link": most_common_link,
                "consensus_ratio": consensus_ratio,
                "votes": count,
                "total_votes": total_searches,
                "method": "majority_vote",
            }

        # EMPATE ou consenso insuficiente
        logger.warning(f"{self.log_prefix} ‚ö†Ô∏è CONSENSO INSUFICIENTE ({consensus_ratio:.1%} < {threshold:.1%})")

        # Se habilitado, usar GPT-5 Mini como tiebreaker
        if LINK_CONSENSUS_USE_GPT5_TIEBREAKER and len(link_counts) > 1:
            logger.info(f"{self.log_prefix} Usando GPT-5 Mini como desempate...")
            tiebreaker_link = self._tiebreaker_with_gpt5(list(link_counts.keys()), event)
            if tiebreaker_link:
                logger.info(f"{self.log_prefix} ‚úÖ DESEMPATE GPT-5: {tiebreaker_link}")
                return {
                    "link": tiebreaker_link,
                    "consensus_ratio": 0.0,  # N√£o teve consenso por voto
                    "votes": 0,
                    "total_votes": total_searches,
                    "method": "gpt5_tiebreaker",
                    "tiebreaker_candidates": list(link_counts.keys()),
                }

        # Sem consenso e sem desempate
        logger.warning(f"{self.log_prefix} ‚ùå Falha no consenso para: {titulo}")
        return None

    def _tiebreaker_with_gpt5(self, candidate_links: list[str], event: dict) -> str | None:
        """Usa GPT-5 Mini com web search para escolher o melhor link em caso de empate.

        Args:
            candidate_links: Links candidatos (2 ou mais)
            event: Dados do evento

        Returns:
            URL escolhido pelo GPT-5 Mini, ou None se falhar
        """
        from phidata.agent import Agent
        from phidata.models.openai import OpenAIChat
        from config import MODELS, OPENROUTER_API_KEY, OPENROUTER_BASE_URL

        titulo = event.get("titulo", "")
        data = event.get("data", "")
        local = event.get("local", "")

        logger.info(f"{self.log_prefix} GPT-5 Mini: escolhendo entre {len(candidate_links)} links candidatos")

        # Criar agente GPT-5 Mini com web search
        tiebreaker_agent = Agent(
            name="Link Tiebreaker Agent",
            model=OpenAIChat(
                id=MODELS["link_consensus"],  # openai/gpt-5-mini:online
                api_key=OPENROUTER_API_KEY,
                base_url=OPENROUTER_BASE_URL,
            ),
            description="Agente especializado em escolher o melhor link entre candidatos",
            instructions=[
                "Usar web search para verificar qual link √© mais confi√°vel",
                "Priorizar links de plataformas oficiais (Sympla, Eventbrite, etc)",
                "Verificar se o link realmente corresponde ao evento espec√≠fico",
                "Retornar APENAS o URL escolhido (nada mais)",
            ],
        )

        # Formatar links candidatos
        links_formatted = "\n".join([f"  {i+1}. {link}" for i, link in enumerate(candidate_links)])

        prompt = f"""Escolha o MELHOR link para este evento entre os candidatos abaixo.

EVENTO:
- T√≠tulo: {titulo}
- Data: {data}
- Local: {local}

LINKS CANDIDATOS (escolha apenas 1):
{links_formatted}

CRIT√âRIOS DE ESCOLHA:
1. Link de plataforma oficial de venda (Sympla, Eventbrite, etc) > site do venue > outros
2. Link que realmente corresponde ao evento espec√≠fico (n√£o gen√©rico)
3. Link ativo e acess√≠vel (verificar com web search se poss√≠vel)

RETORNE APENAS:
- O URL completo do link escolhido (copie exatamente como est√° acima)
- N√ÉO adicione explica√ß√µes ou justificativas"""

        try:
            response = tiebreaker_agent.run(prompt)
            chosen_link = response.content.strip()

            # Validar que √© um dos candidatos
            normalized_chosen = chosen_link.rstrip('/').split('#')[0]
            for candidate in candidate_links:
                normalized_candidate = candidate.rstrip('/').split('#')[0]
                if normalized_chosen == normalized_candidate:
                    logger.info(f"{self.log_prefix} GPT-5 Mini escolheu: {candidate}")
                    return candidate

            logger.warning(f"{self.log_prefix} GPT-5 Mini retornou link inv√°lido: {chosen_link}")
            return None

        except Exception as e:
            logger.error(f"{self.log_prefix} Erro no tiebreaker GPT-5: {e}")
            return None

    async def _search_with_variants(self, event: dict, variant_suffix: str = "") -> str | None:
        """Executa busca com Perplexity com pequenas varia√ß√µes para diversificar resultados.

        Args:
            event: Dados do evento
            variant_suffix: Sufixo para adicionar varia√ß√£o na busca (ex: "sympla", "eventbrite")

        Returns:
            URL encontrado ou None
        """
        from phidata.agent import Agent
        from phidata.models.openai import OpenAIChat
        from config import MODELS, OPENROUTER_API_KEY, OPENROUTER_BASE_URL

        titulo = event.get("titulo", "") or event.get("nome", "")
        data = event.get("data", "")
        local = event.get("local", "")

        # Criar agente de busca com Perplexity
        search_agent = Agent(
            name="Link Search Agent",
            model=OpenAIChat(
                id=MODELS["search"],  # perplexity/sonar
                api_key=OPENROUTER_API_KEY,
                base_url=OPENROUTER_BASE_URL,
            ),
            description="Agente especializado em encontrar links oficiais de eventos",
            instructions=[
                "Buscar apenas links OFICIAIS de venda/informa√ß√µes",
                "Priorizar Sympla, Eventbrite, Ticketmaster, site do venue",
                "N√ÉO retornar links gen√©ricos de homepage",
                "Retornar APENAS o URL ou 'NONE' se n√£o encontrar",
            ],
        )

        # Adicionar varia√ß√£o √† busca se especificada
        platform_hint = f"\nPRIORIDADE: Buscar preferencialmente em {variant_suffix}" if variant_suffix else ""

        prompt = f"""Encontre o link OFICIAL de compra/venda de ingresso para este evento:

EVENTO:
- T√≠tulo: {titulo}
- Data: {data}
- Local: {local}{platform_hint}

‚ö†Ô∏è REGRAS:
1. N√ÉO retorne homepages ou p√°ginas de listagem
2. Link DEVE ser espec√≠fico do evento (com ID/slug √∫nico)
3. Retorne APENAS o URL completo ou "NONE"

RETORNE APENAS:
- O URL completo (https://...) OU
- A palavra "NONE" (se n√£o encontrar)"""

        try:
            response = search_agent.run(prompt)
            link = response.content.strip()

            if link and link != "NONE" and link.startswith("http"):
                logger.info(f"{self.log_prefix} Busca{f' ({variant_suffix})' if variant_suffix else ''}: {link}")
                return link
            else:
                logger.warning(f"{self.log_prefix} Busca{f' ({variant_suffix})' if variant_suffix else ''}: NONE")
                return None

        except Exception as e:
            logger.error(f"{self.log_prefix} Erro na busca com variante: {e}")
            return None

    async def _intelligent_link_search_with_consensus(self, event: dict, attempt: int = 1) -> dict[str, Any]:
        """Vers√£o com consenso multi-modelo do _intelligent_link_search().

        Executa m√∫ltiplas buscas independentes e usa consenso para reduzir alucina√ß√µes.

        Returns:
            dict com: link (str), quality_score (int), validation (dict), consensus_info (dict)
        """
        from config import (
            LINK_CONSENSUS_ENABLED,
            LINK_CONSENSUS_SEARCHES,
            LINK_MAX_INTELLIGENT_SEARCHES,
            LINK_QUALITY_THRESHOLD,
        )

        titulo = event.get("titulo", "") or event.get("nome", "")

        # Se consenso desabilitado, usar m√©todo tradicional
        if not LINK_CONSENSUS_ENABLED:
            return await self._intelligent_link_search(event, attempt)

        if attempt > LINK_MAX_INTELLIGENT_SEARCHES:
            logger.warning(f"{self.log_prefix} Limite de {LINK_MAX_INTELLIGENT_SEARCHES} tentativas atingido: {titulo}")
            return None

        logger.info(f"{self.log_prefix} Busca com consenso (tentativa {attempt}/{LINK_MAX_INTELLIGENT_SEARCHES}): {titulo}")

        # Executar m√∫ltiplas buscas independentes em paralelo
        search_tasks = []

        # Primeira busca sem varia√ß√£o
        search_tasks.append(self._search_with_variants(event, ""))

        # Demais buscas com hints de plataforma (para diversificar)
        platform_variants = ["sympla", "eventbrite", "site oficial"]
        for i in range(1, LINK_CONSENSUS_SEARCHES):
            variant = platform_variants[i % len(platform_variants)]
            search_tasks.append(self._search_with_variants(event, variant))

        # Executar todas em paralelo
        search_results = await asyncio.gather(*search_tasks, return_exceptions=True)

        # Converter exce√ß√µes em None
        links = [result if not isinstance(result, Exception) else None for result in search_results]

        logger.info(f"{self.log_prefix} Resultados de {LINK_CONSENSUS_SEARCHES} buscas: {links}")

        # Encontrar consenso
        consensus_result = self._find_consensus(links, event)

        if not consensus_result:
            logger.warning(f"{self.log_prefix} Nenhum consenso encontrado para: {titulo}")

            # Retry se ainda tiver tentativas
            if attempt < LINK_MAX_INTELLIGENT_SEARCHES:
                logger.info(f"{self.log_prefix} Tentando novamente com novos crit√©rios...")
                return await self._intelligent_link_search_with_consensus(event, attempt + 1)
            else:
                return None

        # Validar qualidade do link consensual
        consensus_link = consensus_result["link"]

        # Verificar se link √© gen√©rico
        if self._is_generic_link(consensus_link):
            logger.warning(f"{self.log_prefix} ‚ùå Link consensual √© gen√©rico: {consensus_link}")
            if attempt < LINK_MAX_INTELLIGENT_SEARCHES:
                return await self._intelligent_link_search_with_consensus(event, attempt + 1)
            else:
                return None

        # Validar qualidade
        try:
            from agents.validation_agent import ValidationAgent

            validation_agent = ValidationAgent()
            link_info = await validation_agent._fetch_link_info(consensus_link, event)

            quality_validation = link_info.get("quality_validation")

            if quality_validation:
                score = quality_validation["score"]
                is_quality = quality_validation["is_quality"]

                if is_quality:
                    logger.info(f"{self.log_prefix} ‚úÖ Link consensual aprovado (score: {score}/100)")
                    return {
                        "link": consensus_link,
                        "quality_score": score,
                        "validation": quality_validation,
                        "consensus_info": consensus_result,
                        "structured_data": link_info.get("structured_data", {}),
                    }
                else:
                    logger.warning(f"{self.log_prefix} ‚ùå Link consensual rejeitado (score: {score}/{LINK_QUALITY_THRESHOLD})")
                    if attempt < LINK_MAX_INTELLIGENT_SEARCHES:
                        return await self._intelligent_link_search_with_consensus(event, attempt + 1)
                    else:
                        # √öltima tentativa - retornar mesmo com baixa qualidade
                        return {
                            "link": consensus_link,
                            "quality_score": score,
                            "validation": quality_validation,
                            "consensus_info": consensus_result,
                            "low_quality": True,
                        }
            else:
                # Sem valida√ß√£o, aceitar link consensual
                logger.warning(f"{self.log_prefix} Valida√ß√£o falhou, aceitando link consensual")
                return {
                    "link": consensus_link,
                    "quality_score": None,
                    "validation": None,
                    "consensus_info": consensus_result,
                }

        except Exception as e:
            logger.error(f"{self.log_prefix} Erro ao validar link consensual: {e}")
            return {
                "link": consensus_link,
                "quality_score": None,
                "validation": None,
                "consensus_info": consensus_result,
            }

    def _classify_link_type(self, url: str, event: dict) -> str:
        """Classifica o tipo de link do evento.

        Args:
            url: URL do link
            event: Dados do evento (para contexto)

        Returns:
            "purchase" (plataforma de venda), "info" (site informativo), ou "venue" (p√°gina do local)
        """
        return self.link_validator.classify_link_type(url, event)

    def _matches_url_pattern(self, url: str) -> bool:
        """Valida se URL corresponde ao padr√£o esperado para o dom√≠nio.

        Args:
            url: URL a validar

        Returns:
            True se URL corresponde ao padr√£o do dom√≠nio, False caso contr√°rio
        """
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()

            # Verificar se dom√≠nio tem padr√£o definido
            for pattern_domain, pattern in URL_PATTERNS.items():
                if pattern_domain in domain:
                    # Validar contra o padr√£o
                    full_path = parsed.path + parsed.fragment  # ElevenTickets usa fragment (#/...)
                    if re.search(pattern, full_path):
                        return True
                    else:
                        logger.warning(f"URL n√£o corresponde ao padr√£o esperado para {pattern_domain}: {url}")
                        return False

            # Dom√≠nio sem padr√£o definido = aceitar
            return True

        except Exception as e:
            logger.error(f"Erro ao validar padr√£o de URL: {e}")
            return True  # Em caso de erro, n√£o bloquear

    async def _validate_link_content(self, link: str, event: dict) -> dict:
        """Valida conte√∫do de link SPA verificando se informa√ß√µes do evento correspondem.

        Args:
            link: URL a validar
            event: Evento com informa√ß√µes esperadas

        Returns:
            dict com: valid (bool), reason (str), details (dict)
        """
        try:
            # Fetch + parse usando HttpClientWrapper
            result = await self.http_client.fetch_and_parse(
                link,
                extract_text=True,
                text_max_length=5000,  # Mais texto para valida√ß√£o de conte√∫do
                clean_html=True
            )

            if not result["success"]:
                status_code = result["status_code"]
                return {
                    "valid": False,
                    "reason": f"HTTP {status_code}" if status_code else result["error"],
                    "details": {}
                }

            # Extrair texto vis√≠vel da p√°gina
            page_text = result["text"].lower()

            # QUICK WIN #1: Validar conte√∫do m√≠nimo (detectar soft 404s)
            from config import LINK_VALIDATION
            if not page_text or len(page_text.strip()) < LINK_VALIDATION["min_page_length"]:
                return {
                    "valid": False,
                    "reason": "P√°gina vazia ou conte√∫do muito curto (poss√≠vel 404 disfar√ßado)",
                    "details": {"page_length": len(page_text.strip()) if page_text else 0}
                }

            # Informa√ß√µes do evento para validar
            from utils.event_normalizer import EventNormalizer
            titulo = EventNormalizer.get_title(event).lower()
            local = EventNormalizer.get_venue(event).lower()

            # Valida√ß√µes de conte√∫do
            issues = []
            matches = []

            # Verificar t√≠tulo (pelo menos 70% das palavras) - threshold aumentado para reduzir falsos positivos
            titulo_words = [w for w in titulo.split() if len(w) > 3]  # palavras > 3 chars
            titulo_match_ratio = 0
            if titulo_words:
                titulo_matches = sum(1 for word in titulo_words if word in page_text)
                titulo_match_ratio = titulo_matches / len(titulo_words)

                if titulo_match_ratio >= LINK_VALIDATION["title_match_threshold"]:
                    matches.append(f"T√≠tulo encontrado ({titulo_match_ratio:.0%})")
                else:
                    issues.append(f"T√≠tulo n√£o encontrado ({titulo_match_ratio:.0%} match)")

            # Verificar local (palavras principais) - QUICK WIN #2: threshold reduzido de 50% para 40%
            local_words = [w for w in local.split() if len(w) > 4]  # palavras > 4 chars
            local_match_ratio = 0
            if local_words:
                local_matches = sum(1 for word in local_words if word in page_text)
                local_match_ratio = local_matches / len(local_words) if local_words else 0

                if local_match_ratio >= LINK_VALIDATION["venue_match_threshold"]:
                    matches.append(f"Local encontrado ({local_match_ratio:.0%})")
                else:
                    issues.append(f"Local n√£o encontrado ({local_match_ratio:.0%} match)")

            # Verificar se p√°gina tem indicadores de venda (bot√µes de compra)
            buy_indicators = ['comprar', 'ingresso', 'ticket', 'buy', 'cart', 'carrinho']
            has_buy_button = any(indicator in page_text for indicator in buy_indicators)

            if has_buy_button:
                matches.append("Bot√£o de compra encontrado")
            else:
                issues.append("Nenhum bot√£o de compra encontrado")

            # Decis√£o final
            valid = len(matches) >= 2 and len(issues) <= 1

            return {
                "valid": valid,
                "reason": "Conte√∫do validado" if valid else f"Valida√ß√£o falhou: {', '.join(issues)}",
                "details": {
                    "matches": matches,
                    "issues": issues,
                    "titulo_match": titulo_match_ratio if titulo_words else None,
                    "local_match": local_match_ratio if local_words else None,
                }
            }

        except Exception as e:
            logger.error(f"Erro ao validar conte√∫do do link: {e}")
            return {
                "valid": True,  # Em caso de erro, n√£o bloquear (pode ser problema tempor√°rio)
                "reason": f"Erro na valida√ß√£o: {str(e)}",
                "details": {}
            }

    async def _validate_link_referencia(self, link: str, event: dict) -> dict:
        """Valida link_referencia especificamente (sem exigir bot√µes de compra).

        Args:
            link: URL do link_referencia a validar
            event: Evento com informa√ß√µes esperadas

        Returns:
            dict com: valid (bool), reason (str), details (dict)
        """
        try:
            # STEP 1: Verificar DNS resolution (prevenir URLs alucinadas)
            import socket
            from urllib.parse import urlparse

            parsed = urlparse(link)
            hostname = parsed.netloc or parsed.path.split('/')[0]

            try:
                socket.gethostbyname(hostname)
            except socket.gaierror:
                logger.warning(f"DNS resolution failed for: {hostname}")
                return {
                    "valid": False,
                    "reason": f"Domain does not exist: {hostname}",
                    "details": {"error_type": "dns_failure", "domain": hostname}
                }

            # STEP 2: Verificar HTTP status (200 OK)
            result = await self.http_client.fetch_and_parse(
                link,
                extract_text=True,
                text_max_length=5000,
                clean_html=True
            )

            if not result["success"]:
                status_code = result["status_code"]
                error_msg = f"HTTP {status_code}" if status_code else result["error"]
                logger.warning(f"link_referencia HTTP error: {error_msg} - {link}")
                return {
                    "valid": False,
                    "reason": error_msg,
                    "details": {"error_type": "http_error", "status_code": status_code}
                }

            # STEP 3: Verificar conte√∫do m√≠nimo (detectar soft 404s)
            page_text = result["text"].lower()
            if not page_text or len(page_text.strip()) < 50:
                logger.warning(f"link_referencia has empty/minimal content: {link}")
                return {
                    "valid": False,
                    "reason": "Empty or minimal content (likely soft 404)",
                    "details": {"error_type": "empty_content", "page_length": len(page_text.strip())}
                }

            # STEP 4: Verificar se menciona o evento espec√≠fico
            from utils.event_normalizer import EventNormalizer
            titulo = EventNormalizer.get_title(event).lower()
            titulo_words = [w for w in titulo.split() if len(w) > 3]  # palavras > 3 chars

            titulo_match_ratio = 0
            if titulo_words:
                titulo_matches = sum(1 for word in titulo_words if word in page_text)
                titulo_match_ratio = titulo_matches / len(titulo_words)

            # Para link_referencia, aceitar threshold mais baixo (50% vs 70% para link_ingresso)
            # Raz√£o: Links informativos podem ter estrutura diferente
            if titulo_match_ratio >= 0.5:
                logger.info(f"‚úì link_referencia v√°lido ({titulo_match_ratio:.0%} title match): {link}")
                return {
                    "valid": True,
                    "reason": f"Valid reference link ({titulo_match_ratio:.0%} title match)",
                    "details": {
                        "titulo_match": titulo_match_ratio,
                        "is_event_specific": titulo_match_ratio >= 0.7
                    }
                }
            else:
                # Link gen√©rico detectado
                logger.warning(f"link_referencia √© gen√©rico ({titulo_match_ratio:.0%} title match): {link}")
                return {
                    "valid": False,
                    "reason": f"Generic link (only {titulo_match_ratio:.0%} title match)",
                    "details": {"error_type": "generic_content", "titulo_match": titulo_match_ratio}
                }

        except Exception as e:
            logger.error(f"Erro ao validar link_referencia: {e}")
            # Para link_referencia, REJEITAR em caso de erro (diferente de link_ingresso)
            # Raz√£o: √â melhor ter null do que link quebrado
            return {
                "valid": False,
                "reason": f"Validation error: {str(e)}",
                "details": {"error_type": "exception", "error": str(e)}
            }

    async def verify_events(self, events_json: str) -> dict[str, Any]:
        """Verifica e valida eventos extra√≠dos pelo agente de busca."""
        logger.info(f"{self.log_prefix} Iniciando verifica√ß√£o de eventos...")

        try:
            events_data = json.loads(events_json) if isinstance(events_json, str) else events_json
        except json.JSONDecodeError:
            logger.error("Erro ao decodificar JSON de eventos")
            return {"verified_events": [], "rejected_events": [], "warnings": ["JSON inv√°lido"]}

        # Validar links em paralelo (valida√ß√£o b√°sica)
        events_with_link_validation = await self._validate_links(events_data)

        # Processar com LLM para verifica√ß√£o inteligente (decis√£o final)
        verified_data = self._verify_with_llm(events_with_link_validation)

        logger.info(
            f"Verifica√ß√£o conclu√≠da. Eventos aprovados: {len(verified_data.get('verified_events', []))}, "
            f"rejeitados: {len(verified_data.get('rejected_events', []))}"
        )

        return verified_data

    async def _intelligent_link_search(self, event: dict, attempt: int = 1) -> dict[str, Any]:
        """Usa Perplexity para buscar o link correto de um evento e valida o conte√∫do.

        Returns:
            dict com: link (str), quality_score (int), validation (dict) ou None se n√£o encontrar
        """
        from config import LINK_MAX_INTELLIGENT_SEARCHES, LINK_QUALITY_THRESHOLD

        if attempt > LINK_MAX_INTELLIGENT_SEARCHES:
            logger.warning(f"{self.log_prefix} Limite de {LINK_MAX_INTELLIGENT_SEARCHES} tentativas atingido para: {event.get('titulo')}")
            return None

        titulo = event.get("titulo", "") or event.get("nome", "")
        data = event.get("data", "")
        horario = event.get("horario", "")
        local = event.get("local", "")
        categoria = event.get("categoria", "")
        preco = event.get("preco", "")
        descricao = event.get("descricao_enriquecida") or event.get("descricao", "")
        fontes = event.get("fontes", [])

        logger.info(f"{self.log_prefix} Buscando link correto (tentativa {attempt}/{LINK_MAX_INTELLIGENT_SEARCHES}): {titulo}")

        # Criar agente de busca com Perplexity
        search_agent = Agent(
            name="Link Search Agent",
            model=OpenAIChat(
                id=MODELS["search"],  # perplexity/sonar-pro
                api_key=OPENROUTER_API_KEY,
                base_url=OPENROUTER_BASE_URL,
            ),
            description="Agente especializado em encontrar links oficiais de eventos",
            instructions=[
                "Buscar apenas links OFICIAIS de venda/informa√ß√µes",
                "Priorizar Sympla, Eventbrite, Ticketmaster, site do venue",
                "N√ÉO retornar links gen√©ricos de homepage",
                "Retornar APENAS o URL ou 'NONE' se n√£o encontrar",
            ],
        )

        # Construir prompt com todas as informa√ß√µes dispon√≠veis
        fonte_info = f"\nFontes mencionadas: {', '.join(fontes[:3])}" if fontes else ""
        horario_info = f"\nHor√°rio: {horario}" if horario else ""
        categoria_info = f"\nCategoria: {categoria}" if categoria else ""
        preco_info = f"\nPre√ßo: {preco}" if preco else ""

        # Se √© retry, adicionar contexto do problema anterior
        retry_context = ""
        if attempt > 1:
            retry_context = f"""

‚ö†Ô∏è TENTATIVA {attempt}/{LINK_MAX_INTELLIGENT_SEARCHES} - TENTATIVA ANTERIOR FALHOU

PROBLEMAS DETECTADOS:
- Link retornado foi gen√©rico/homepage ou n√£o foi encontrado
- Tente buscar em OUTRAS plataformas al√©m da anterior
- Verifique redes sociais do venue (Instagram/Facebook) por links nos posts
- Se realmente n√£o existir link online, retorne "NONE"
"""

        prompt = f"""Encontre o link OFICIAL de compra/venda de ingresso para este evento:

EVENTO:
- T√≠tulo: {titulo}
- Data: {data}{horario_info}
- Local: {local}{categoria_info}{preco_info}
- Descri√ß√£o: {descricao[:200]}{fonte_info}{retry_context}

‚ö†Ô∏è REGRAS CR√çTICAS (leia com aten√ß√£o):

1. N√ÉO RETORNE sites de ARTISTAS ou VENUES (p√°ginas institucionais):
   ‚ùå ERRADO: raphaelghanem.com.br (site do artista)
   ‚ùå ERRADO: casadochoro.com.br (homepage do venue)
   ‚ùå ERRADO: sympla.com.br (homepage do Sympla)
   ‚ùå ERRADO: salaceliciameireles.rj.gov.br/programacao (listagem geral)

   ‚úÖ CORRETO: sympla.com.br/evento/raphael-ghanem-18112025/12345
   ‚úÖ CORRETO: eventbrite.com.br/e/quarteto-de-cordas-tickets-67890
   ‚úÖ CORRETO: bluenoterio.com.br/shows/nome-show__abc123/

2. O link DEVE conter identificador √öNICO do evento:
   - ID num√©rico: /evento/nome/123456
   - Slug com data: /evento-18-11-2025
   - Hash alfanum√©rico: /show/nome__abc123de/
   - Par√¢metro: ?event_id=789

3. PLATAFORMAS PRIORIT√ÅRIAS (nesta ordem):
   a) Sympla: sympla.com.br/evento/[nome-evento]/[ID-numerico]
   b) Eventbrite: eventbrite.com.br/e/[nome-evento]-tickets-[ID]
   c) Ticketmaster: ticketmaster.com.br/event/[ID]
   d) Ingresso.com: ingresso.com/evento/[nome-evento]/[ID]
   e) Site do venue com p√°gina espec√≠fica (ex: bluenoterio.com.br/shows/[evento]/)

4. SE N√ÉO ENCONTRAR link espec√≠fico em NENHUMA plataforma:
   - Retorne "NONE"
   - N√ÉO invente links gen√©ricos

EXEMPLOS DE LINKS V√ÅLIDOS (RETORNE ASSIM):
‚úÖ https://www.sympla.com.br/evento/raphael-ghanem-stand-up/2345678
‚úÖ https://www.eventbrite.com.br/e/quarteto-de-cordas-da-osb-tickets-987654321
‚úÖ https://bluenoterio.com.br/shows/irma-you-and-my-guitar__22hz624n/
‚úÖ https://sis.ingressodigital.com/lojanew/detalhes_evento.asp?eve_cod=15246

EXEMPLOS DE LINKS INV√ÅLIDOS (N√ÉO RETORNE):
‚ùå https://raphaelghanem.com.br (site oficial do artista)
‚ùå https://www.sympla.com.br (homepage do Sympla)
‚ùå https://www.sympla.com.br/eventos/rio-de-janeiro (listagem por cidade)
‚ùå https://casadochoro.com.br/programacao (calend√°rio do venue)
‚ùå https://salaceliciameireles.rj.gov.br/programacao (agenda geral)

RETORNE APENAS:
- O URL completo (https://...) OU
- A palavra "NONE" (se n√£o encontrar link espec√≠fico de venda)"""

        try:
            response = search_agent.run(prompt)
            new_link = response.content.strip()

            # Validar resposta
            if new_link and new_link != "NONE" and new_link.startswith("http"):
                logger.info(f"{self.log_prefix} Link encontrado: {new_link}")

                # Verificar se link √© gen√©rico (p√°gina de listagem)
                if self._is_generic_link(new_link):
                    logger.warning(f"{self.log_prefix} ‚ùå Link gen√©rico detectado: {new_link}")

                    # Retry se ainda tiver tentativas
                    if attempt < LINK_MAX_INTELLIGENT_SEARCHES:
                        logger.info(f"{self.log_prefix} Tentando busca novamente solicitando link ESPEC√çFICO...")
                        return await self._intelligent_link_search(event, attempt + 1)
                    else:
                        logger.warning(f"{self.log_prefix} Todas tentativas esgotadas. Link gen√©rico rejeitado.")
                        return None

                # NOVO: Validar qualidade do link encontrado
                try:
                    from agents.validation_agent import ValidationAgent

                    validation_agent = ValidationAgent()
                    link_info = await validation_agent._fetch_link_info(new_link, event)

                    quality_validation = link_info.get("quality_validation")

                    if quality_validation:
                        score = quality_validation["score"]
                        is_quality = quality_validation["is_quality"]

                        if is_quality:
                            logger.info(f"{self.log_prefix} ‚úÖ Link aprovado (score: {score}/100)")
                            return {
                                "link": new_link,
                                "quality_score": score,
                                "validation": quality_validation,
                                "structured_data": link_info.get("structured_data", {}),
                            }
                        else:
                            logger.warning(
                                f"{self.log_prefix} ‚ùå Link rejeitado (score: {score}/{LINK_QUALITY_THRESHOLD})"
                            )
                            logger.warning(f"{self.log_prefix} Issues: {', '.join(quality_validation['issues'])}")

                            # Retry se ainda tiver tentativas
                            if attempt < LINK_MAX_INTELLIGENT_SEARCHES:
                                logger.info(f"{self.log_prefix} Tentando busca novamente com crit√©rios mais rigorosos...")
                                return await self._intelligent_link_search(event, attempt + 1)
                            else:
                                logger.warning(f"{self.log_prefix} Todas tentativas esgotadas. Retornando link mesmo com baixa qualidade.")
                                return {
                                    "link": new_link,
                                    "quality_score": score,
                                    "validation": quality_validation,
                                    "low_quality": True,
                                }
                    else:
                        # Sem valida√ß√£o de qualidade (erro), aceitar link
                        logger.warning(f"{self.log_prefix} Valida√ß√£o de qualidade falhou, aceitando link")
                        return {"link": new_link, "quality_score": None, "validation": None}

                except Exception as e:
                    logger.error(f"{self.log_prefix} Erro ao validar qualidade do link: {e}")
                    # Em caso de erro, retornar link sem valida√ß√£o
                    return {"link": new_link, "quality_score": None, "validation": None}

            else:
                logger.warning(f"{self.log_prefix} ‚úó Nenhum link v√°lido encontrado para: {titulo}")
                return None

        except Exception as e:
            logger.error(f"{self.log_prefix} Erro na busca inteligente de link: {e}")
            return None

    async def _validate_single_event_link(self, event: dict) -> dict:
        """Valida o link de um √∫nico evento com retry e busca inteligente.

        Args:
            event: Evento a validar

        Returns:
            Dicion√°rio com estat√≠sticas da valida√ß√£o deste evento
        """
        stats = {
            "total_links": 0,
            "validated_first_try": 0,
            "failed_all_retries": 0,
            "intelligent_searches": 0,
            "links_fixed": 0,
            "no_retry_needed": 0,
            "generic_links_detected": 0,
        }

        # NOVA VALIDA√á√ÉO SEPARADA: Validar link_referencia independentemente
        # Isso acontece ANTES da valida√ß√£o do link principal para garantir que sempre roda
        link_ref = event.get("link_referencia")
        if link_ref and link_ref not in ["INCOMPLETO", "incompleto", "/INCOMPLETO", "NONE", "none"]:
            logger.info(f"‚Üí Validando link_referencia: {link_ref[:60]}...")
            ref_validation = await self._validate_link_referencia(link_ref, event)

            if not ref_validation["valid"]:
                # link_referencia inv√°lido - definir como null
                reason = ref_validation["reason"]
                logger.warning(f"‚úó link_referencia inv√°lido ({reason}), definindo como null")
                event["link_referencia"] = None
                event["link_referencia_rejected"] = reason
                event["link_referencia_details"] = ref_validation["details"]
            else:
                # link_referencia v√°lido
                logger.info(f"‚úì link_referencia validado com sucesso")
                event["link_referencia_validated"] = True
                event["link_referencia_details"] = ref_validation["details"]

        # Priorizar link_ingresso, fallback para link_referencia
        from utils.event_normalizer import EventNormalizer
        link = EventNormalizer.get_link(event)

        if not link:
            # OTIMIZA√á√ÉO: N√£o buscar link, aceitar evento sem link
            logger.info(f"‚Üí Evento sem link: {event.get('titulo')}")
            stats["total_links"] += 1
            event["link_valid"] = None
            event["link_status_code"] = None
            return stats

        # Detectar placeholder "INCOMPLETO"
        if link in ["INCOMPLETO", "incompleto", "/INCOMPLETO", "NONE", "none"]:
            # OTIMIZA√á√ÉO: N√£o buscar link, aceitar placeholder
            logger.info(f"‚Üí Link placeholder detectado ({link}): {event.get('titulo')}")
            stats["total_links"] += 1
            event["link_valid"] = None
            event["link_status_code"] = None
            return stats

        # Detectar link gen√©rico (p√°gina de busca/categoria)
        if self._is_generic_link(link):
            logger.info(f"‚Üí Link gen√©rico detectado: {link[:80]}...")
            stats["total_links"] += 1
            stats["generic_links_detected"] += 1

            # SOLU√á√ÉO 2: Bloquear links gen√©ricos de venues com scrapers dedicados
            parsed_url = urlparse(link)
            domain = parsed_url.netloc.lower()

            # Verificar se √© de um venue com scraper dedicado
            is_scraper_venue = any(scraper_domain in domain for scraper_domain in SCRAPER_VENUE_DOMAINS)

            if is_scraper_venue:
                # Rejeitar link gen√©rico de venue com scraper
                logger.warning(f"‚úó REJEITADO: Link gen√©rico de venue com scraper dedicado ({domain})")
                event["link_valid"] = False
                event["link_is_generic"] = True
                event["link_status_code"] = None
                event["rejection_reason"] = f"Link gen√©rico n√£o permitido para venue com scraper dedicado ({domain})"
                stats["generic_links_rejected_scraper_venue"] = stats.get("generic_links_rejected_scraper_venue", 0) + 1
                return stats

            # Aceitar link gen√©rico para outros venues
            event["link_valid"] = True
            event["link_is_generic"] = True
            event["link_status_code"] = 200
            return stats

        stats["total_links"] += 1
        original_link = link

        # EXCE√á√ÉO 1: Links do Eventim n√£o respondem bem a HEAD requests
        if 'eventim.com.br/artist/blue-note-rio/' in link.lower():
            event["link_valid"] = True
            event["link_status_code"] = 200
            event["validation_skipped"] = "Eventim links are trusted (HEAD requests not supported)"
            stats["validated_first_try"] += 1
            logger.info(f"‚úì Link Eventim v√°lido (sem valida√ß√£o HTTP): {link}")
            return stats

        # EXCE√á√ÉO 2: Links oficiais da Sala Cec√≠lia Meireles (.gov.br)
        if 'salaceciliameireles.rj.gov.br/programacao/' in link.lower():
            event["link_valid"] = True
            event["link_status_code"] = 200
            event["validation_skipped"] = "Official Sala Cec√≠lia Meireles links are trusted"
            stats["validated_first_try"] += 1
            logger.info(f"‚úì Link oficial Sala Cec√≠lia v√°lido (sem valida√ß√£o HTTP): {link}")
            return stats

        # EXCE√á√ÉO 3: Links oficiais do Teatro Municipal (.gov.br) - SSL issues
        if 'theatromunicipal.rj.gov.br' in link.lower():
            event["link_valid"] = True
            event["link_status_code"] = 200
            event["validation_skipped"] = "Official Teatro Municipal links are trusted (SSL issues)"
            stats["validated_first_try"] += 1
            logger.info(f"‚úì Link oficial Teatro Municipal v√°lido (sem valida√ß√£o HTTP): {link}")
            return stats

        # Validar link via HTTP request
        http_client = HttpClientWrapper()
        link_status = await http_client.check_link_status(link)

        # Link acess√≠vel (200 OK) - validar conte√∫do antes de aceitar
        if link_status["accessible"]:
            # NOVA VALIDA√á√ÉO: Verificar se conte√∫do corresponde ao evento
            logger.info(f"‚Üí Link HTTP 200 OK, validando conte√∫do: {link[:80]}...")
            content_validation = await self._validate_link_content(link, event)

            if content_validation["valid"]:
                # Conte√∫do v√°lido - aceitar link
                event["link_valid"] = True
                event["link_status_code"] = link_status["status_code"]
                event["link_content_validated"] = True
                event["link_validation_details"] = content_validation["details"]
                stats["validated_first_try"] += 1
                logger.info(f"‚úì Link v√°lido (HTTP 200 + conte√∫do OK): {link}")
                return stats
            else:
                # Conte√∫do inv√°lido - OTIMIZA√á√ÉO: aceitar mesmo assim
                reason = content_validation["reason"]
                logger.warning(f"‚ö†Ô∏è Link HTTP 200 mas conte√∫do possivelmente inv√°lido ({reason}): {link}")
                event["link_valid"] = True  # Aceitar com aviso
                event["link_status_code"] = link_status["status_code"]
                event["link_content_warning"] = reason
                event["link_validation_details"] = content_validation["details"]
                stats["validated_first_try"] += 1
                return stats

        # Link com erro (404, 403, timeout) - OTIMIZA√á√ÉO: aceitar com aviso
        status_code = link_status.get("status_code")
        reason = link_status.get("reason", "Unknown error")

        logger.warning(f"‚ö†Ô∏è Link com erro ({reason}): {link}")
        stats["total_links"] += 1
        stats["link_errors"] = stats.get("link_errors", 0) + 1

        # Aceitar evento mesmo com link quebrado
        event["link_valid"] = False
        event["link_status_code"] = status_code
        event["link_error"] = f"{status_code} - {reason}" if status_code else reason
        return stats

    async def _validate_links(self, events: dict | list) -> dict | list:
        """Valida se os links de eventos s√£o acess√≠veis com retry e busca inteligente (paralelizado)."""
        logger.info(f"{self.log_prefix} Validando links de eventos em paralelo com retry autom√°tico...")

        # Extrair eventos da estrutura complexa do structured_events.json
        event_list = []
        if isinstance(events, dict):
            # Estrutura: {"eventos_gerais": {"eventos": [...]}, "eventos_locais_especiais": {...}}
            if "eventos_gerais" in events:
                event_list.extend(events["eventos_gerais"].get("eventos", []))

            if "eventos_locais_especiais" in events:
                for local_name, local_events in events["eventos_locais_especiais"].items():
                    if isinstance(local_events, list):
                        # Filtra eventos reais (apenas dicts, ignora __checagem e outros tipos)
                        event_list.extend([e for e in local_events if isinstance(e, dict) and "__checagem" not in e])

            # Fallback para estrutura simples
            if not event_list:
                event_list = events.get("events", [])
        else:
            event_list = events

        # Estat√≠sticas agregadas
        stats = {
            "total_links": 0,
            "validated_first_try": 0,
            "validated_after_retry": 0,
            "failed_all_retries": 0,
            "intelligent_searches": 0,
            "links_fixed": 0,
            "no_retry_needed": 0,
            "generic_links_detected": 0,
        }

        # Validar todos os links em paralelo com rate limiting
        # Criar sem√°foro para limitar concorr√™ncia
        semaphore = asyncio.Semaphore(LINK_VALIDATION_MAX_CONCURRENT)

        async def validate_with_semaphore(event):
            """Wrapper para valida√ß√£o com sem√°foro."""
            async with semaphore:
                return await self._validate_single_event_link(event)

        # Criar tasks para validar todos os eventos em paralelo
        validation_tasks = [
            validate_with_semaphore(event)
            for event in event_list
        ]

        # Executar todas as valida√ß√µes em paralelo (respeitando sem√°foro)
        logger.info(f"Iniciando valida√ß√£o paralela de {len(event_list)} links (m√°x {LINK_VALIDATION_MAX_CONCURRENT} concorrentes)...")
        validation_results = await asyncio.gather(*validation_tasks, return_exceptions=True)

        # Agregar estat√≠sticas
        for result in validation_results:
            if isinstance(result, dict):
                for key in stats:
                    stats[key] += result.get(key, 0)

        # Log de estat√≠sticas
        logger.info(f"\n{'='*60}")
        logger.info("üìä Estat√≠sticas de Valida√ß√£o de Links:")
        logger.info(f"  Total de links verificados: {stats['total_links']}")
        logger.info(f"  ‚úì Validados na 1¬™ tentativa: {stats['validated_first_try']}")
        logger.info(f"  ‚úó Falharam ap√≥s todos os retries: {stats['failed_all_retries']}")
        logger.info(f"  ‚úó Erros permanentes (404, 403, etc): {stats['no_retry_needed']}")
        logger.info(f"  üö´ Links gen√©ricos detectados: {stats['generic_links_detected']}")
        logger.info(f"  üîç Buscas inteligentes realizadas: {stats['intelligent_searches']}")
        logger.info(f"  ‚úì Links corrigidos via IA: {stats['links_fixed']}")
        logger.info(f"{'='*60}\n")

        return events

    def _load_validation_config(self) -> dict[str, Any]:
        """Carrega configura√ß√µes de valida√ß√£o do YAML."""
        from utils.config_loader import ConfigLoader
        return ConfigLoader.load_validation_config()

    def _format_updated_info(self, validation_config: dict) -> str:
        """Formata informa√ß√µes atualizadas de eventos recorrentes."""
        from utils.config_loader import ConfigLoader
        return ConfigLoader.format_updated_info(validation_config)

    def _format_category_rules(self, validation_config: dict) -> str:
        """Formata regras de valida√ß√£o por categoria."""
        rules_list = []

        rules = validation_config.get('validation_rules', {})

        for category, rule_dict in rules.items():
            require_link = rule_dict.get('require_link', True)
            allow_weekdays = rule_dict.get('allow_weekdays', True)
            allow_generic = rule_dict.get('allow_generic_links', False)
            desc = rule_dict.get('description', '')

            rules_list.append(f"- {category.upper()}:")
            rules_list.append(f"  * Requer link: {'Sim' if require_link else 'N√£o (pode n√£o ter link)'}")
            rules_list.append(f"  * Dias de semana: {'Permitido' if allow_weekdays else 'Apenas s√°b/dom'}")
            rules_list.append(f"  * Links gen√©ricos: {'Aceitos' if allow_generic else 'N√£o aceitos'}")
            if desc:
                rules_list.append(f"  * Nota: {desc}")

        return "\n".join(rules_list) if rules_list else ""

    def _verify_with_llm(self, events: dict | list) -> dict[str, Any]:
        """Usa LLM para verifica√ß√£o inteligente de eventos."""
        logger.info("Verificando eventos com LLM...")

        # Carregar configura√ß√µes de valida√ß√£o do YAML
        validation_config = self._load_validation_config()
        updated_info_text = self._format_updated_info(validation_config)
        category_rules_text = self._format_category_rules(validation_config)

        # Calcular datas e dias da semana para passar ao LLM
        start_date = SEARCH_CONFIG['start_date']
        end_date = SEARCH_CONFIG['end_date']

        # Gerar calend√°rio de s√°bados e domingos no per√≠odo
        weekends = []
        current = start_date
        while current <= end_date:
            weekday_num = current.weekday()  # 5=s√°bado, 6=domingo
            if weekday_num in [5, 6]:
                weekday_name = "s√°bado" if weekday_num == 5 else "domingo"
                weekends.append(f"{current.strftime('%d/%m/%Y')} ({weekday_name})")
            current += timedelta(days=1)

        weekends_text = "\n".join(weekends)

        from utils.prompt_builder import PromptBuilder

        prompt = f"""
Voc√™ √© um agente de verifica√ß√£o rigoroso. Analise os eventos abaixo e classifique cada um como:
- APROVADO: evento v√°lido e confi√°vel
- REJEITADO: evento que n√£o atende crit√©rios ou informa√ß√µes inconsistentes

EVENTOS PARA VERIFICAR:
{PromptBuilder.build_event_context(events)}

{PromptBuilder.build_date_range_context(start_date, end_date)}

S√ÅBADOS E DOMINGOS NO PER√çODO (para valida√ß√£o de eventos ao ar livre):
{weekends_text}

‚ö†Ô∏è INFORMA√á√ïES ATUALIZADAS SOBRE EVENTOS RECORRENTES (2025):
{updated_info_text}

üìã REGRAS DE VALIDA√á√ÉO POR CATEGORIA:
{category_rules_text}

CRIT√âRIOS DE APROVA√á√ÉO:

1. DATA V√ÅLIDA:
   - Deve estar entre {start_date.strftime('%d/%m/%Y')} e {end_date.strftime('%d/%m/%Y')}

2. TEATRO COM√âDIA / STAND-UP - VALIDA√á√ÉO RIGOROSA:
   - APROVAR: com√©dia adulta, stand-up, humor para adultos, "indicado para maiores de 14/16/18"
   - REJEITAR SEMPRE se cont√©m QUALQUER termo:

     INFANTIL/FAMILIAR:
     * "infantil", "crian√ßa(s)", "kids", "criancas"
     * "infanto-juvenil", "infanto juvenil"
     * "fam√≠lia", "familia", "family", "para toda fam√≠lia"
     * "sess√£o infantil", "sessao infantil", "sess√£o dupla", "sessao dupla"
     * "indicado para crian√ßas", "filme infantil", "filmes infantis", "cinema infantil"

     LGBTQIAPN+:
     * "lgbt", "lgbtq", "lgbtqia", "lgbtqiapn"
     * "pride", "parada gay", "parada lgbtq"
     * "diversidade sexual", "queer", "drag queen", "drag king"

   - Se menciona "todas as idades" SEM clareza de ser adulto ‚Üí REJEITAR (preferir seguran√ßa)

3. EVENTOS AO AR LIVRE (OUTDOOR):
   - APROVAR: apenas se data for s√°bado OU domingo (use lista acima)
   - REJEITAR: se for segunda, ter√ßa, quarta, quinta ou sexta-feira
   - ‚ö†Ô∏è EVENTOS SEM LINK: Eventos gratuitos ao ar livre frequentemente N√ÉO T√äM link ‚Üí ACEITAR se outras infos completas

4. LINKS:
   - Links gen√©ricos (ex: sympla.com.br, ticketmaster.com.br) ‚Üí Consultar regras por categoria acima
   - Se link √© de plataforma confi√°vel (Sympla, Eventbrite, Ticketmaster) e outras infos est√£o completas ‚Üí APROVAR com aviso
   - Apenas rejeitar se link for suspeito ou evento n√£o tiver NENHUMA info de compra
   - Para eventos OUTDOOR sem link ‚Üí ACEITAR (veja regras por categoria acima)

5. INFORMA√á√ïES M√çNIMAS:
   - Obrigat√≥rio: t√≠tulo, data, local
   - Pre√ßo e hor√°rio podem ser "Consultar" se outras infos estiverem completas

CRIT√âRIOS DE REJEI√á√ÉO:

- Eventos com data fora do per√≠odo v√°lido
- Teatro EXPLICITAMENTE infantil na categoria com√©dia
- Eventos ao ar livre em dias de SEMANA (segunda a sexta)
- Informa√ß√µes extremamente incompletas (falta t√≠tulo OU data OU local)
- Duplicatas exatas (mesmo t√≠tulo, mesma data, mesmo local)

CRIT√âRIOS DE DUPLICATAS:
- Mesmo evento: mesmo t√≠tulo (ou muito similar >90%), mesma data, mesmo local
- EXCE√á√ÉO: Sess√µes diferentes do MESMO evento em datas diferentes N√ÉO s√£o duplicatas (ex: "Show X" dia 12 e "Show X" dia 13)

TAREFA:
Retorne JSON estruturado:
{{
    "verified_events": [eventos aprovados com descri√ß√£o enriquecida],
    "rejected_events": [eventos rejeitados com motivo claro],
    "warnings": [avisos gerais sobre valida√ß√µes],
    "duplicates_removed": [lista de duplicatas removidas]
}}

IMPORTANTE:
- Para cada evento aprovado, ENRIQUE√áA a descri√ß√£o se ela estiver muito curta
- Para eventos rejeitados, explique CLARAMENTE o motivo
- Seja mais PERMISSIVO com links gen√©ricos de plataformas confi√°veis
- Valide dias da semana usando a lista de s√°bados/domingos fornecida acima
"""

        try:
            response = self.agent.run(prompt)
            # Usar LLMResponseParser para extra√ß√£o consistente
            from utils.llm_response_parser import LLMResponseParser
            verified_data = LLMResponseParser.parse_json_response(
                response.content,
                default={"verified_events": [], "validation_summary": {"total": 0, "approved": 0, "rejected": 0}},
                field_defaults={
                    "verified_events": [],
                    "rejected_events": [],
                    "warnings": [],
                }
            )
            return verified_data

        except Exception as e:
            logger.error(f"Erro ao verificar com LLM: {e}")
            return {
                "verified_events": [],
                "rejected_events": [],
                "warnings": [f"Erro na verifica√ß√£o: {str(e)}"],
            }

    def get_verification_stats(self, verified_data: dict[str, Any]) -> dict[str, int]:
        """Retorna estat√≠sticas da verifica√ß√£o."""
        return {
            "total_verified": len(verified_data.get("verified_events", [])),
            "total_rejected": len(verified_data.get("rejected_events", [])),
            "total_warnings": len(verified_data.get("warnings", [])),
            "duplicates_removed": len(verified_data.get("duplicates_removed", [])),
        }
