"""Agente de verifica√ß√£o e valida√ß√£o de eventos."""

import asyncio
import json
import logging
import re
from datetime import datetime, timedelta
from typing import Any
from urllib.parse import urlparse

import httpx
from bs4 import BeautifulSoup
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from config import (
    HTTP_TIMEOUT,
    MAX_RETRIES,
    SEARCH_CONFIG,
)
from utils.agent_factory import AgentFactory

logger = logging.getLogger(__name__)

# Prefixo para logs deste agente
LOG_PREFIX = "[VerifyAgent] ‚úîÔ∏è"

# Sites SPAs que sempre retornam 200 OK (requerem valida√ß√£o de conte√∫do)
SPA_DOMAINS = [
    'eleventickets.com',
    'eventbrite.com.br',
    'eventbrite.com',
]

# Padr√µes de URL v√°lidos por dom√≠nio
URL_PATTERNS = {
    'eleventickets.com': r'!/apresentacao/[a-f0-9]{40}$',  # Hash SHA1 de 40 chars hex (fragment sem #)
}


class VerifyAgent:
    """Agente respons√°vel por verificar e validar informa√ß√µes de eventos."""

    def __init__(self):
        self.log_prefix = "[VerifyAgent] ‚úîÔ∏è"

        self.agent = AgentFactory.create_agent(
            name="Event Verification Agent",
            model_type="important",  # GPT-5 - tarefa cr√≠tica (verifica√ß√£o rigorosa)
            description="Agente especializado em verificar e validar informa√ß√µes de eventos",
            instructions=[
                "Verificar se as datas dos eventos est√£o no per√≠odo correto "
                f"({SEARCH_CONFIG['start_date'].strftime('%d/%m/%Y')} a {SEARCH_CONFIG['end_date'].strftime('%d/%m/%Y')})",
                "Validar se os links de compra s√£o v√°lidos e acess√≠veis",
                "Identificar e remover eventos duplicados",
                "Confirmar se eventos de com√©dia n√£o s√£o infantis",
                "Verificar consist√™ncia de informa√ß√µes (data/hora/local/pre√ßo)",
                "Enriquecer descri√ß√µes quando necess√°rio",
                "Validar se eventos ao ar livre s√£o realmente em fim de semana",
                "Marcar eventos com baixa confiabilidade para revis√£o",
            ],
            markdown=True,
        )

    def _is_generic_link(self, url: str) -> bool:
        """Detecta se um link √© gen√©rico (p√°gina de busca/categoria/listagem).

        Args:
            url: URL a verificar

        Returns:
            True se o link for gen√©rico (n√£o espec√≠fico de um evento)
        """
        if not url or not isinstance(url, str):
            return False

        # EXCE√á√ïES: URLs conhecidas e confi√°veis (n√£o marcar como gen√©rico)
        # Estes venues t√™m apenas p√°gina de listagem ou links espec√≠ficos confi√°veis
        trusted_listing_pages = [
            'bluenoterio.com.br/shows',
            'eventim.com.br/artist/blue-note-rio',  # Aceita tanto /artist/ quanto /artist/blue-note-rio/event-name-id/
        ]

        for trusted in trusted_listing_pages:
            if trusted in url.lower():
                return False  # N√£o √© gen√©rico, √© confi√°vel

        # Padr√µes de URLs gen√©ricas
        generic_patterns = [
            r'/eventos/[^/]+\?',  # /eventos/categoria?params
            r'/eventos\?',         # /eventos?params
            r'/eventos/?$',        # /eventos ou /eventos/ no final
            r'/shows/?$',          # /shows ou /shows/ no final (Blue Note, etc)
            r'/agenda/?$',         # /agenda ou /agenda/ no final
            r'/programacao/?$',    # /programacao ou /programacao/ no final
            r'/calendar/?$',       # /calendar ou /calendar/ no final
            r'/schedule/?$',       # /schedule ou /schedule/ no final
            r'/busca\?',          # /busca?query=
            r'/search\?',         # /search?q=
            r'[?&]city=',         # query param de cidade
            r'[?&]partnership=',  # query param de partnership
            r'/d/brazil--',       # eventbrite listings
            r'/eventos/rio-de-janeiro',  # p√°ginas de listagem por cidade
            r'/events/rio-de-janeiro',   # p√°ginas de listagem por cidade
        ]

        for pattern in generic_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return True

        # Verificar se URL √© homepage (muito curta)
        # Ex: salaceliciameireles.com.br/ ou casadochoro.com.br/
        path = url.split('?')[0]  # Remover query params
        path_parts = [p for p in path.split('/') if p and p not in ['http:', 'https:', '']]

        # URL com apenas dom√≠nio (homepage) √© gen√©rica
        if len(path_parts) == 1:
            return True

        # URL com dom√≠nio + apenas 1 segmento gen√©rico tamb√©m √© gen√©rica
        # Ex: bluenoterio.com.br/shows (2 partes, mas shows √© gen√©rico)
        if len(path_parts) == 2:
            generic_segments = ['shows', 'eventos', 'events', 'agenda', 'programacao', 'calendar', 'schedule']
            last_segment = path_parts[-1].lower().rstrip('/')
            if last_segment in generic_segments:
                return True

        return False

    def _matches_url_pattern(self, url: str) -> bool:
        """Valida se URL corresponde ao padr√£o esperado para o dom√≠nio.

        Args:
            url: URL a validar

        Returns:
            True se URL corresponde ao padr√£o do dom√≠nio, False caso contr√°rio
        """
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()

            # Verificar se dom√≠nio tem padr√£o definido
            for pattern_domain, pattern in URL_PATTERNS.items():
                if pattern_domain in domain:
                    # Validar contra o padr√£o
                    full_path = parsed.path + parsed.fragment  # ElevenTickets usa fragment (#/...)
                    if re.search(pattern, full_path):
                        return True
                    else:
                        logger.warning(f"URL n√£o corresponde ao padr√£o esperado para {pattern_domain}: {url}")
                        return False

            # Dom√≠nio sem padr√£o definido = aceitar
            return True

        except Exception as e:
            logger.error(f"Erro ao validar padr√£o de URL: {e}")
            return True  # Em caso de erro, n√£o bloquear

    async def _validate_link_content(self, link: str, event: dict) -> dict:
        """Valida conte√∫do de link SPA verificando se informa√ß√µes do evento correspondem.

        Args:
            link: URL a validar
            event: Evento com informa√ß√µes esperadas

        Returns:
            dict com: valid (bool), reason (str), details (dict)
        """
        try:
            # Fetch HTML da p√°gina
            async with httpx.AsyncClient(timeout=HTTP_TIMEOUT, follow_redirects=True) as client:
                response = await client.get(link)

                if response.status_code != 200:
                    return {
                        "valid": False,
                        "reason": f"HTTP {response.status_code}",
                        "details": {}
                    }

                # Parse HTML
                soup = BeautifulSoup(response.text, 'html.parser')

                # Extrair texto vis√≠vel da p√°gina
                page_text = soup.get_text(separator=' ', strip=True).lower()

                # Informa√ß√µes do evento para validar
                titulo = (event.get("titulo") or event.get("nome", "")).lower()
                local = (event.get("local", "")).lower()

                # Valida√ß√µes de conte√∫do
                issues = []
                matches = []

                # Verificar t√≠tulo (pelo menos 60% das palavras)
                titulo_words = [w for w in titulo.split() if len(w) > 3]  # palavras > 3 chars
                if titulo_words:
                    titulo_matches = sum(1 for word in titulo_words if word in page_text)
                    titulo_match_ratio = titulo_matches / len(titulo_words)

                    if titulo_match_ratio >= 0.6:
                        matches.append(f"T√≠tulo encontrado ({titulo_match_ratio:.0%})")
                    else:
                        issues.append(f"T√≠tulo n√£o encontrado ({titulo_match_ratio:.0%} match)")

                # Verificar local (palavras principais)
                local_words = [w for w in local.split() if len(w) > 4]  # palavras > 4 chars
                if local_words:
                    local_matches = sum(1 for word in local_words if word in page_text)
                    local_match_ratio = local_matches / len(local_words) if local_words else 0

                    if local_match_ratio >= 0.5:
                        matches.append(f"Local encontrado ({local_match_ratio:.0%})")
                    else:
                        issues.append(f"Local n√£o encontrado ({local_match_ratio:.0%} match)")

                # Verificar se p√°gina tem indicadores de venda (bot√µes de compra)
                buy_indicators = ['comprar', 'ingresso', 'ticket', 'buy', 'cart', 'carrinho']
                has_buy_button = any(indicator in page_text for indicator in buy_indicators)

                if has_buy_button:
                    matches.append("Bot√£o de compra encontrado")
                else:
                    issues.append("Nenhum bot√£o de compra encontrado")

                # Decis√£o final
                valid = len(matches) >= 2 and len(issues) <= 1

                return {
                    "valid": valid,
                    "reason": "Conte√∫do validado" if valid else f"Valida√ß√£o falhou: {', '.join(issues)}",
                    "details": {
                        "matches": matches,
                        "issues": issues,
                        "titulo_match": titulo_match_ratio if titulo_words else None,
                        "local_match": local_match_ratio if local_words else None,
                    }
                }

        except Exception as e:
            logger.error(f"Erro ao validar conte√∫do do link: {e}")
            return {
                "valid": True,  # Em caso de erro, n√£o bloquear (pode ser problema tempor√°rio)
                "reason": f"Erro na valida√ß√£o: {str(e)}",
                "details": {}
            }

    async def verify_events(self, events_json: str) -> dict[str, Any]:
        """Verifica e valida eventos extra√≠dos pelo agente de busca."""
        logger.info(f"{self.log_prefix} Iniciando verifica√ß√£o de eventos...")

        try:
            events_data = json.loads(events_json) if isinstance(events_json, str) else events_json
        except json.JSONDecodeError:
            logger.error("Erro ao decodificar JSON de eventos")
            return {"verified_events": [], "rejected_events": [], "warnings": ["JSON inv√°lido"]}

        # Validar links em paralelo (valida√ß√£o b√°sica)
        events_with_link_validation = await self._validate_links(events_data)

        # Processar com LLM para verifica√ß√£o inteligente (primeira camada)
        verified_data = self._verify_with_llm(events_with_link_validation)

        # NOVA CAMADA: Valida√ß√£o individual rigorosa com ValidationAgent
        logger.info(f"{self.log_prefix} Iniciando valida√ß√£o individual rigorosa...")
        from agents.validation_agent import ValidationAgent

        validation_agent = ValidationAgent()
        individual_validation = await validation_agent.validate_events_batch(
            verified_data.get("verified_events", [])
        )

        # Combinar resultados
        final_verified = individual_validation["validated_events"]
        final_rejected = (
            verified_data.get("rejected_events", [])
            + individual_validation["rejected_events"]
        )
        final_warnings = (
            verified_data.get("warnings", [])
            + individual_validation["validation_warnings"]
        )

        logger.info(
            f"Verifica√ß√£o conclu√≠da. Eventos finais aprovados: {len(final_verified)} "
            f"(rejeitados na valida√ß√£o individual: {len(individual_validation['rejected_events'])})"
        )

        return {
            "verified_events": final_verified,
            "rejected_events": final_rejected,
            "warnings": final_warnings,
            "duplicates_removed": verified_data.get("duplicates_removed", []),
        }

    @retry(
        stop=stop_after_attempt(MAX_RETRIES),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((
            httpx.TimeoutException,
            httpx.ConnectError,
            httpx.ReadTimeout,
        )),
        reraise=True,
    )
    async def _validate_single_link(
        self, client: httpx.AsyncClient, link: str, event: dict = None, attempt_num: int = 1
    ) -> dict:
        """Valida um √∫nico link com retry autom√°tico para erros tempor√°rios.

        Args:
            client: Cliente HTTP ass√≠ncrono
            link: URL a validar
            event: Evento (opcional, necess√°rio para valida√ß√£o de conte√∫do SPA)
            attempt_num: N√∫mero da tentativa

        Returns:
            dict com: valid (bool), status_code (int), spa_validation (dict, opcional)
        """
        logger.info(f"Validando link (tentativa {attempt_num}): {link}")
        response = await client.head(link, timeout=HTTP_TIMEOUT)

        status_valid = 200 <= response.status_code < 400

        if not status_valid:
            return {
                "valid": False,
                "status_code": response.status_code,
            }

        # Se HTTP 200, verificar se √© SPA que precisa valida√ß√£o adicional
        parsed = urlparse(link)
        domain = parsed.netloc.lower()
        is_spa = any(spa_domain in domain for spa_domain in SPA_DOMAINS)

        if is_spa:
            logger.info(f"üîç Link SPA detectado ({domain}), aplicando valida√ß√£o adicional...")

            # 1. Validar padr√£o de URL
            pattern_valid = self._matches_url_pattern(link)

            if not pattern_valid:
                logger.warning(f"‚ùå Link SPA falhou valida√ß√£o de padr√£o: {link}")

                # 2. Tentar valida√ß√£o de conte√∫do se padr√£o falhar E temos dados do evento
                if event:
                    logger.info("‚Üí Tentando valida√ß√£o de conte√∫do...")
                    content_validation = await self._validate_link_content(link, event)

                    if content_validation["valid"]:
                        logger.info(f"‚úÖ Link SPA aprovado por valida√ß√£o de conte√∫do: {content_validation['reason']}")
                        return {
                            "valid": True,
                            "status_code": response.status_code,
                            "spa_validation": {
                                "type": "content",
                                "result": content_validation
                            }
                        }
                    else:
                        logger.warning(f"‚ùå Link SPA rejeitado: {content_validation['reason']}")
                        return {
                            "valid": False,
                            "status_code": response.status_code,
                            "spa_validation": {
                                "type": "content",
                                "result": content_validation
                            }
                        }
                else:
                    # Sem dados do evento, n√£o podemos validar conte√∫do
                    logger.warning("‚ùå Link SPA falhou valida√ß√£o de padr√£o e sem dados para validar conte√∫do")
                    return {
                        "valid": False,
                        "status_code": response.status_code,
                        "spa_validation": {
                            "type": "pattern",
                            "reason": "URL n√£o corresponde ao padr√£o esperado"
                        }
                    }
            else:
                logger.info("‚úÖ Link SPA aprovado por padr√£o de URL")
                return {
                    "valid": True,
                    "status_code": response.status_code,
                    "spa_validation": {
                        "type": "pattern",
                        "reason": "URL corresponde ao padr√£o esperado"
                    }
                }

        # Link n√£o-SPA, valida√ß√£o HTTP √© suficiente
        return {
            "valid": status_valid,
            "status_code": response.status_code,
        }

    async def _intelligent_link_search(self, event: dict, attempt: int = 1) -> dict[str, Any]:
        """Usa Perplexity para buscar o link correto de um evento e valida o conte√∫do.

        Returns:
            dict com: link (str), quality_score (int), validation (dict) ou None se n√£o encontrar
        """
        from config import LINK_MAX_INTELLIGENT_SEARCHES, LINK_QUALITY_THRESHOLD

        if attempt > LINK_MAX_INTELLIGENT_SEARCHES:
            logger.warning(f"{self.log_prefix} Limite de {LINK_MAX_INTELLIGENT_SEARCHES} tentativas atingido para: {event.get('titulo')}")
            return None

        titulo = event.get("titulo", "") or event.get("nome", "")
        data = event.get("data", "")
        horario = event.get("horario", "")
        local = event.get("local", "")
        categoria = event.get("categoria", "")
        preco = event.get("preco", "")
        descricao = event.get("descricao_enriquecida") or event.get("descricao", "")
        fontes = event.get("fontes", [])

        logger.info(f"{self.log_prefix} Buscando link correto (tentativa {attempt}/{LINK_MAX_INTELLIGENT_SEARCHES}): {titulo}")

        # Criar agente de busca com Perplexity
        search_agent = Agent(
            name="Link Search Agent",
            model=OpenAIChat(
                id=MODELS["search"],  # perplexity/sonar-pro
                api_key=OPENROUTER_API_KEY,
                base_url=OPENROUTER_BASE_URL,
            ),
            description="Agente especializado em encontrar links oficiais de eventos",
            instructions=[
                "Buscar apenas links OFICIAIS de venda/informa√ß√µes",
                "Priorizar Sympla, Eventbrite, Ticketmaster, site do venue",
                "N√ÉO retornar links gen√©ricos de homepage",
                "Retornar APENAS o URL ou 'NONE' se n√£o encontrar",
            ],
        )

        # Construir prompt com todas as informa√ß√µes dispon√≠veis
        fonte_info = f"\nFontes mencionadas: {', '.join(fontes[:3])}" if fontes else ""
        horario_info = f"\nHor√°rio: {horario}" if horario else ""
        categoria_info = f"\nCategoria: {categoria}" if categoria else ""
        preco_info = f"\nPre√ßo: {preco}" if preco else ""

        # Se √© retry, adicionar contexto do problema anterior
        retry_context = ""
        if attempt > 1:
            retry_context = "\n\n‚ö†Ô∏è TENTATIVA ANTERIOR RETORNOU LINK DE BAIXA QUALIDADE. Por favor, busque um link MAIS ESPEC√çFICO que contenha:\n- T√≠tulo EXATO do evento\n- Data espec√≠fica\n- Nomes dos artistas/m√∫sicos (se houver)\n- Bot√£o de compra de ingresso"

        prompt = f"""Encontre o link de compra/informa√ß√µes OFICIAL para este evento no Rio de Janeiro:

T√≠tulo: {titulo}
Data: {data}{horario_info}
Local: {local}{categoria_info}{preco_info}
Descri√ß√£o: {descricao[:200]}{fonte_info}{retry_context}

IMPORTANTE:
- Busque o link ESPEC√çFICO deste evento, n√£o a p√°gina principal do local
- Priorize plataformas de venda: Sympla, Eventbrite, Ticketmaster
- Se n√£o encontrar em plataformas, busque no site oficial do venue
- Valide que a data e t√≠tulo correspondem ao evento solicitado

Busque em:
- Sympla (sympla.com.br) - busque pelo t√≠tulo exato
- Eventbrite (eventbrite.com.br) - busque pelo t√≠tulo exato
- Ticketmaster Brasil
- Site oficial do venue/local (ex: bluenoterio.com.br, casadochoro.com.br)
- Instagram oficial do evento/local (apenas se tiver link de venda)

Retorne APENAS o URL completo e v√°lido (come√ßando com http:// ou https://), ou "NONE" se n√£o encontrar nada confi√°vel.
N√ÉO retorne:
- Links gen√©ricos de homepage
- Agregadores de eventos
- Links quebrados
- P√°ginas de busca"""

        try:
            response = search_agent.run(prompt)
            new_link = response.content.strip()

            # Validar resposta
            if new_link and new_link != "NONE" and new_link.startswith("http"):
                logger.info(f"{self.log_prefix} Link encontrado: {new_link}")

                # Verificar se link √© gen√©rico (p√°gina de listagem)
                if self._is_generic_link(new_link):
                    logger.warning(f"{self.log_prefix} ‚ùå Link gen√©rico detectado: {new_link}")

                    # Retry se ainda tiver tentativas
                    if attempt < LINK_MAX_INTELLIGENT_SEARCHES:
                        logger.info(f"{self.log_prefix} Tentando busca novamente solicitando link ESPEC√çFICO...")
                        return await self._intelligent_link_search(event, attempt + 1)
                    else:
                        logger.warning(f"{self.log_prefix} Todas tentativas esgotadas. Link gen√©rico rejeitado.")
                        return None

                # NOVO: Validar qualidade do link encontrado
                try:
                    from agents.validation_agent import ValidationAgent

                    validation_agent = ValidationAgent()
                    link_info = await validation_agent._fetch_link_info(new_link, event)

                    quality_validation = link_info.get("quality_validation")

                    if quality_validation:
                        score = quality_validation["score"]
                        is_quality = quality_validation["is_quality"]

                        if is_quality:
                            logger.info(f"{self.log_prefix} ‚úÖ Link aprovado (score: {score}/100)")
                            return {
                                "link": new_link,
                                "quality_score": score,
                                "validation": quality_validation,
                                "structured_data": link_info.get("structured_data", {}),
                            }
                        else:
                            logger.warning(
                                f"{self.log_prefix} ‚ùå Link rejeitado (score: {score}/{LINK_QUALITY_THRESHOLD})"
                            )
                            logger.warning(f"{self.log_prefix} Issues: {', '.join(quality_validation['issues'])}")

                            # Retry se ainda tiver tentativas
                            if attempt < LINK_MAX_INTELLIGENT_SEARCHES:
                                logger.info(f"{self.log_prefix} Tentando busca novamente com crit√©rios mais rigorosos...")
                                return await self._intelligent_link_search(event, attempt + 1)
                            else:
                                logger.warning(f"{self.log_prefix} Todas tentativas esgotadas. Retornando link mesmo com baixa qualidade.")
                                return {
                                    "link": new_link,
                                    "quality_score": score,
                                    "validation": quality_validation,
                                    "low_quality": True,
                                }
                    else:
                        # Sem valida√ß√£o de qualidade (erro), aceitar link
                        logger.warning(f"{self.log_prefix} Valida√ß√£o de qualidade falhou, aceitando link")
                        return {"link": new_link, "quality_score": None, "validation": None}

                except Exception as e:
                    logger.error(f"{self.log_prefix} Erro ao validar qualidade do link: {e}")
                    # Em caso de erro, retornar link sem valida√ß√£o
                    return {"link": new_link, "quality_score": None, "validation": None}

            else:
                logger.warning(f"{self.log_prefix} ‚úó Nenhum link v√°lido encontrado para: {titulo}")
                return None

        except Exception as e:
            logger.error(f"{self.log_prefix} Erro na busca inteligente de link: {e}")
            return None

    async def _validate_single_event_link(
        self, event: dict, client: httpx.AsyncClient
    ) -> dict:
        """Valida o link de um √∫nico evento com retry e busca inteligente.

        Args:
            event: Evento a validar
            client: Cliente HTTP ass√≠ncrono compartilhado

        Returns:
            Dicion√°rio com estat√≠sticas da valida√ß√£o deste evento
        """
        stats = {
            "total_links": 0,
            "validated_first_try": 0,
            "failed_all_retries": 0,
            "intelligent_searches": 0,
            "links_fixed": 0,
            "no_retry_needed": 0,
            "generic_links_detected": 0,
        }

        link = event.get("link") or event.get("link_ingresso") or event.get("ticket_link")

        if not link:
            logger.info(f"‚Üí Evento sem link, iniciando busca inteligente: {event.get('titulo')}")
            stats["total_links"] += 1
            stats["intelligent_searches"] += 1

            link_result = await self._intelligent_link_search(event)

            if link_result and link_result.get("link"):
                new_link = link_result["link"]
                event["link"] = new_link
                event["link_updated_by_ai"] = True
                event["link_added_by_ai"] = True  # Novo campo para indicar que foi adicionado (n√£o apenas corrigido)
                event["link_quality_score"] = link_result.get("quality_score")
                event["link_quality_validation"] = link_result.get("validation")

                # Armazenar dados estruturados extra√≠dos do link
                if link_result.get("structured_data"):
                    event["link_structured_data"] = link_result["structured_data"]

                # Link j√° foi validado no _intelligent_link_search
                event["link_valid"] = True
                event["link_status_code"] = 200
                stats["links_fixed"] += 1
                logger.info(f"‚úì Link adicionado com sucesso: {new_link}")
            else:
                event["link_valid"] = None
                event["link_error"] = "Nenhum link encontrado via busca inteligente"
                event["requires_manual_link_check"] = True
                logger.warning(f"‚ö† Nenhum link encontrado para: {event.get('titulo')}")

            return stats

        # Detectar placeholder "INCOMPLETO" e ir direto para busca inteligente
        if link in ["INCOMPLETO", "incompleto", "/INCOMPLETO", "NONE", "none"]:
            logger.info(f"‚Üí Link placeholder detectado ({link}), iniciando busca inteligente: {event.get('titulo')}")
            stats["total_links"] += 1
            stats["intelligent_searches"] += 1

            link_result = await self._intelligent_link_search(event)

            if link_result and link_result.get("link"):
                new_link = link_result["link"]
                event["link_original"] = link
                event["link"] = new_link
                event["link_updated_by_ai"] = True
                event["link_quality_score"] = link_result.get("quality_score")
                event["link_quality_validation"] = link_result.get("validation")

                # Armazenar dados estruturados extra√≠dos do link
                if link_result.get("structured_data"):
                    event["link_structured_data"] = link_result["structured_data"]

                # Link j√° foi validado no _intelligent_link_search
                event["link_valid"] = True
                event["link_status_code"] = 200
                stats["links_fixed"] += 1
                logger.info(f"‚úì Link corrigido com sucesso: {new_link}")
            else:
                event["link_valid"] = False
                event["link_error"] = "Placeholder sem link v√°lido encontrado"
                event["requires_manual_link_check"] = True
                logger.warning(f"‚ö† Nenhum link encontrado para: {event.get('titulo')}")

            return stats

        # Detectar link gen√©rico (p√°gina de busca/categoria) e ir para busca inteligente
        if self._is_generic_link(link):
            logger.info(f"üö´ Link gen√©rico detectado, iniciando busca inteligente: {link[:80]}...")
            stats["total_links"] += 1
            stats["generic_links_detected"] += 1
            stats["intelligent_searches"] += 1

            link_result = await self._intelligent_link_search(event)

            if link_result and link_result.get("link"):
                new_link = link_result["link"]
                event["link_original"] = link
                event["link"] = new_link
                event["link_updated_by_ai"] = True
                event["link_was_generic"] = True
                event["link_quality_score"] = link_result.get("quality_score")
                event["link_quality_validation"] = link_result.get("validation")

                # Armazenar dados estruturados extra√≠dos do link
                if link_result.get("structured_data"):
                    event["link_structured_data"] = link_result["structured_data"]

                # Link j√° foi validado no _intelligent_link_search
                event["link_valid"] = True
                event["link_status_code"] = 200
                stats["links_fixed"] += 1
                logger.info(f"‚úì Link gen√©rico substitu√≠do por link espec√≠fico: {new_link}")
            else:
                # Nenhum link espec√≠fico encontrado, manter gen√©rico mas marcar
                event["link_valid"] = False
                event["link_is_generic"] = True
                event["link_error"] = "Link gen√©rico - p√°gina de busca/categoria"
                event["requires_manual_link_check"] = True
                logger.warning(f"‚ö† Nenhum link espec√≠fico encontrado para: {event.get('titulo')}")

            return stats

        stats["total_links"] += 1
        original_link = link

        # EXCE√á√ÉO 1: Links do Eventim n√£o respondem bem a HEAD requests
        if 'eventim.com.br/artist/blue-note-rio/' in link.lower():
            event["link_valid"] = True
            event["link_status_code"] = 200
            event["validation_skipped"] = "Eventim links are trusted (HEAD requests not supported)"
            stats["validated_first_try"] += 1
            logger.info(f"‚úì Link Eventim v√°lido (sem valida√ß√£o HTTP): {link}")
            return stats

        # EXCE√á√ÉO 2: Links oficiais da Sala Cec√≠lia Meireles (.gov.br)
        if 'salaceciliameireles.rj.gov.br/programacao/' in link.lower():
            event["link_valid"] = True
            event["link_status_code"] = 200
            event["validation_skipped"] = "Official Sala Cec√≠lia Meireles links are trusted"
            stats["validated_first_try"] += 1
            logger.info(f"‚úì Link oficial Sala Cec√≠lia v√°lido (sem valida√ß√£o HTTP): {link}")
            return stats

        try:
            # Tentar validar com retry autom√°tico (passando evento para valida√ß√£o SPA)
            result = await self._validate_single_link(client, link, event=event, attempt_num=1)
            event["link_valid"] = result["valid"]
            event["link_status_code"] = result["status_code"]

            # Adicionar informa√ß√µes de valida√ß√£o SPA se presente
            if "spa_validation" in result:
                event["spa_validation"] = result["spa_validation"]

            stats["validated_first_try"] += 1
            logger.info(f"‚úì Link v√°lido: {link} (status: {result['status_code']})")

        except Exception as e:
            # Verificar se √© erro que n√£o deve ter retry (404, 403, etc)
            if isinstance(e, httpx.HTTPStatusError):
                if e.response.status_code in [404, 403, 401, 410]:
                    # Erros permanentes - n√£o fazer retry
                    event["link_valid"] = False
                    event["link_status_code"] = e.response.status_code
                    event["link_error"] = f"HTTP {e.response.status_code}"
                    stats["no_retry_needed"] += 1
                    logger.warning(f"‚úó Link com erro permanente: {link} ({e.response.status_code})")

                    # Ir direto para busca inteligente
                    stats["intelligent_searches"] += 1
                    link_result = await self._intelligent_link_search(event)

                    if link_result and link_result.get("link") and link_result["link"] != original_link:
                        new_link = link_result["link"]
                        event["link_original"] = original_link
                        event["link"] = new_link
                        event["link_updated_by_ai"] = True
                        event["link_quality_score"] = link_result.get("quality_score")
                        event["link_quality_validation"] = link_result.get("validation")

                        # Armazenar dados estruturados extra√≠dos do link
                        if link_result.get("structured_data"):
                            event["link_structured_data"] = link_result["structured_data"]

                        # Link j√° foi validado no _intelligent_link_search
                        event["link_valid"] = True
                        event["link_status_code"] = 200
                        stats["links_fixed"] += 1
                        logger.info(f"‚úì Link corrigido com sucesso: {new_link}")

                    return stats

            # Todas as tentativas de retry falharam (timeout, connection error, etc)
            logger.warning(f"‚úó Todas as {MAX_RETRIES} tentativas falharam para: {link}")
            logger.warning(f"   Erro: {type(e).__name__}: {e}")

            event["link_valid"] = False
            event["link_error"] = f"{type(e).__name__}: {str(e)}"
            event["link_validation_failed"] = True
            stats["failed_all_retries"] += 1

            # Tentar busca inteligente como √∫ltimo recurso
            logger.info(f"‚Üí Tentando busca inteligente para: {event.get('titulo')}")
            stats["intelligent_searches"] += 1

            link_result = await self._intelligent_link_search(event)

            if link_result and link_result.get("link") and link_result["link"] != original_link:
                new_link = link_result["link"]
                event["link_original"] = original_link
                event["link"] = new_link
                event["link_updated_by_ai"] = True
                event["link_quality_score"] = link_result.get("quality_score")
                event["link_quality_validation"] = link_result.get("validation")

                # Armazenar dados estruturados extra√≠dos do link
                if link_result.get("structured_data"):
                    event["link_structured_data"] = link_result["structured_data"]

                # Link j√° foi validado no _intelligent_link_search
                event["link_valid"] = True
                event["link_status_code"] = 200
                stats["links_fixed"] += 1
                logger.info(f"‚úì Link corrigido com sucesso: {new_link}")
            else:
                # Marcar para revis√£o manual
                event["requires_manual_link_check"] = True
                logger.warning(f"‚ö† Evento requer revis√£o manual de link: {event.get('titulo')}")

        return stats

    async def _validate_links(self, events: dict | list) -> dict | list:
        """Valida se os links de eventos s√£o acess√≠veis com retry e busca inteligente (paralelizado)."""
        logger.info(f"{self.log_prefix} Validando links de eventos em paralelo com retry autom√°tico...")

        # Extrair eventos da estrutura complexa do structured_events.json
        event_list = []
        if isinstance(events, dict):
            # Estrutura: {"eventos_gerais": {"eventos": [...]}, "eventos_locais_especiais": {...}}
            if "eventos_gerais" in events:
                event_list.extend(events["eventos_gerais"].get("eventos", []))

            if "eventos_locais_especiais" in events:
                for local_name, local_events in events["eventos_locais_especiais"].items():
                    if isinstance(local_events, list):
                        # Filtra eventos reais (apenas dicts, ignora __checagem e outros tipos)
                        event_list.extend([e for e in local_events if isinstance(e, dict) and "__checagem" not in e])

            # Fallback para estrutura simples
            if not event_list:
                event_list = events.get("events", [])
        else:
            event_list = events

        # Estat√≠sticas agregadas
        stats = {
            "total_links": 0,
            "validated_first_try": 0,
            "validated_after_retry": 0,
            "failed_all_retries": 0,
            "intelligent_searches": 0,
            "links_fixed": 0,
            "no_retry_needed": 0,
            "generic_links_detected": 0,
        }

        # Validar todos os links em paralelo
        async with httpx.AsyncClient(timeout=HTTP_TIMEOUT, follow_redirects=True) as client:
            # Criar tasks para validar todos os eventos em paralelo
            validation_tasks = [
                self._validate_single_event_link(event, client)
                for event in event_list
            ]

            # Executar todas as valida√ß√µes em paralelo
            logger.info(f"Iniciando valida√ß√£o paralela de {len(event_list)} links...")
            validation_results = await asyncio.gather(*validation_tasks, return_exceptions=True)

            # Agregar estat√≠sticas
            for result in validation_results:
                if isinstance(result, dict):
                    for key in stats:
                        stats[key] += result.get(key, 0)

        # Log de estat√≠sticas
        logger.info(f"\n{'='*60}")
        logger.info("üìä Estat√≠sticas de Valida√ß√£o de Links:")
        logger.info(f"  Total de links verificados: {stats['total_links']}")
        logger.info(f"  ‚úì Validados na 1¬™ tentativa: {stats['validated_first_try']}")
        logger.info(f"  ‚úó Falharam ap√≥s todos os retries: {stats['failed_all_retries']}")
        logger.info(f"  ‚úó Erros permanentes (404, 403, etc): {stats['no_retry_needed']}")
        logger.info(f"  üö´ Links gen√©ricos detectados: {stats['generic_links_detected']}")
        logger.info(f"  üîç Buscas inteligentes realizadas: {stats['intelligent_searches']}")
        logger.info(f"  ‚úì Links corrigidos via IA: {stats['links_fixed']}")
        logger.info(f"{'='*60}\n")

        return events

    def _verify_with_llm(self, events: dict | list) -> dict[str, Any]:
        """Usa LLM para verifica√ß√£o inteligente de eventos."""
        logger.info("Verificando eventos com LLM...")

        # Calcular datas e dias da semana para passar ao LLM
        start_date = SEARCH_CONFIG['start_date']
        end_date = SEARCH_CONFIG['end_date']

        # Gerar calend√°rio de s√°bados e domingos no per√≠odo
        weekends = []
        current = start_date
        while current <= end_date:
            weekday_num = current.weekday()  # 5=s√°bado, 6=domingo
            if weekday_num in [5, 6]:
                weekday_name = "s√°bado" if weekday_num == 5 else "domingo"
                weekends.append(f"{current.strftime('%d/%m/%Y')} ({weekday_name})")
            current += timedelta(days=1)

        weekends_text = "\n".join(weekends)

        prompt = f"""
Voc√™ √© um agente de verifica√ß√£o rigoroso. Analise os eventos abaixo e classifique cada um como:
- APROVADO: evento v√°lido e confi√°vel
- REJEITADO: evento que n√£o atende crit√©rios ou informa√ß√µes inconsistentes

EVENTOS PARA VERIFICAR:
{json.dumps(events, indent=2, ensure_ascii=False)}

PER√çODO V√ÅLIDO: {start_date.strftime('%d/%m/%Y')} a {end_date.strftime('%d/%m/%Y')}

S√ÅBADOS E DOMINGOS NO PER√çODO (para valida√ß√£o de eventos ao ar livre):
{weekends_text}

CRIT√âRIOS DE APROVA√á√ÉO:

1. DATA V√ÅLIDA:
   - Deve estar entre {start_date.strftime('%d/%m/%Y')} e {end_date.strftime('%d/%m/%Y')}

2. TEATRO COM√âDIA / STAND-UP:
   - APROVAR: com√©dia adulta, stand-up, humor para adultos, "indicado para maiores de 14/16/18"
   - REJEITAR: apenas se EXPLICITAMENTE infantil ("teatro infantil", "para crian√ßas", "kids", "fam√≠lia")
   - DICA: Se diz "todas as idades mas voltado ao p√∫blico adulto" ‚Üí APROVAR (n√£o √© infantil)

3. EVENTOS AO AR LIVRE:
   - APROVAR: apenas se data for s√°bado OU domingo (use lista acima)
   - REJEITAR: se for segunda, ter√ßa, quarta, quinta ou sexta-feira

4. LINKS:
   - Links gen√©ricos (ex: sympla.com.br, ticketmaster.com.br) ‚Üí N√ÉO rejeitar automaticamente
   - Se link √© de plataforma confi√°vel (Sympla, Eventbrite, Ticketmaster) e outras infos est√£o completas ‚Üí APROVAR com aviso
   - Apenas rejeitar se link for suspeito ou evento n√£o tiver NENHUMA info de compra

5. INFORMA√á√ïES M√çNIMAS:
   - Obrigat√≥rio: t√≠tulo, data, local
   - Pre√ßo e hor√°rio podem ser "Consultar" se outras infos estiverem completas

CRIT√âRIOS DE REJEI√á√ÉO:

- Eventos com data fora do per√≠odo v√°lido
- Teatro EXPLICITAMENTE infantil na categoria com√©dia
- Eventos ao ar livre em dias de SEMANA (segunda a sexta)
- Informa√ß√µes extremamente incompletas (falta t√≠tulo OU data OU local)
- Duplicatas exatas (mesmo t√≠tulo, mesma data, mesmo local)

CRIT√âRIOS DE DUPLICATAS:
- Mesmo evento: mesmo t√≠tulo (ou muito similar >90%), mesma data, mesmo local
- EXCE√á√ÉO: Sess√µes diferentes do MESMO evento em datas diferentes N√ÉO s√£o duplicatas (ex: "Show X" dia 12 e "Show X" dia 13)

TAREFA:
Retorne JSON estruturado:
{{
    "verified_events": [eventos aprovados com descri√ß√£o enriquecida],
    "rejected_events": [eventos rejeitados com motivo claro],
    "warnings": [avisos gerais sobre valida√ß√µes],
    "duplicates_removed": [lista de duplicatas removidas]
}}

IMPORTANTE:
- Para cada evento aprovado, ENRIQUE√áA a descri√ß√£o se ela estiver muito curta
- Para eventos rejeitados, explique CLARAMENTE o motivo
- Seja mais PERMISSIVO com links gen√©ricos de plataformas confi√°veis
- Valide dias da semana usando a lista de s√°bados/domingos fornecida acima
"""

        try:
            response = self.agent.run(prompt)
            content = response.content

            # Tentar extrair JSON da resposta
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            verified_data = json.loads(content)
            return verified_data

        except Exception as e:
            logger.error(f"Erro ao verificar com LLM: {e}")
            return {
                "verified_events": [],
                "rejected_events": [],
                "warnings": [f"Erro na verifica√ß√£o: {str(e)}"],
            }

    def get_verification_stats(self, verified_data: dict[str, Any]) -> dict[str, int]:
        """Retorna estat√≠sticas da verifica√ß√£o."""
        return {
            "total_verified": len(verified_data.get("verified_events", [])),
            "total_rejected": len(verified_data.get("rejected_events", [])),
            "total_warnings": len(verified_data.get("warnings", [])),
            "duplicates_removed": len(verified_data.get("duplicates_removed", [])),
        }
