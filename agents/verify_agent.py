"""Agente de verifica√ß√£o e valida√ß√£o de eventos."""

import asyncio
import difflib
import json
import logging
import re
from datetime import datetime, timedelta
from typing import Any
from urllib.parse import urlparse

from bs4 import BeautifulSoup
from config import (
    HTTP_TIMEOUT,
    LINK_VALIDATION_MAX_CONCURRENT,
    MAX_RETRIES,
    SEARCH_CONFIG,
)
from utils.agent_factory import AgentFactory
from utils.http_client import HttpClientWrapper

logger = logging.getLogger(__name__)

# Prefixo para logs deste agente
LOG_PREFIX = "[VerifyAgent] ‚úîÔ∏è"

# Sites SPAs que sempre retornam 200 OK (requerem valida√ß√£o de conte√∫do)
SPA_DOMAINS = [
    'eleventickets.com',
    'eventbrite.com.br',
    'eventbrite.com',
]

# Padr√µes de URL v√°lidos por dom√≠nio
URL_PATTERNS = {
    'eleventickets.com': r'!/apresentacao/[a-f0-9]{40}$',  # Hash SHA1 de 40 chars hex (fragment sem #)
}


class VerifyAgent:
    """Agente respons√°vel por verificar e validar informa√ß√µes de eventos."""

    def __init__(self):
        self.log_prefix = "[VerifyAgent] ‚úîÔ∏è"
        self.http_client = HttpClientWrapper()

        self.agent = AgentFactory.create_agent(
            name="Event Verification Agent",
            model_type="important",  # GPT-5 - tarefa cr√≠tica (verifica√ß√£o rigorosa)
            description="Agente especializado em verificar e validar informa√ß√µes de eventos",
            instructions=[
                "Verificar se as datas dos eventos est√£o no per√≠odo correto "
                f"({SEARCH_CONFIG['start_date'].strftime('%d/%m/%Y')} a {SEARCH_CONFIG['end_date'].strftime('%d/%m/%Y')})",
                "Validar se os links de compra s√£o v√°lidos e acess√≠veis",
                "Identificar e remover eventos duplicados",
                "Confirmar se eventos de com√©dia n√£o s√£o infantis",
                "Verificar consist√™ncia de informa√ß√µes (data/hora/local/pre√ßo)",
                "Enriquecer descri√ß√µes quando necess√°rio",
                "Validar se eventos ao ar livre s√£o realmente em fim de semana",
                "Marcar eventos com baixa confiabilidade para revis√£o",
            ],
            markdown=True,
        )

    def _is_generic_link(self, url: str) -> bool:
        """Detecta se um link √© gen√©rico (p√°gina de busca/categoria/listagem).

        Args:
            url: URL a verificar

        Returns:
            True se o link for gen√©rico (n√£o espec√≠fico de um evento)
        """
        if not url or not isinstance(url, str):
            return False

        # EXCE√á√ïES: URLs conhecidas e confi√°veis (n√£o marcar como gen√©rico)
        # Estes venues t√™m apenas p√°gina de listagem ou links espec√≠ficos confi√°veis
        trusted_listing_pages = [
            'bluenoterio.com.br/shows',
            'eventim.com.br/artist/blue-note-rio',  # Aceita tanto /artist/ quanto /artist/blue-note-rio/event-name-id/
        ]

        for trusted in trusted_listing_pages:
            if trusted in url.lower():
                return False  # N√£o √© gen√©rico, √© confi√°vel

        # Padr√µes de URLs gen√©ricas
        generic_patterns = [
            r'/eventos/[^/]+\?',  # /eventos/categoria?params
            r'/eventos\?',         # /eventos?params
            r'/eventos/?$',        # /eventos ou /eventos/ no final
            r'/shows/?$',          # /shows ou /shows/ no final (Blue Note, etc)
            r'/agenda/?$',         # /agenda ou /agenda/ no final
            r'/programacao/?$',    # /programacao ou /programacao/ no final
            r'/calendar/?$',       # /calendar ou /calendar/ no final
            r'/schedule/?$',       # /schedule ou /schedule/ no final
            r'/busca\?',          # /busca?query=
            r'/search\?',         # /search?q=
            r'[?&]city=',         # query param de cidade
            r'[?&]partnership=',  # query param de partnership
            r'/d/brazil--',       # eventbrite listings
            r'/eventos/rio-de-janeiro',  # p√°ginas de listagem por cidade
            r'/events/rio-de-janeiro',   # p√°ginas de listagem por cidade
        ]

        for pattern in generic_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return True

        # Verificar se URL √© homepage (muito curta)
        # Ex: salaceliciameireles.com.br/ ou casadochoro.com.br/
        path = url.split('?')[0]  # Remover query params
        path_parts = [p for p in path.split('/') if p and p not in ['http:', 'https:', '']]

        # URL com apenas dom√≠nio (homepage) √© gen√©rica
        if len(path_parts) == 1:
            return True

        # URL com dom√≠nio + apenas 1 segmento gen√©rico tamb√©m √© gen√©rica
        # Ex: bluenoterio.com.br/shows (2 partes, mas shows √© gen√©rico)
        if len(path_parts) == 2:
            generic_segments = ['shows', 'eventos', 'events', 'agenda', 'programacao', 'calendar', 'schedule']
            last_segment = path_parts[-1].lower().rstrip('/')
            if last_segment in generic_segments:
                return True

        return False

    def _find_consensus(self, links: list[str], event: dict) -> dict[str, Any] | None:
        """Encontra consenso entre m√∫ltiplos links retornados por diferentes buscas.

        Args:
            links: Lista de links encontrados (pode conter duplicatas e None)
            event: Dados do evento (para logging)

        Returns:
            dict com link consensual e metadados, ou None se n√£o houver consenso
        """
        from collections import Counter
        from config import LINK_CONSENSUS_THRESHOLD, LINK_CONSENSUS_USE_GPT5_TIEBREAKER

        titulo = event.get("titulo", "")

        # Filtrar links v√°lidos (remover None, vazios, "NONE")
        valid_links = [
            link.strip()
            for link in links
            if link and isinstance(link, str) and link.strip() and link.strip().upper() != "NONE"
        ]

        if not valid_links:
            logger.warning(f"{self.log_prefix} Nenhum link v√°lido para consenso: {titulo}")
            return None

        # Normalizar URLs (remover trailing slashes, query params opcionais)
        def normalize_url(url: str) -> str:
            """Normaliza URL para compara√ß√£o (mant√©m path e dom√≠nio)."""
            # Remove trailing slash
            normalized = url.rstrip('/')
            # Remove fragmentos (#)
            normalized = normalized.split('#')[0]
            return normalized

        normalized_links = [normalize_url(link) for link in valid_links]

        # Contar ocorr√™ncias
        link_counts = Counter(normalized_links)
        total_searches = len(links)  # Total de buscas (incluindo falhas)
        threshold = LINK_CONSENSUS_THRESHOLD

        logger.info(f"{self.log_prefix} Consenso de links para '{titulo}':")
        logger.info(f"{self.log_prefix}   Total de buscas: {total_searches}")
        logger.info(f"{self.log_prefix}   Links v√°lidos: {len(valid_links)}")
        logger.info(f"{self.log_prefix}   Links √∫nicos: {len(link_counts)}")

        # Encontrar link com maior frequ√™ncia
        most_common_link, count = link_counts.most_common(1)[0]
        consensus_ratio = count / total_searches

        logger.info(f"{self.log_prefix}   Link mais comum: {most_common_link}")
        logger.info(f"{self.log_prefix}   Apari√ß√µes: {count}/{total_searches} ({consensus_ratio:.1%})")

        # Verificar se atingiu threshold
        if consensus_ratio >= threshold:
            logger.info(f"{self.log_prefix} ‚úÖ CONSENSO ATINGIDO ({consensus_ratio:.1%} >= {threshold:.1%})")
            return {
                "link": most_common_link,
                "consensus_ratio": consensus_ratio,
                "votes": count,
                "total_votes": total_searches,
                "method": "majority_vote",
            }

        # EMPATE ou consenso insuficiente
        logger.warning(f"{self.log_prefix} ‚ö†Ô∏è CONSENSO INSUFICIENTE ({consensus_ratio:.1%} < {threshold:.1%})")

        # Se habilitado, usar GPT-5 Mini como tiebreaker
        if LINK_CONSENSUS_USE_GPT5_TIEBREAKER and len(link_counts) > 1:
            logger.info(f"{self.log_prefix} Usando GPT-5 Mini como desempate...")
            tiebreaker_link = self._tiebreaker_with_gpt5(list(link_counts.keys()), event)
            if tiebreaker_link:
                logger.info(f"{self.log_prefix} ‚úÖ DESEMPATE GPT-5: {tiebreaker_link}")
                return {
                    "link": tiebreaker_link,
                    "consensus_ratio": 0.0,  # N√£o teve consenso por voto
                    "votes": 0,
                    "total_votes": total_searches,
                    "method": "gpt5_tiebreaker",
                    "tiebreaker_candidates": list(link_counts.keys()),
                }

        # Sem consenso e sem desempate
        logger.warning(f"{self.log_prefix} ‚ùå Falha no consenso para: {titulo}")
        return None

    def _tiebreaker_with_gpt5(self, candidate_links: list[str], event: dict) -> str | None:
        """Usa GPT-5 Mini com web search para escolher o melhor link em caso de empate.

        Args:
            candidate_links: Links candidatos (2 ou mais)
            event: Dados do evento

        Returns:
            URL escolhido pelo GPT-5 Mini, ou None se falhar
        """
        from phidata.agent import Agent
        from phidata.models.openai import OpenAIChat
        from config import MODELS, OPENROUTER_API_KEY, OPENROUTER_BASE_URL

        titulo = event.get("titulo", "")
        data = event.get("data", "")
        local = event.get("local", "")

        logger.info(f"{self.log_prefix} GPT-5 Mini: escolhendo entre {len(candidate_links)} links candidatos")

        # Criar agente GPT-5 Mini com web search
        tiebreaker_agent = Agent(
            name="Link Tiebreaker Agent",
            model=OpenAIChat(
                id=MODELS["link_consensus"],  # openai/gpt-5-mini:online
                api_key=OPENROUTER_API_KEY,
                base_url=OPENROUTER_BASE_URL,
            ),
            description="Agente especializado em escolher o melhor link entre candidatos",
            instructions=[
                "Usar web search para verificar qual link √© mais confi√°vel",
                "Priorizar links de plataformas oficiais (Sympla, Eventbrite, etc)",
                "Verificar se o link realmente corresponde ao evento espec√≠fico",
                "Retornar APENAS o URL escolhido (nada mais)",
            ],
        )

        # Formatar links candidatos
        links_formatted = "\n".join([f"  {i+1}. {link}" for i, link in enumerate(candidate_links)])

        prompt = f"""Escolha o MELHOR link para este evento entre os candidatos abaixo.

EVENTO:
- T√≠tulo: {titulo}
- Data: {data}
- Local: {local}

LINKS CANDIDATOS (escolha apenas 1):
{links_formatted}

CRIT√âRIOS DE ESCOLHA:
1. Link de plataforma oficial de venda (Sympla, Eventbrite, etc) > site do venue > outros
2. Link que realmente corresponde ao evento espec√≠fico (n√£o gen√©rico)
3. Link ativo e acess√≠vel (verificar com web search se poss√≠vel)

RETORNE APENAS:
- O URL completo do link escolhido (copie exatamente como est√° acima)
- N√ÉO adicione explica√ß√µes ou justificativas"""

        try:
            response = tiebreaker_agent.run(prompt)
            chosen_link = response.content.strip()

            # Validar que √© um dos candidatos
            normalized_chosen = chosen_link.rstrip('/').split('#')[0]
            for candidate in candidate_links:
                normalized_candidate = candidate.rstrip('/').split('#')[0]
                if normalized_chosen == normalized_candidate:
                    logger.info(f"{self.log_prefix} GPT-5 Mini escolheu: {candidate}")
                    return candidate

            logger.warning(f"{self.log_prefix} GPT-5 Mini retornou link inv√°lido: {chosen_link}")
            return None

        except Exception as e:
            logger.error(f"{self.log_prefix} Erro no tiebreaker GPT-5: {e}")
            return None

    async def _search_with_variants(self, event: dict, variant_suffix: str = "") -> str | None:
        """Executa busca com Perplexity com pequenas varia√ß√µes para diversificar resultados.

        Args:
            event: Dados do evento
            variant_suffix: Sufixo para adicionar varia√ß√£o na busca (ex: "sympla", "eventbrite")

        Returns:
            URL encontrado ou None
        """
        from phidata.agent import Agent
        from phidata.models.openai import OpenAIChat
        from config import MODELS, OPENROUTER_API_KEY, OPENROUTER_BASE_URL

        titulo = event.get("titulo", "") or event.get("nome", "")
        data = event.get("data", "")
        local = event.get("local", "")

        # Criar agente de busca com Perplexity
        search_agent = Agent(
            name="Link Search Agent",
            model=OpenAIChat(
                id=MODELS["search"],  # perplexity/sonar
                api_key=OPENROUTER_API_KEY,
                base_url=OPENROUTER_BASE_URL,
            ),
            description="Agente especializado em encontrar links oficiais de eventos",
            instructions=[
                "Buscar apenas links OFICIAIS de venda/informa√ß√µes",
                "Priorizar Sympla, Eventbrite, Ticketmaster, site do venue",
                "N√ÉO retornar links gen√©ricos de homepage",
                "Retornar APENAS o URL ou 'NONE' se n√£o encontrar",
            ],
        )

        # Adicionar varia√ß√£o √† busca se especificada
        platform_hint = f"\nPRIORIDADE: Buscar preferencialmente em {variant_suffix}" if variant_suffix else ""

        prompt = f"""Encontre o link OFICIAL de compra/venda de ingresso para este evento:

EVENTO:
- T√≠tulo: {titulo}
- Data: {data}
- Local: {local}{platform_hint}

‚ö†Ô∏è REGRAS:
1. N√ÉO retorne homepages ou p√°ginas de listagem
2. Link DEVE ser espec√≠fico do evento (com ID/slug √∫nico)
3. Retorne APENAS o URL completo ou "NONE"

RETORNE APENAS:
- O URL completo (https://...) OU
- A palavra "NONE" (se n√£o encontrar)"""

        try:
            response = search_agent.run(prompt)
            link = response.content.strip()

            if link and link != "NONE" and link.startswith("http"):
                logger.info(f"{self.log_prefix} Busca{f' ({variant_suffix})' if variant_suffix else ''}: {link}")
                return link
            else:
                logger.warning(f"{self.log_prefix} Busca{f' ({variant_suffix})' if variant_suffix else ''}: NONE")
                return None

        except Exception as e:
            logger.error(f"{self.log_prefix} Erro na busca com variante: {e}")
            return None

    async def _intelligent_link_search_with_consensus(self, event: dict, attempt: int = 1) -> dict[str, Any]:
        """Vers√£o com consenso multi-modelo do _intelligent_link_search().

        Executa m√∫ltiplas buscas independentes e usa consenso para reduzir alucina√ß√µes.

        Returns:
            dict com: link (str), quality_score (int), validation (dict), consensus_info (dict)
        """
        from config import (
            LINK_CONSENSUS_ENABLED,
            LINK_CONSENSUS_SEARCHES,
            LINK_MAX_INTELLIGENT_SEARCHES,
            LINK_QUALITY_THRESHOLD,
        )

        titulo = event.get("titulo", "") or event.get("nome", "")

        # Se consenso desabilitado, usar m√©todo tradicional
        if not LINK_CONSENSUS_ENABLED:
            return await self._intelligent_link_search(event, attempt)

        if attempt > LINK_MAX_INTELLIGENT_SEARCHES:
            logger.warning(f"{self.log_prefix} Limite de {LINK_MAX_INTELLIGENT_SEARCHES} tentativas atingido: {titulo}")
            return None

        logger.info(f"{self.log_prefix} Busca com consenso (tentativa {attempt}/{LINK_MAX_INTELLIGENT_SEARCHES}): {titulo}")

        # Executar m√∫ltiplas buscas independentes em paralelo
        search_tasks = []

        # Primeira busca sem varia√ß√£o
        search_tasks.append(self._search_with_variants(event, ""))

        # Demais buscas com hints de plataforma (para diversificar)
        platform_variants = ["sympla", "eventbrite", "site oficial"]
        for i in range(1, LINK_CONSENSUS_SEARCHES):
            variant = platform_variants[i % len(platform_variants)]
            search_tasks.append(self._search_with_variants(event, variant))

        # Executar todas em paralelo
        search_results = await asyncio.gather(*search_tasks, return_exceptions=True)

        # Converter exce√ß√µes em None
        links = [result if not isinstance(result, Exception) else None for result in search_results]

        logger.info(f"{self.log_prefix} Resultados de {LINK_CONSENSUS_SEARCHES} buscas: {links}")

        # Encontrar consenso
        consensus_result = self._find_consensus(links, event)

        if not consensus_result:
            logger.warning(f"{self.log_prefix} Nenhum consenso encontrado para: {titulo}")

            # Retry se ainda tiver tentativas
            if attempt < LINK_MAX_INTELLIGENT_SEARCHES:
                logger.info(f"{self.log_prefix} Tentando novamente com novos crit√©rios...")
                return await self._intelligent_link_search_with_consensus(event, attempt + 1)
            else:
                return None

        # Validar qualidade do link consensual
        consensus_link = consensus_result["link"]

        # Verificar se link √© gen√©rico
        if self._is_generic_link(consensus_link):
            logger.warning(f"{self.log_prefix} ‚ùå Link consensual √© gen√©rico: {consensus_link}")
            if attempt < LINK_MAX_INTELLIGENT_SEARCHES:
                return await self._intelligent_link_search_with_consensus(event, attempt + 1)
            else:
                return None

        # Validar qualidade
        try:
            from agents.validation_agent import ValidationAgent

            validation_agent = ValidationAgent()
            link_info = await validation_agent._fetch_link_info(consensus_link, event)

            quality_validation = link_info.get("quality_validation")

            if quality_validation:
                score = quality_validation["score"]
                is_quality = quality_validation["is_quality"]

                if is_quality:
                    logger.info(f"{self.log_prefix} ‚úÖ Link consensual aprovado (score: {score}/100)")
                    return {
                        "link": consensus_link,
                        "quality_score": score,
                        "validation": quality_validation,
                        "consensus_info": consensus_result,
                        "structured_data": link_info.get("structured_data", {}),
                    }
                else:
                    logger.warning(f"{self.log_prefix} ‚ùå Link consensual rejeitado (score: {score}/{LINK_QUALITY_THRESHOLD})")
                    if attempt < LINK_MAX_INTELLIGENT_SEARCHES:
                        return await self._intelligent_link_search_with_consensus(event, attempt + 1)
                    else:
                        # √öltima tentativa - retornar mesmo com baixa qualidade
                        return {
                            "link": consensus_link,
                            "quality_score": score,
                            "validation": quality_validation,
                            "consensus_info": consensus_result,
                            "low_quality": True,
                        }
            else:
                # Sem valida√ß√£o, aceitar link consensual
                logger.warning(f"{self.log_prefix} Valida√ß√£o falhou, aceitando link consensual")
                return {
                    "link": consensus_link,
                    "quality_score": None,
                    "validation": None,
                    "consensus_info": consensus_result,
                }

        except Exception as e:
            logger.error(f"{self.log_prefix} Erro ao validar link consensual: {e}")
            return {
                "link": consensus_link,
                "quality_score": None,
                "validation": None,
                "consensus_info": consensus_result,
            }

    def _classify_link_type(self, url: str, event: dict) -> str:
        """Classifica o tipo de link do evento.

        Args:
            url: URL do link
            event: Dados do evento (para contexto)

        Returns:
            "purchase" (plataforma de venda), "info" (site informativo), ou "venue" (p√°gina do local)
        """
        if not url:
            return "info"  # Default para links ausentes

        url_lower = url.lower()

        # 1. PLATAFORMAS DE VENDA (maior prioridade)
        purchase_platforms = [
            'sympla.com',
            'eventbrite.com',
            'ticketmaster.com',
            'ingresso.com',
            'ingressodigital.com',
            'tickets.com',
            'eventim.com.br/artist',  # Eventim com artista espec√≠fico
            'eleventickets.com',
        ]

        for platform in purchase_platforms:
            if platform in url_lower:
                return "purchase"

        # 2. VENUES COM P√ÅGINAS ESPEC√çFICAS DE EVENTOS
        venue_event_patterns = [
            ('bluenoterio.com.br/shows/', 5),  # Blue Note com slug do show
            ('salaceliciameireles.rj.gov.br/programacao/', 5),  # Sala Cec√≠lia com evento espec√≠fico
        ]

        for pattern, min_length in venue_event_patterns:
            if pattern in url_lower:
                # Verificar se tem slug/ID significativo no final
                slug = url.split('/')[-1].rstrip('/')
                if len(slug) > min_length:
                    return "purchase"  # P√°gina espec√≠fica de evento

        # 3. VERIFICAR SE √â LINK GEN√âRICO (listagem/homepage)
        if self._is_generic_link(url):
            return "venue"  # Links gen√©ricos geralmente s√£o p√°ginas do venue

        # 4. VERIFICAR SE √â SITE DE ARTISTA (informativo)
        # NOTA: _is_artist_or_venue_site precisa do t√≠tulo do evento
        titulo = event.get("titulo", "")
        if titulo:
            try:
                if hasattr(self, '_validation_agent') and self._validation_agent:
                    if self._validation_agent._is_artist_or_venue_site(url, titulo):
                        return "info"
            except:
                pass  # Se falhar, continuar com outras verifica√ß√µes

        # 5. DOM√çNIOS .GOV.BR = venue
        if '.gov.br' in url_lower:
            return "venue"

        # 6. FALLBACK: Link externo gen√©rico = info
        return "info"

    def _matches_url_pattern(self, url: str) -> bool:
        """Valida se URL corresponde ao padr√£o esperado para o dom√≠nio.

        Args:
            url: URL a validar

        Returns:
            True se URL corresponde ao padr√£o do dom√≠nio, False caso contr√°rio
        """
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()

            # Verificar se dom√≠nio tem padr√£o definido
            for pattern_domain, pattern in URL_PATTERNS.items():
                if pattern_domain in domain:
                    # Validar contra o padr√£o
                    full_path = parsed.path + parsed.fragment  # ElevenTickets usa fragment (#/...)
                    if re.search(pattern, full_path):
                        return True
                    else:
                        logger.warning(f"URL n√£o corresponde ao padr√£o esperado para {pattern_domain}: {url}")
                        return False

            # Dom√≠nio sem padr√£o definido = aceitar
            return True

        except Exception as e:
            logger.error(f"Erro ao validar padr√£o de URL: {e}")
            return True  # Em caso de erro, n√£o bloquear

    async def _validate_link_content(self, link: str, event: dict) -> dict:
        """Valida conte√∫do de link SPA verificando se informa√ß√µes do evento correspondem.

        Args:
            link: URL a validar
            event: Evento com informa√ß√µes esperadas

        Returns:
            dict com: valid (bool), reason (str), details (dict)
        """
        try:
            # Fetch + parse usando HttpClientWrapper
            result = await self.http_client.fetch_and_parse(
                link,
                extract_text=True,
                text_max_length=5000,  # Mais texto para valida√ß√£o de conte√∫do
                clean_html=True
            )

            if not result["success"]:
                status_code = result["status_code"]
                return {
                    "valid": False,
                    "reason": f"HTTP {status_code}" if status_code else result["error"],
                    "details": {}
                }

            # Extrair texto vis√≠vel da p√°gina
            page_text = result["text"].lower()

            # QUICK WIN #1: Validar conte√∫do m√≠nimo (detectar soft 404s)
            if not page_text or len(page_text.strip()) < 50:
                return {
                    "valid": False,
                    "reason": "P√°gina vazia ou conte√∫do muito curto (poss√≠vel 404 disfar√ßado)",
                    "details": {"page_length": len(page_text.strip()) if page_text else 0}
                }

            # Informa√ß√µes do evento para validar
            titulo = (event.get("titulo") or event.get("nome", "")).lower()
            local = (event.get("local", "")).lower()

            # Valida√ß√µes de conte√∫do
            issues = []
            matches = []

            # Verificar t√≠tulo (pelo menos 70% das palavras) - threshold aumentado para reduzir falsos positivos
            titulo_words = [w for w in titulo.split() if len(w) > 3]  # palavras > 3 chars
            titulo_match_ratio = 0
            if titulo_words:
                titulo_matches = sum(1 for word in titulo_words if word in page_text)
                titulo_match_ratio = titulo_matches / len(titulo_words)

                if titulo_match_ratio >= 0.7:  # Aumentado de 0.5 para 0.7 (reduzir falsos positivos)
                    matches.append(f"T√≠tulo encontrado ({titulo_match_ratio:.0%})")
                else:
                    issues.append(f"T√≠tulo n√£o encontrado ({titulo_match_ratio:.0%} match)")

            # Verificar local (palavras principais) - QUICK WIN #2: threshold reduzido de 50% para 40%
            local_words = [w for w in local.split() if len(w) > 4]  # palavras > 4 chars
            local_match_ratio = 0
            if local_words:
                local_matches = sum(1 for word in local_words if word in page_text)
                local_match_ratio = local_matches / len(local_words) if local_words else 0

                if local_match_ratio >= 0.4:  # Reduzido de 0.5 para 0.4 (menos falsos negativos)
                    matches.append(f"Local encontrado ({local_match_ratio:.0%})")
                else:
                    issues.append(f"Local n√£o encontrado ({local_match_ratio:.0%} match)")

            # Verificar se p√°gina tem indicadores de venda (bot√µes de compra)
            buy_indicators = ['comprar', 'ingresso', 'ticket', 'buy', 'cart', 'carrinho']
            has_buy_button = any(indicator in page_text for indicator in buy_indicators)

            if has_buy_button:
                matches.append("Bot√£o de compra encontrado")
            else:
                issues.append("Nenhum bot√£o de compra encontrado")

            # Decis√£o final
            valid = len(matches) >= 2 and len(issues) <= 1

            return {
                "valid": valid,
                "reason": "Conte√∫do validado" if valid else f"Valida√ß√£o falhou: {', '.join(issues)}",
                "details": {
                    "matches": matches,
                    "issues": issues,
                    "titulo_match": titulo_match_ratio if titulo_words else None,
                    "local_match": local_match_ratio if local_words else None,
                }
            }

        except Exception as e:
            logger.error(f"Erro ao validar conte√∫do do link: {e}")
            return {
                "valid": True,  # Em caso de erro, n√£o bloquear (pode ser problema tempor√°rio)
                "reason": f"Erro na valida√ß√£o: {str(e)}",
                "details": {}
            }

    async def verify_events(self, events_json: str) -> dict[str, Any]:
        """Verifica e valida eventos extra√≠dos pelo agente de busca."""
        logger.info(f"{self.log_prefix} Iniciando verifica√ß√£o de eventos...")

        try:
            events_data = json.loads(events_json) if isinstance(events_json, str) else events_json
        except json.JSONDecodeError:
            logger.error("Erro ao decodificar JSON de eventos")
            return {"verified_events": [], "rejected_events": [], "warnings": ["JSON inv√°lido"]}

        # Validar links em paralelo (valida√ß√£o b√°sica)
        events_with_link_validation = await self._validate_links(events_data)

        # Processar com LLM para verifica√ß√£o inteligente (primeira camada)
        verified_data = self._verify_with_llm(events_with_link_validation)

        # NOVA CAMADA: Valida√ß√£o individual rigorosa com ValidationAgent
        logger.info(f"{self.log_prefix} Iniciando valida√ß√£o individual rigorosa...")
        from agents.validation_agent import ValidationAgent

        validation_agent = ValidationAgent()
        individual_validation = await validation_agent.validate_events_batch(
            verified_data.get("verified_events", [])
        )

        # Combinar resultados
        final_verified = individual_validation["validated_events"]
        final_rejected = (
            verified_data.get("rejected_events", [])
            + individual_validation["rejected_events"]
        )
        final_warnings = (
            verified_data.get("warnings", [])
            + individual_validation["validation_warnings"]
        )

        logger.info(
            f"Verifica√ß√£o conclu√≠da. Eventos finais aprovados: {len(final_verified)} "
            f"(rejeitados na valida√ß√£o individual: {len(individual_validation['rejected_events'])})"
        )

        return {
            "verified_events": final_verified,
            "rejected_events": final_rejected,
            "warnings": final_warnings,
            "duplicates_removed": verified_data.get("duplicates_removed", []),
        }

    async def _intelligent_link_search(self, event: dict, attempt: int = 1) -> dict[str, Any]:
        """Usa Perplexity para buscar o link correto de um evento e valida o conte√∫do.

        Returns:
            dict com: link (str), quality_score (int), validation (dict) ou None se n√£o encontrar
        """
        from config import LINK_MAX_INTELLIGENT_SEARCHES, LINK_QUALITY_THRESHOLD

        if attempt > LINK_MAX_INTELLIGENT_SEARCHES:
            logger.warning(f"{self.log_prefix} Limite de {LINK_MAX_INTELLIGENT_SEARCHES} tentativas atingido para: {event.get('titulo')}")
            return None

        titulo = event.get("titulo", "") or event.get("nome", "")
        data = event.get("data", "")
        horario = event.get("horario", "")
        local = event.get("local", "")
        categoria = event.get("categoria", "")
        preco = event.get("preco", "")
        descricao = event.get("descricao_enriquecida") or event.get("descricao", "")
        fontes = event.get("fontes", [])

        logger.info(f"{self.log_prefix} Buscando link correto (tentativa {attempt}/{LINK_MAX_INTELLIGENT_SEARCHES}): {titulo}")

        # Criar agente de busca com Perplexity
        search_agent = Agent(
            name="Link Search Agent",
            model=OpenAIChat(
                id=MODELS["search"],  # perplexity/sonar-pro
                api_key=OPENROUTER_API_KEY,
                base_url=OPENROUTER_BASE_URL,
            ),
            description="Agente especializado em encontrar links oficiais de eventos",
            instructions=[
                "Buscar apenas links OFICIAIS de venda/informa√ß√µes",
                "Priorizar Sympla, Eventbrite, Ticketmaster, site do venue",
                "N√ÉO retornar links gen√©ricos de homepage",
                "Retornar APENAS o URL ou 'NONE' se n√£o encontrar",
            ],
        )

        # Construir prompt com todas as informa√ß√µes dispon√≠veis
        fonte_info = f"\nFontes mencionadas: {', '.join(fontes[:3])}" if fontes else ""
        horario_info = f"\nHor√°rio: {horario}" if horario else ""
        categoria_info = f"\nCategoria: {categoria}" if categoria else ""
        preco_info = f"\nPre√ßo: {preco}" if preco else ""

        # Se √© retry, adicionar contexto do problema anterior
        retry_context = ""
        if attempt > 1:
            retry_context = f"""

‚ö†Ô∏è TENTATIVA {attempt}/{LINK_MAX_INTELLIGENT_SEARCHES} - TENTATIVA ANTERIOR FALHOU

PROBLEMAS DETECTADOS:
- Link retornado foi gen√©rico/homepage ou n√£o foi encontrado
- Tente buscar em OUTRAS plataformas al√©m da anterior
- Verifique redes sociais do venue (Instagram/Facebook) por links nos posts
- Se realmente n√£o existir link online, retorne "NONE"
"""

        prompt = f"""Encontre o link OFICIAL de compra/venda de ingresso para este evento:

EVENTO:
- T√≠tulo: {titulo}
- Data: {data}{horario_info}
- Local: {local}{categoria_info}{preco_info}
- Descri√ß√£o: {descricao[:200]}{fonte_info}{retry_context}

‚ö†Ô∏è REGRAS CR√çTICAS (leia com aten√ß√£o):

1. N√ÉO RETORNE sites de ARTISTAS ou VENUES (p√°ginas institucionais):
   ‚ùå ERRADO: raphaelghanem.com.br (site do artista)
   ‚ùå ERRADO: casadochoro.com.br (homepage do venue)
   ‚ùå ERRADO: sympla.com.br (homepage do Sympla)
   ‚ùå ERRADO: salaceliciameireles.rj.gov.br/programacao (listagem geral)

   ‚úÖ CORRETO: sympla.com.br/evento/raphael-ghanem-18112025/12345
   ‚úÖ CORRETO: eventbrite.com.br/e/quarteto-de-cordas-tickets-67890
   ‚úÖ CORRETO: bluenoterio.com.br/shows/nome-show__abc123/

2. O link DEVE conter identificador √öNICO do evento:
   - ID num√©rico: /evento/nome/123456
   - Slug com data: /evento-18-11-2025
   - Hash alfanum√©rico: /show/nome__abc123de/
   - Par√¢metro: ?event_id=789

3. PLATAFORMAS PRIORIT√ÅRIAS (nesta ordem):
   a) Sympla: sympla.com.br/evento/[nome-evento]/[ID-numerico]
   b) Eventbrite: eventbrite.com.br/e/[nome-evento]-tickets-[ID]
   c) Ticketmaster: ticketmaster.com.br/event/[ID]
   d) Ingresso.com: ingresso.com/evento/[nome-evento]/[ID]
   e) Site do venue com p√°gina espec√≠fica (ex: bluenoterio.com.br/shows/[evento]/)

4. SE N√ÉO ENCONTRAR link espec√≠fico em NENHUMA plataforma:
   - Retorne "NONE"
   - N√ÉO invente links gen√©ricos

EXEMPLOS DE LINKS V√ÅLIDOS (RETORNE ASSIM):
‚úÖ https://www.sympla.com.br/evento/raphael-ghanem-stand-up/2345678
‚úÖ https://www.eventbrite.com.br/e/quarteto-de-cordas-da-osb-tickets-987654321
‚úÖ https://bluenoterio.com.br/shows/irma-you-and-my-guitar__22hz624n/
‚úÖ https://sis.ingressodigital.com/lojanew/detalhes_evento.asp?eve_cod=15246

EXEMPLOS DE LINKS INV√ÅLIDOS (N√ÉO RETORNE):
‚ùå https://raphaelghanem.com.br (site oficial do artista)
‚ùå https://www.sympla.com.br (homepage do Sympla)
‚ùå https://www.sympla.com.br/eventos/rio-de-janeiro (listagem por cidade)
‚ùå https://casadochoro.com.br/programacao (calend√°rio do venue)
‚ùå https://salaceliciameireles.rj.gov.br/programacao (agenda geral)

RETORNE APENAS:
- O URL completo (https://...) OU
- A palavra "NONE" (se n√£o encontrar link espec√≠fico de venda)"""

        try:
            response = search_agent.run(prompt)
            new_link = response.content.strip()

            # Validar resposta
            if new_link and new_link != "NONE" and new_link.startswith("http"):
                logger.info(f"{self.log_prefix} Link encontrado: {new_link}")

                # Verificar se link √© gen√©rico (p√°gina de listagem)
                if self._is_generic_link(new_link):
                    logger.warning(f"{self.log_prefix} ‚ùå Link gen√©rico detectado: {new_link}")

                    # Retry se ainda tiver tentativas
                    if attempt < LINK_MAX_INTELLIGENT_SEARCHES:
                        logger.info(f"{self.log_prefix} Tentando busca novamente solicitando link ESPEC√çFICO...")
                        return await self._intelligent_link_search(event, attempt + 1)
                    else:
                        logger.warning(f"{self.log_prefix} Todas tentativas esgotadas. Link gen√©rico rejeitado.")
                        return None

                # NOVO: Validar qualidade do link encontrado
                try:
                    from agents.validation_agent import ValidationAgent

                    validation_agent = ValidationAgent()
                    link_info = await validation_agent._fetch_link_info(new_link, event)

                    quality_validation = link_info.get("quality_validation")

                    if quality_validation:
                        score = quality_validation["score"]
                        is_quality = quality_validation["is_quality"]

                        if is_quality:
                            logger.info(f"{self.log_prefix} ‚úÖ Link aprovado (score: {score}/100)")
                            return {
                                "link": new_link,
                                "quality_score": score,
                                "validation": quality_validation,
                                "structured_data": link_info.get("structured_data", {}),
                            }
                        else:
                            logger.warning(
                                f"{self.log_prefix} ‚ùå Link rejeitado (score: {score}/{LINK_QUALITY_THRESHOLD})"
                            )
                            logger.warning(f"{self.log_prefix} Issues: {', '.join(quality_validation['issues'])}")

                            # Retry se ainda tiver tentativas
                            if attempt < LINK_MAX_INTELLIGENT_SEARCHES:
                                logger.info(f"{self.log_prefix} Tentando busca novamente com crit√©rios mais rigorosos...")
                                return await self._intelligent_link_search(event, attempt + 1)
                            else:
                                logger.warning(f"{self.log_prefix} Todas tentativas esgotadas. Retornando link mesmo com baixa qualidade.")
                                return {
                                    "link": new_link,
                                    "quality_score": score,
                                    "validation": quality_validation,
                                    "low_quality": True,
                                }
                    else:
                        # Sem valida√ß√£o de qualidade (erro), aceitar link
                        logger.warning(f"{self.log_prefix} Valida√ß√£o de qualidade falhou, aceitando link")
                        return {"link": new_link, "quality_score": None, "validation": None}

                except Exception as e:
                    logger.error(f"{self.log_prefix} Erro ao validar qualidade do link: {e}")
                    # Em caso de erro, retornar link sem valida√ß√£o
                    return {"link": new_link, "quality_score": None, "validation": None}

            else:
                logger.warning(f"{self.log_prefix} ‚úó Nenhum link v√°lido encontrado para: {titulo}")
                return None

        except Exception as e:
            logger.error(f"{self.log_prefix} Erro na busca inteligente de link: {e}")
            return None

    async def _validate_single_event_link(self, event: dict) -> dict:
        """Valida o link de um √∫nico evento com retry e busca inteligente.

        Args:
            event: Evento a validar

        Returns:
            Dicion√°rio com estat√≠sticas da valida√ß√£o deste evento
        """
        stats = {
            "total_links": 0,
            "validated_first_try": 0,
            "failed_all_retries": 0,
            "intelligent_searches": 0,
            "links_fixed": 0,
            "no_retry_needed": 0,
            "generic_links_detected": 0,
        }

        link = event.get("link") or event.get("link_ingresso") or event.get("ticket_link")

        if not link:
            logger.info(f"‚Üí Evento sem link, iniciando busca inteligente: {event.get('titulo')}")
            stats["total_links"] += 1
            stats["intelligent_searches"] += 1

            link_result = await self._intelligent_link_search_with_consensus(event)

            if link_result and link_result.get("link"):
                new_link = link_result["link"]
                event["link_ingresso"] = new_link
                event["link_type"] = self._classify_link_type(new_link, event)
                event["link_updated_by_ai"] = True
                event["link_added_by_ai"] = True  # Novo campo para indicar que foi adicionado (n√£o apenas corrigido)
                event["link_quality_score"] = link_result.get("quality_score")
                event["link_quality_validation"] = link_result.get("validation")

                # Armazenar informa√ß√µes de consenso (se dispon√≠vel)
                if link_result.get("consensus_info"):
                    event["link_consensus_info"] = link_result["consensus_info"]

                # Armazenar dados estruturados extra√≠dos do link
                if link_result.get("structured_data"):
                    event["link_structured_data"] = link_result["structured_data"]

                # Link j√° foi validado no _intelligent_link_search
                event["link_valid"] = True
                event["link_status_code"] = 200
                stats["links_fixed"] += 1
                logger.info(f"‚úì Link adicionado com sucesso: {new_link}")
            else:
                event["link_valid"] = None
                event["link_error"] = "Nenhum link encontrado via busca inteligente"
                event["requires_manual_link_check"] = True
                logger.warning(f"‚ö† Nenhum link encontrado para: {event.get('titulo')}")

            return stats

        # Detectar placeholder "INCOMPLETO" e ir direto para busca inteligente
        if link in ["INCOMPLETO", "incompleto", "/INCOMPLETO", "NONE", "none"]:
            logger.info(f"‚Üí Link placeholder detectado ({link}), iniciando busca inteligente: {event.get('titulo')}")
            stats["total_links"] += 1
            stats["intelligent_searches"] += 1

            link_result = await self._intelligent_link_search_with_consensus(event)

            if link_result and link_result.get("link"):
                new_link = link_result["link"]
                event["link_original"] = link
                event["link_ingresso"] = new_link
                event["link_type"] = self._classify_link_type(new_link, event)
                event["link_updated_by_ai"] = True
                event["link_quality_score"] = link_result.get("quality_score")
                event["link_quality_validation"] = link_result.get("validation")

                # Armazenar informa√ß√µes de consenso (se dispon√≠vel)
                if link_result.get("consensus_info"):
                    event["link_consensus_info"] = link_result["consensus_info"]

                # Armazenar dados estruturados extra√≠dos do link
                if link_result.get("structured_data"):
                    event["link_structured_data"] = link_result["structured_data"]

                # Link j√° foi validado no _intelligent_link_search
                event["link_valid"] = True
                event["link_status_code"] = 200
                stats["links_fixed"] += 1
                logger.info(f"‚úì Link corrigido com sucesso: {new_link}")
            else:
                event["link_valid"] = False
                event["link_error"] = "Placeholder sem link v√°lido encontrado"
                event["requires_manual_link_check"] = True
                logger.warning(f"‚ö† Nenhum link encontrado para: {event.get('titulo')}")

            return stats

        # Detectar link gen√©rico (p√°gina de busca/categoria) e ir para busca inteligente
        if self._is_generic_link(link):
            logger.info(f"üö´ Link gen√©rico detectado, iniciando busca inteligente: {link[:80]}...")
            stats["total_links"] += 1
            stats["generic_links_detected"] += 1
            stats["intelligent_searches"] += 1

            link_result = await self._intelligent_link_search_with_consensus(event)

            if link_result and link_result.get("link"):
                new_link = link_result["link"]
                event["link_original"] = link
                event["link_ingresso"] = new_link
                event["link_type"] = self._classify_link_type(new_link, event)
                event["link_updated_by_ai"] = True
                event["link_was_generic"] = True
                event["link_quality_score"] = link_result.get("quality_score")
                event["link_quality_validation"] = link_result.get("validation")

                # Armazenar informa√ß√µes de consenso (se dispon√≠vel)
                if link_result.get("consensus_info"):
                    event["link_consensus_info"] = link_result["consensus_info"]

                # Armazenar dados estruturados extra√≠dos do link
                if link_result.get("structured_data"):
                    event["link_structured_data"] = link_result["structured_data"]

                # Link j√° foi validado no _intelligent_link_search
                event["link_valid"] = True
                event["link_status_code"] = 200
                stats["links_fixed"] += 1
                logger.info(f"‚úì Link gen√©rico substitu√≠do por link espec√≠fico: {new_link}")
            else:
                # Nenhum link espec√≠fico encontrado, manter gen√©rico mas marcar
                event["link_valid"] = False
                event["link_is_generic"] = True
                event["link_error"] = "Link gen√©rico - p√°gina de busca/categoria"
                event["requires_manual_link_check"] = True
                logger.warning(f"‚ö† Nenhum link espec√≠fico encontrado para: {event.get('titulo')}")

            return stats

        stats["total_links"] += 1
        original_link = link

        # EXCE√á√ÉO 1: Links do Eventim n√£o respondem bem a HEAD requests
        if 'eventim.com.br/artist/blue-note-rio/' in link.lower():
            event["link_valid"] = True
            event["link_status_code"] = 200
            event["validation_skipped"] = "Eventim links are trusted (HEAD requests not supported)"
            stats["validated_first_try"] += 1
            logger.info(f"‚úì Link Eventim v√°lido (sem valida√ß√£o HTTP): {link}")
            return stats

        # EXCE√á√ÉO 2: Links oficiais da Sala Cec√≠lia Meireles (.gov.br)
        if 'salaceciliameireles.rj.gov.br/programacao/' in link.lower():
            event["link_valid"] = True
            event["link_status_code"] = 200
            event["validation_skipped"] = "Official Sala Cec√≠lia Meireles links are trusted"
            stats["validated_first_try"] += 1
            logger.info(f"‚úì Link oficial Sala Cec√≠lia v√°lido (sem valida√ß√£o HTTP): {link}")
            return stats

        # EXCE√á√ÉO 3: Links oficiais do Teatro Municipal (.gov.br) - SSL issues
        if 'theatromunicipal.rj.gov.br' in link.lower():
            event["link_valid"] = True
            event["link_status_code"] = 200
            event["validation_skipped"] = "Official Teatro Municipal links are trusted (SSL issues)"
            stats["validated_first_try"] += 1
            logger.info(f"‚úì Link oficial Teatro Municipal v√°lido (sem valida√ß√£o HTTP): {link}")
            return stats

        # Validar link via HTTP request
        http_client = HttpClientWrapper()
        link_status = await http_client.check_link_status(link)

        # Link acess√≠vel (200 OK) - validar conte√∫do antes de aceitar
        if link_status["accessible"]:
            # NOVA VALIDA√á√ÉO: Verificar se conte√∫do corresponde ao evento
            logger.info(f"‚Üí Link HTTP 200 OK, validando conte√∫do: {link[:80]}...")
            content_validation = await self._validate_link_content(link, event)

            if content_validation["valid"]:
                # Conte√∫do v√°lido - aceitar link
                event["link_valid"] = True
                event["link_status_code"] = link_status["status_code"]
                event["link_content_validated"] = True
                event["link_validation_details"] = content_validation["details"]
                stats["validated_first_try"] += 1
                logger.info(f"‚úì Link v√°lido (HTTP 200 + conte√∫do OK): {link}")
                return stats
            else:
                # Conte√∫do inv√°lido - link est√° acess√≠vel mas aponta para evento errado
                reason = content_validation["reason"]
                logger.warning(f"‚ö†Ô∏è Link HTTP 200 mas conte√∫do inv√°lido ({reason}): {link} - Tentando busca inteligente...")

                # Marcar como erro de conte√∫do e tentar busca inteligente
                event["link_original"] = link
                event["link_content_error"] = reason
                event["link_validation_details"] = content_validation["details"]

                stats["total_links"] += 1
                stats["link_errors"] = stats.get("link_errors", 0) + 1
                stats["intelligent_searches"] += 1

                # Tentar encontrar link correto
                link_result = await self._intelligent_link_search_with_consensus(event)

                if link_result and link_result.get("link"):
                    new_link = link_result["link"]
                    event["link_ingresso"] = new_link
                    event["link_type"] = self._classify_link_type(new_link, event)
                    event["link_updated_by_ai"] = True
                    event["link_recovered_from_content_error"] = True
                    event["link_quality_score"] = link_result.get("quality_score")
                    event["link_quality_validation"] = link_result.get("validation")

                    # Armazenar informa√ß√µes de consenso (se dispon√≠vel)
                    if link_result.get("consensus_info"):
                        event["link_consensus_info"] = link_result["consensus_info"]

                    # Armazenar dados estruturados extra√≠dos do link
                    if link_result.get("structured_data"):
                        event["link_structured_data"] = link_result["structured_data"]

                    # Link j√° foi validado no _intelligent_link_search
                    event["link_valid"] = True
                    event["link_status_code"] = 200
                    stats["links_fixed"] += 1
                    stats["links_recovered"] = stats.get("links_recovered", 0) + 1
                    logger.info(f"‚úì Link recuperado ap√≥s erro de conte√∫do: {new_link}")
                else:
                    # Nenhum link alternativo encontrado - manter link original mas marcar como suspeito
                    event["link_valid"] = False
                    event["link_status_code"] = link_status["status_code"]
                    event["link_error"] = f"Conte√∫do inv√°lido: {reason}"
                    event["requires_manual_link_check"] = True
                    stats["links_failed_permanently"] = stats.get("links_failed_permanently", 0) + 1
                    logger.error(f"‚ùå Link com conte√∫do inv√°lido ({reason}): {link} - Nenhum link alternativo encontrado")

                return stats

        # Link com erro (404, 403, timeout) - tentar busca inteligente
        status_code = link_status.get("status_code")
        reason = link_status.get("reason", "Unknown error")

        logger.warning(f"‚ö†Ô∏è Link com erro ({reason}): {link} - Tentando busca inteligente...")
        stats["total_links"] += 1
        stats["link_errors"] = stats.get("link_errors", 0) + 1
        stats["intelligent_searches"] += 1

        # Tentar encontrar link alternativo
        link_result = await self._intelligent_link_search_with_consensus(event)

        if link_result and link_result.get("link"):
            new_link = link_result["link"]
            event["link_original"] = link
            event["link_ingresso"] = new_link
            event["link_type"] = self._classify_link_type(new_link, event)
            event["link_updated_by_ai"] = True
            event["link_recovered_from_error"] = True
            event["link_original_error"] = f"{status_code} - {reason}" if status_code else reason
            event["link_quality_score"] = link_result.get("quality_score")
            event["link_quality_validation"] = link_result.get("validation")

            # Armazenar informa√ß√µes de consenso (se dispon√≠vel)
            if link_result.get("consensus_info"):
                event["link_consensus_info"] = link_result["consensus_info"]

            # Armazenar dados estruturados extra√≠dos do link
            if link_result.get("structured_data"):
                event["link_structured_data"] = link_result["structured_data"]

            # Link j√° foi validado no _intelligent_link_search
            event["link_valid"] = True
            event["link_status_code"] = 200
            stats["links_fixed"] += 1
            stats["links_recovered"] = stats.get("links_recovered", 0) + 1
            logger.info(f"‚úì Link recuperado com sucesso: {new_link} (original tinha erro: {reason})")
        else:
            # Nenhum link alternativo encontrado
            event["link_valid"] = False
            event["link_status_code"] = status_code
            event["link_error"] = f"{status_code} - {reason}" if status_code else reason
            event["requires_manual_link_check"] = True
            stats["links_failed_permanently"] = stats.get("links_failed_permanently", 0) + 1
            logger.error(f"‚ùå Link permanentemente inv√°lido ({reason}): {link} - Nenhum link alternativo encontrado")

        return stats

    async def _validate_links(self, events: dict | list) -> dict | list:
        """Valida se os links de eventos s√£o acess√≠veis com retry e busca inteligente (paralelizado)."""
        logger.info(f"{self.log_prefix} Validando links de eventos em paralelo com retry autom√°tico...")

        # Extrair eventos da estrutura complexa do structured_events.json
        event_list = []
        if isinstance(events, dict):
            # Estrutura: {"eventos_gerais": {"eventos": [...]}, "eventos_locais_especiais": {...}}
            if "eventos_gerais" in events:
                event_list.extend(events["eventos_gerais"].get("eventos", []))

            if "eventos_locais_especiais" in events:
                for local_name, local_events in events["eventos_locais_especiais"].items():
                    if isinstance(local_events, list):
                        # Filtra eventos reais (apenas dicts, ignora __checagem e outros tipos)
                        event_list.extend([e for e in local_events if isinstance(e, dict) and "__checagem" not in e])

            # Fallback para estrutura simples
            if not event_list:
                event_list = events.get("events", [])
        else:
            event_list = events

        # Estat√≠sticas agregadas
        stats = {
            "total_links": 0,
            "validated_first_try": 0,
            "validated_after_retry": 0,
            "failed_all_retries": 0,
            "intelligent_searches": 0,
            "links_fixed": 0,
            "no_retry_needed": 0,
            "generic_links_detected": 0,
        }

        # Validar todos os links em paralelo com rate limiting
        # Criar sem√°foro para limitar concorr√™ncia
        semaphore = asyncio.Semaphore(LINK_VALIDATION_MAX_CONCURRENT)

        async def validate_with_semaphore(event):
            """Wrapper para valida√ß√£o com sem√°foro."""
            async with semaphore:
                return await self._validate_single_event_link(event)

        # Criar tasks para validar todos os eventos em paralelo
        validation_tasks = [
            validate_with_semaphore(event)
            for event in event_list
        ]

        # Executar todas as valida√ß√µes em paralelo (respeitando sem√°foro)
        logger.info(f"Iniciando valida√ß√£o paralela de {len(event_list)} links (m√°x {LINK_VALIDATION_MAX_CONCURRENT} concorrentes)...")
        validation_results = await asyncio.gather(*validation_tasks, return_exceptions=True)

        # Agregar estat√≠sticas
        for result in validation_results:
            if isinstance(result, dict):
                for key in stats:
                    stats[key] += result.get(key, 0)

        # Log de estat√≠sticas
        logger.info(f"\n{'='*60}")
        logger.info("üìä Estat√≠sticas de Valida√ß√£o de Links:")
        logger.info(f"  Total de links verificados: {stats['total_links']}")
        logger.info(f"  ‚úì Validados na 1¬™ tentativa: {stats['validated_first_try']}")
        logger.info(f"  ‚úó Falharam ap√≥s todos os retries: {stats['failed_all_retries']}")
        logger.info(f"  ‚úó Erros permanentes (404, 403, etc): {stats['no_retry_needed']}")
        logger.info(f"  üö´ Links gen√©ricos detectados: {stats['generic_links_detected']}")
        logger.info(f"  üîç Buscas inteligentes realizadas: {stats['intelligent_searches']}")
        logger.info(f"  ‚úì Links corrigidos via IA: {stats['links_fixed']}")
        logger.info(f"{'='*60}\n")

        return events

    def _verify_with_llm(self, events: dict | list) -> dict[str, Any]:
        """Usa LLM para verifica√ß√£o inteligente de eventos."""
        logger.info("Verificando eventos com LLM...")

        # Calcular datas e dias da semana para passar ao LLM
        start_date = SEARCH_CONFIG['start_date']
        end_date = SEARCH_CONFIG['end_date']

        # Gerar calend√°rio de s√°bados e domingos no per√≠odo
        weekends = []
        current = start_date
        while current <= end_date:
            weekday_num = current.weekday()  # 5=s√°bado, 6=domingo
            if weekday_num in [5, 6]:
                weekday_name = "s√°bado" if weekday_num == 5 else "domingo"
                weekends.append(f"{current.strftime('%d/%m/%Y')} ({weekday_name})")
            current += timedelta(days=1)

        weekends_text = "\n".join(weekends)

        prompt = f"""
Voc√™ √© um agente de verifica√ß√£o rigoroso. Analise os eventos abaixo e classifique cada um como:
- APROVADO: evento v√°lido e confi√°vel
- REJEITADO: evento que n√£o atende crit√©rios ou informa√ß√µes inconsistentes

EVENTOS PARA VERIFICAR:
{json.dumps(events, indent=2, ensure_ascii=False)}

PER√çODO V√ÅLIDO: {start_date.strftime('%d/%m/%Y')} a {end_date.strftime('%d/%m/%Y')}

S√ÅBADOS E DOMINGOS NO PER√çODO (para valida√ß√£o de eventos ao ar livre):
{weekends_text}

CRIT√âRIOS DE APROVA√á√ÉO:

1. DATA V√ÅLIDA:
   - Deve estar entre {start_date.strftime('%d/%m/%Y')} e {end_date.strftime('%d/%m/%Y')}

2. TEATRO COM√âDIA / STAND-UP - VALIDA√á√ÉO RIGOROSA:
   - APROVAR: com√©dia adulta, stand-up, humor para adultos, "indicado para maiores de 14/16/18"
   - REJEITAR SEMPRE se cont√©m QUALQUER termo:

     INFANTIL/FAMILIAR:
     * "infantil", "crian√ßa(s)", "kids", "criancas"
     * "infanto-juvenil", "infanto juvenil"
     * "fam√≠lia", "familia", "family", "para toda fam√≠lia"
     * "sess√£o infantil", "sessao infantil", "sess√£o dupla", "sessao dupla"
     * "indicado para crian√ßas", "filme infantil", "filmes infantis", "cinema infantil"

     LGBTQIAPN+:
     * "lgbt", "lgbtq", "lgbtqia", "lgbtqiapn"
     * "pride", "parada gay", "parada lgbtq"
     * "diversidade sexual", "queer", "drag queen", "drag king"

   - Se menciona "todas as idades" SEM clareza de ser adulto ‚Üí REJEITAR (preferir seguran√ßa)

3. EVENTOS AO AR LIVRE:
   - APROVAR: apenas se data for s√°bado OU domingo (use lista acima)
   - REJEITAR: se for segunda, ter√ßa, quarta, quinta ou sexta-feira

4. LINKS:
   - Links gen√©ricos (ex: sympla.com.br, ticketmaster.com.br) ‚Üí N√ÉO rejeitar automaticamente
   - Se link √© de plataforma confi√°vel (Sympla, Eventbrite, Ticketmaster) e outras infos est√£o completas ‚Üí APROVAR com aviso
   - Apenas rejeitar se link for suspeito ou evento n√£o tiver NENHUMA info de compra

5. INFORMA√á√ïES M√çNIMAS:
   - Obrigat√≥rio: t√≠tulo, data, local
   - Pre√ßo e hor√°rio podem ser "Consultar" se outras infos estiverem completas

CRIT√âRIOS DE REJEI√á√ÉO:

- Eventos com data fora do per√≠odo v√°lido
- Teatro EXPLICITAMENTE infantil na categoria com√©dia
- Eventos ao ar livre em dias de SEMANA (segunda a sexta)
- Informa√ß√µes extremamente incompletas (falta t√≠tulo OU data OU local)
- Duplicatas exatas (mesmo t√≠tulo, mesma data, mesmo local)

CRIT√âRIOS DE DUPLICATAS:
- Mesmo evento: mesmo t√≠tulo (ou muito similar >90%), mesma data, mesmo local
- EXCE√á√ÉO: Sess√µes diferentes do MESMO evento em datas diferentes N√ÉO s√£o duplicatas (ex: "Show X" dia 12 e "Show X" dia 13)

TAREFA:
Retorne JSON estruturado:
{{
    "verified_events": [eventos aprovados com descri√ß√£o enriquecida],
    "rejected_events": [eventos rejeitados com motivo claro],
    "warnings": [avisos gerais sobre valida√ß√µes],
    "duplicates_removed": [lista de duplicatas removidas]
}}

IMPORTANTE:
- Para cada evento aprovado, ENRIQUE√áA a descri√ß√£o se ela estiver muito curta
- Para eventos rejeitados, explique CLARAMENTE o motivo
- Seja mais PERMISSIVO com links gen√©ricos de plataformas confi√°veis
- Valide dias da semana usando a lista de s√°bados/domingos fornecida acima
"""

        try:
            response = self.agent.run(prompt)
            content = response.content

            # Tentar extrair JSON da resposta
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            verified_data = json.loads(content)
            return verified_data

        except Exception as e:
            logger.error(f"Erro ao verificar com LLM: {e}")
            return {
                "verified_events": [],
                "rejected_events": [],
                "warnings": [f"Erro na verifica√ß√£o: {str(e)}"],
            }

    def get_verification_stats(self, verified_data: dict[str, Any]) -> dict[str, int]:
        """Retorna estat√≠sticas da verifica√ß√£o."""
        return {
            "total_verified": len(verified_data.get("verified_events", [])),
            "total_rejected": len(verified_data.get("rejected_events", [])),
            "total_warnings": len(verified_data.get("warnings", [])),
            "duplicates_removed": len(verified_data.get("duplicates_removed", [])),
        }
