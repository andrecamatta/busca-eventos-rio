"""Agente de busca de eventos."""

import asyncio
import json
import logging
from datetime import datetime
from typing import Any

from pydantic import ValidationError

from agents.base_agent import BaseAgent
from config import SEARCH_CONFIG, MAX_EVENTS_PER_VENUE, ENABLED_CATEGORIES, ENABLED_VENUES
from models.event_models import ResultadoBuscaCategoria
from utils.deduplicator import deduplicate_events
from utils.prompt_templates import PromptBuilder
from utils.prompt_loader import get_prompt_loader
from utils.date_helpers import DateParser

logger = logging.getLogger(__name__)


class SearchAgent(BaseAgent):
    """Agente responsÃ¡vel por buscar eventos em mÃºltiplas fontes."""

    def __init__(self):
        super().__init__(
            agent_name="SearchAgent",
            log_emoji="ğŸ”",
            model_type="search",  # perplexity/sonar-pro
            description="Agente com busca web em tempo real para encontrar eventos culturais no Rio de Janeiro",
            instructions=[
                f"VocÃª tem acesso Ã  busca web em tempo real. Use para encontrar eventos no Rio de Janeiro "
                f"entre {SEARCH_CONFIG['start_date'].strftime('%d/%m/%Y')} "
                f"e {SEARCH_CONFIG['end_date'].strftime('%d/%m/%Y')}",
                "Busque nas seguintes categorias:",
                "1. Shows de jazz no Rio (prÃ³ximas 3 semanas)",
                "2. Teatro comÃ©dia/stand-up no Rio (EXCETO eventos infantis)",
                "3. Eventos na Casa do Choro, Sala CecÃ­lia Meireles e Teatro Municipal",
                "4. Eventos ao ar livre em fim de semana no Rio",
                "Para cada evento, extrair: tÃ­tulo, data completa, horÃ¡rio, local, valor/preÃ§o, link para compra de ingressos",
                "Buscar em sites como: Sympla, Eventbrite, Fever, TimeOut Rio, sites oficiais dos locais",
                "Retorne no formato JSON estruturado",
            ],
            markdown=True,
        )

        # Renomear agent para compatibilidade
        self.search_agent = self.agent

    def _initialize_dependencies(self, **kwargs):
        """Inicializa prompt loader."""
        self.prompt_loader = get_prompt_loader()
        self.log_info(
            f"ğŸ“‹ Prompts carregados: "
            f"{len(self.prompt_loader.get_all_categorias())} categorias, "
            f"{len(self.prompt_loader.get_all_venues())} venues"
        )

    def _limit_events_per_venue(self, eventos_por_venue: dict[str, list[dict]]) -> dict[str, list[dict]]:
        """
        Limita eventos por venue ao mÃ¡ximo definido em MAX_EVENTS_PER_VENUE.

        CritÃ©rios de priorizaÃ§Ã£o (em ordem):
        1. Eventos com link vÃ¡lido (prioridade alta)
        2. Diversidade de datas (evita concentraÃ§Ã£o no mesmo dia)
        3. Completude da descriÃ§Ã£o (mais informaÃ§Ã£o = melhor)
        4. Ordem cronolÃ³gica (mais prÃ³ximos primeiro)
        """
        limited_events = {}

        for venue_name, eventos in eventos_por_venue.items():
            if len(eventos) <= MAX_EVENTS_PER_VENUE:
                limited_events[venue_name] = eventos
                continue

            # Calcular score para cada evento
            scored_events = []
            for evento in eventos:
                score = 0

                # 1. Link vÃ¡lido = +100 pontos
                if evento.get("link_ingresso") and evento["link_ingresso"].lower() not in ("null", "none", ""):
                    score += 100

                # 2. DescriÃ§Ã£o completa = +50 pontos (se > 50 palavras)
                descricao = evento.get("descricao", "") or ""
                if len(descricao.split()) > 50:
                    score += 50
                elif len(descricao.split()) > 20:
                    score += 25

                # 3. Data mais prÃ³xima = +1 a +30 pontos (inverso da posiÃ§Ã£o)
                try:
                    data_str = evento.get("data", "")
                    if data_str:
                        # Parsear DD/MM/YYYY
                        data_evento = datetime.strptime(data_str, "%d/%m/%Y")
                        # Quanto mais prÃ³ximo, maior o score (max 30 pontos)
                        days_diff = (data_evento - datetime.now()).days
                        if days_diff >= 0:
                            # Normalizar: 0-21 dias â†’ 30-10 pontos
                            score += max(10, 30 - days_diff)
                except:
                    score += 15  # score neutro se data invÃ¡lida

                scored_events.append((score, evento))

            # Ordenar por score (maior primeiro)
            scored_events.sort(key=lambda x: x[0], reverse=True)

            # Selecionar top MAX_EVENTS_PER_VENUE
            selected = [evento for _, evento in scored_events[:MAX_EVENTS_PER_VENUE]]
            limited_events[venue_name] = selected

            # Log da reduÃ§Ã£o
            if len(eventos) > MAX_EVENTS_PER_VENUE:
                logger.info(
                    f"ğŸ“Š Venue '{venue_name}': {len(eventos)} eventos â†’ "
                    f"{len(selected)} selecionados (limite: {MAX_EVENTS_PER_VENUE})"
                )

        return limited_events

    def _normalize_venue_names(self, eventos_por_venue: dict[str, list[dict]]) -> dict[str, list[dict]]:
        """
        Consolida sub-venues em venues principais usando VENUE_ALIASES.

        Exemplo: "CCBB Teatro III" â†’ "CCBB Rio - Centro Cultural Banco do Brasil"
        """
        from config import VENUE_ALIASES

        normalized = {}
        consolidation_log = []

        for venue_name, eventos in eventos_por_venue.items():
            # Obter nome canÃ´nico do venue
            canonical_name = VENUE_ALIASES.get(venue_name, venue_name)

            # Log de consolidaÃ§Ã£o se houve mudanÃ§a
            if canonical_name != venue_name and len(eventos) > 0:
                consolidation_log.append(f"{venue_name} â†’ {canonical_name} ({len(eventos)} eventos)")

            # Merge eventos no venue canÃ´nico
            if canonical_name not in normalized:
                normalized[canonical_name] = []
            normalized[canonical_name].extend(eventos)

        # Log consolidaÃ§Ãµes realizadas
        if consolidation_log:
            logger.info(f"ğŸ”— ConsolidaÃ§Ã£o de venues:")
            for log_msg in consolidation_log:
                logger.info(f"   - {log_msg}")

        return normalized

    async def _run_micro_search(self, prompt: str, search_name: str) -> str:
        """Executa uma micro-search focada de forma assÃ­ncrona."""
        logger.info(f"   ğŸ” Iniciando busca: {search_name}")

        def sync_search():
            try:
                response = self.search_agent.run(prompt)
                return response.content
            except Exception as e:
                logger.error(f"Erro na busca {search_name}: {e}")
                return "{}"

        result = await asyncio.to_thread(sync_search)

        # Log resposta do Perplexity para diagnÃ³stico (primeiros 500 chars)
        if result and isinstance(result, str) and result.strip():
            preview = result[:500].replace('\n', ' ')
            logger.debug(f"   ğŸ“„ Resposta Perplexity [{search_name}]: {preview}...")

        logger.info(f"   âœ“ Busca concluÃ­da: {search_name}")
        return result

    def _deduplicate_events_by_title(self, events: list[dict]) -> list[dict]:
        """
        Deduplica eventos por similaridade de tÃ­tulo.

        Remove eventos duplicados baseado em:
        1. TÃ­tulos idÃªnticos (case-insensitive)
        2. TÃ­tulos muito similares (>85% de similaridade)

        Args:
            events: Lista de eventos para deduplicar

        Returns:
            Lista de eventos Ãºnicos, priorizando os mais completos
        """
        from difflib import SequenceMatcher

        if not events:
            return []

        unique_events = []
        seen_titles = []

        # Ordenar por completude (eventos com link e descriÃ§Ã£o primeiro)
        sorted_events = sorted(
            events,
            key=lambda e: (
                bool(e.get('link_ingresso')),
                bool(e.get('descricao')),
                len(str(e.get('titulo', '')))
            ),
            reverse=True
        )

        for event in sorted_events:
            titulo = str(event.get('titulo', '')).strip().lower()
            if not titulo:
                continue

            # Verificar se Ã© similar a algum tÃ­tulo jÃ¡ visto
            is_duplicate = False
            for seen_title in seen_titles:
                similarity = SequenceMatcher(None, titulo, seen_title).ratio()
                if similarity > 0.85:  # 85% de similaridade
                    is_duplicate = True
                    logger.debug(f"      Duplicata detectada: '{titulo}' vs '{seen_title}' ({similarity:.2%})")
                    break

            if not is_duplicate:
                unique_events.append(event)
                seen_titles.append(titulo)

        return unique_events

    async def _run_parallel_micro_search(
        self,
        prompt: str,
        search_name: str,
        n_parallel: int = 3
    ) -> list[dict]:
        """
        Executa mÃºltiplas buscas paralelas e merge os resultados.

        EstratÃ©gia: Fazer N consultas simultÃ¢neas ao Perplexity para aumentar
        cobertura, jÃ¡ que cada chamada pode retornar eventos diferentes devido
        Ã  variabilidade do modelo.

        Args:
            prompt: Prompt de busca
            search_name: Nome da busca (para logs)
            n_parallel: NÃºmero de consultas paralelas (padrÃ£o: 3)

        Returns:
            Lista de eventos Ãºnicos (deduplificados) de todas as consultas
        """
        import json

        logger.info(f"   ğŸ”âš¡ Iniciando {n_parallel} buscas paralelas: {search_name}")

        # Executar N buscas em paralelo
        tasks = [
            self._run_micro_search(prompt, f"{search_name} #{i+1}")
            for i in range(n_parallel)
        ]

        results = await asyncio.gather(*tasks)

        # Parsear e coletar todos os eventos
        all_events = []
        successful_queries = 0

        for i, result_str in enumerate(results, 1):
            try:
                # Limpar JSON de markdown/texto extra
                clean_json = self._clean_json_from_markdown(result_str)
                if not clean_json:
                    logger.warning(f"      Query #{i}: JSON vazio apÃ³s limpeza")
                    continue

                data = json.loads(clean_json)
                events = data.get('eventos', [])

                if events:
                    all_events.extend(events)
                    successful_queries += 1
                    logger.info(f"      Query #{i}: {len(events)} eventos encontrados")
                else:
                    logger.info(f"      Query #{i}: Nenhum evento encontrado")

            except json.JSONDecodeError as e:
                logger.warning(f"      Query #{i}: Erro ao parsear JSON - {e}")
            except Exception as e:
                logger.error(f"      Query #{i}: Erro inesperado - {e}")

        # Deduplicar eventos
        unique_events = self._deduplicate_events_by_title(all_events)

        logger.info(
            f"   âœ“ Busca paralela concluÃ­da: {search_name} - "
            f"{len(all_events)} eventos brutos â†’ {len(unique_events)} Ãºnicos "
            f"({successful_queries}/{n_parallel} queries OK)"
        )

        return unique_events

    def _clean_json_from_markdown(self, text: str) -> str:
        """Remove markdown code blocks e texto extra do JSON.

        JÃ¡ existe implementaÃ§Ã£o similar, mas criando versÃ£o standalone.
        """
        import re

        if not text or text.strip() == "":
            return ""

        text = text.strip()

        # STEP 1: Extrair JSON de dentro de ```json blocks
        code_block_pattern = r'```(?:json)?\s*([\s\S]*?)\s*```'
        matches = re.findall(code_block_pattern, text)
        if matches:
            text = matches[-1].strip()

        # STEP 2: Remover texto ANTES do primeiro { ou [
        json_start_brace = text.find('{')
        json_start_bracket = text.find('[')
        valid_starts = [pos for pos in [json_start_brace, json_start_bracket] if pos != -1]
        if valid_starts:
            json_start = min(valid_starts)
            text = text[json_start:]

        # STEP 3: Remover texto DEPOIS do Ãºltimo } ou ]
        if text.startswith('{'):
            depth = 0
            for i, char in enumerate(text):
                if char == '{':
                    depth += 1
                elif char == '}':
                    depth -= 1
                    if depth == 0:
                        text = text[:i+1]
                        break
        elif text.startswith('['):
            depth = 0
            for i, char in enumerate(text):
                if char == '[':
                    depth += 1
                elif char == ']':
                    depth -= 1
                    if depth == 0:
                        text = text[:i+1]
                        break

        return text.strip()

    def _build_focused_prompt(
        self,
        categoria: str,
        tipo_busca: str,  # "categoria" ou "venue"
        descricao: str,
        tipos_evento: list[str],
        palavras_chave: list[str],
        venues_sugeridos: list[str],
        instrucoes_especiais: str = "",
        start_date_str: str = "",
        end_date_str: str = "",
        month_year_str: str = "",
        month_str: str = "",
    ) -> str:
        """ConstrÃ³i prompt focado para uma Ãºnica categoria ou venue (DRY)."""

        # Template comum para todos os prompts
        common_header = f"""Execute uma busca FOCADA e DETALHADA exclusivamente para: {categoria}

PERÃODO: {start_date_str} a {end_date_str}

ğŸ¯ FOCO EXCLUSIVO: {descricao}

ESTRATÃ‰GIA DE BUSCA:
"""

        # SeÃ§Ã£o de tipos de evento
        tipos_section = "TIPOS DE EVENTO:\n"
        for tipo in tipos_evento:
            tipos_section += f"- {tipo}\n"

        # SeÃ§Ã£o de palavras-chave
        keywords_section = "\nPALAVRAS-CHAVE PARA BUSCA:\n"
        for keyword in palavras_chave:
            keywords_section += f'- "{keyword}"\n'

        # SeÃ§Ã£o de venues
        venues_section = "\nVENUES/LOCAIS PRIORITÃRIOS:\n"
        for venue in venues_sugeridos:
            venues_section += f"- {venue}\n"

        # Fontes (comum para todos)
        sources_section = """
FONTES PARA BUSCAR:
- Sympla (sympla.com.br), Eventbrite (eventbrite.com.br), Fever (fever.com.br)
- Portais culturais: TimeOut Rio, Veja Rio, O Globo Cultura
- Sites oficiais dos venues e suas redes sociais (Instagram/Facebook)
- Bilheterias online oficiais dos locais
"""

        # Campos obrigatÃ³rios (comum para todos)
        required_fields = """
INFORMAÃ‡Ã•ES OBRIGATÃ“RIAS PARA CADA EVENTO:
- Nome completo do evento
- Data exata (formato DD/MM/YYYY)
- âš ï¸ HorÃ¡rio de inÃ­cio (HH:MM) - CRÃTICO: SEMPRE inclua o horÃ¡rio preciso
- Nome completo do local/venue + endereÃ§o
- PreÃ§o (incluir meia-entrada se disponÃ­vel)
- Link para compra de ingressos (se disponÃ­vel)
- DescriÃ§Ã£o detalhada: artistas, duraÃ§Ã£o, pÃºblico-alvo

ATENÃ‡ÃƒO ESPECIAL AO HORÃRIO:
- O horÃ¡rio Ã© OBRIGATÃ“RIO (nÃ£o opcional)
- Formato: "19:00", "20:30", "21:00" (HH:MM)
- Se o site nÃ£o mostrar horÃ¡rio, busque em Instagram, Facebook, Sympla, Eventbrite
- NUNCA deixe horÃ¡rio em branco
"""

        # Formato de retorno (diferente para categoria vs venue)
        if tipo_busca == "categoria":
            return_format = f"""
FORMATO DE RETORNO:
{{
  "eventos": [
    {{
      "categoria": "{categoria}",
      "titulo": "Nome do evento",
      "data": "DD/MM/YYYY",
      "horario": "HH:MM",
      "local": "Nome completo + EndereÃ§o",
      "preco": "Valor completo",
      "link_ingresso": "URL especÃ­fica ou null",
      "descricao": "DescriÃ§Ã£o detalhada"
    }}
  ]
}}

IMPORTANTE:
- Busque o MÃXIMO de eventos possÃ­vel (objetivo: pelo menos 3 eventos)
- INCLUA TODOS os eventos que encontrar com data, horÃ¡rio, local e descriÃ§Ã£o

âš ï¸ REGRAS CRÃTICAS PARA LINKS (leia com atenÃ§Ã£o - links invÃ¡lidos serÃ£o rejeitados):

1. NÃƒO RETORNE HOMEPAGES/SITES INSTITUCIONAIS:
   âŒ NUNCA retornar sites de ARTISTAS (ex: raphaelghanem.com.br, fabriciolins.com.br)
   âŒ NUNCA retornar homepages de VENUES (ex: casadochoro.com.br, teatroopuscitta.com.br)
   âŒ NUNCA retornar homepages de PLATAFORMAS (ex: sympla.com.br, ingresso.com)
   âŒ NUNCA retornar AGREGADORES genÃ©ricos (ex: shazam.com/events, concerts50.com, songkick.com)
   âŒ NUNCA retornar pÃ¡ginas de PROGRAMAÃ‡ÃƒO GERAL (ex: /programacao, /agenda, /calendario)

2. O link DEVE conter IDENTIFICADOR ÃšNICO do evento (um destes formatos):
   - ID numÃ©rico: /evento/nome-do-evento/123456
   - Slug com data: /evento-nome-18-11-2025
   - Hash alfanumÃ©rico: /shows/nome__abc123de/
   - ParÃ¢metro Ãºnico: ?event_id=789 ou ?eve_cod=15246

3. PLATAFORMAS DE BUSCA (nesta ordem de prioridade):
   ğŸ¥‡ PRIORITÃRIAS (sempre buscar primeiro):
   a) Sympla: sympla.com.br/evento/[nome]/[ID-numerico]
   b) Eventbrite: eventbrite.com.br/e/[nome]-tickets-[ID]
   c) Ticketmaster: ticketmaster.com.br/event/[ID]
   d) Fever: feverup.com/rio-de-janeiro/events/[nome-evento]

   ğŸ¥ˆ SECUNDÃRIAS (se prioritÃ¡rias nÃ£o tiverem):
   e) Ingresso.com: ingresso.com/evento/[nome]/[ID]
   f) Bileto: bileto.sympla.com.br/event/[ID]

   ğŸ¥‰ VENUES ESPECÃFICOS (apenas com pÃ¡gina do evento):
   g) Blue Note: bluenoterio.com.br/shows/[nome-show]__[hash]/
   h) Sites oficiais com link ESPECÃFICO do evento (NÃƒO homepage)

âœ… EXEMPLOS DE LINKS VÃLIDOS:
   âœ… https://www.sympla.com.br/evento/raphael-ghanem-stand-up/2345678
   âœ… https://www.eventbrite.com.br/e/quarteto-de-cordas-da-osb-tickets-987654321
   âœ… https://bluenoterio.com.br/shows/irma-you-and-my-guitar__22hz624n/
   âœ… https://www.ingresso.com/evento/caio-martins-segredo-revelado/15246

âŒ EXEMPLOS DE LINKS INVÃLIDOS (NUNCA RETORNAR):
   HOMEPAGES E SITES INSTITUCIONAIS:
   âŒ https://raphaelghanem.com.br (site oficial do artista)
   âŒ https://casadochoro.com.br (homepage do venue)
   âŒ https://teatroopuscitta.com.br (homepage do teatro)
   âŒ https://www.sympla.com.br (homepage da plataforma)

   AGREGADORES GENÃ‰RICOS (nÃ£o vendem ingressos):
   âŒ https://shazam.com/events/rio-de-janeiro (apenas lista eventos)
   âŒ https://concerts50.com/brazil/rio-de-janeiro (agregador de terceiros)

   PÃGINAS DE CATEGORIA/BUSCA/LISTAGEM:
   âŒ https://www.ingresso.com/espetaculos/categorias/stand-up (categoria genÃ©rica)
   âŒ https://www.sympla.com.br/eventos/rio-de-janeiro (listagem por cidade)
   âŒ https://eventbrite.com.br/d/brazil--rio-de-janeiro/events/ (listagem)
   âŒ https://bluenoterio.com.br/shows (listagem de todos os shows - falta ID especÃ­fico)

   PROGRAMAÃ‡ÃƒO GERAL DE VENUES:
   âŒ https://salaceliciameireles.rj.gov.br/programacao (calendÃ¡rio mensal)
   âŒ https://casadochoro.com.br/programacao (agenda geral)

ğŸ“‹ CHECKLIST ANTES DE RETORNAR UM LINK:
   âœ… O link contÃ©m ID/identificador Ãºnico? (numÃ©rico, slug, hash, ou parÃ¢metro)
   âœ… O link Ã© de uma PLATAFORMA de venda (Sympla, Eventbrite, etc) OU pÃ¡gina especÃ­fica do venue?
   âœ… O link aponta para UMA pÃ¡gina especÃ­fica de evento (nÃ£o listagem/categoria)?
   âœ… O link NÃƒO Ã© homepage do artista/venue/plataforma?
   âœ… O link NÃƒO Ã© de agregador genÃ©rico (Shazam, Concerts50, etc)?

   SE TODAS AS RESPOSTAS FOREM âœ… â†’ retornar link
   SE QUALQUER RESPOSTA FOR âŒ â†’ retornar null

4. SE NÃƒO ENCONTRAR link especÃ­fico:
   - Busque em TODAS as plataformas prioritÃ¡rias (Sympla, Eventbrite, Ticketmaster, Fever)
   - Busque em plataformas secundÃ¡rias (Ingresso.com, Bileto)
   - APENAS APÃ“S TENTAR TODAS AS FONTES: retorne null
   - NÃƒO retorne links genÃ©ricos "por garantia" (null Ã© MELHOR que link invÃ¡lido)
"""
        else:  # venue
            return_format = f"""
ENCODING E CARACTERES ESPECIAIS:
- Usar UTF-8 encoding para TODOS os campos
- Caracteres acentuados sÃ£o PERMITIDOS e DEVEM ser escritos normalmente (ex: "CecÃ­lia", "mÃºsica", "sÃ¡bado")
- NÃƒO usar escapes unicode (ex: \\u00ed) - escrever os caracteres acentuados diretamente
- A chave do JSON DEVE ser EXATAMENTE: "{categoria}" (preservar acentuaÃ§Ã£o se houver)

FORMATO DE RETORNO (use exatamente estes nomes de campos):
{{
  "{categoria}": [
    {{
      "titulo": "Nome do evento",
      "data": "DD/MM/YYYY",
      "horario": "HH:MM",
      "local": "{categoria} - EndereÃ§o completo",
      "preco": "Valor completo",
      "link_ingresso": "URL especÃ­fica ou null",
      "descricao": "DescriÃ§Ã£o detalhada"
    }}
  ]
}}

IMPORTANTE - NOMES DE CAMPOS:
- Use "horario" (nÃ£o "hora")
- Use "preco" (nÃ£o "preÃ§o")
- Use "link_ingresso" (nÃ£o "link")
- Use "descricao" (nÃ£o "descriÃ§Ã£o")

REGRAS CRÃTICAS PARA JSON:
1. Comece DIRETAMENTE com {{ (sem markdown, sem textos, sem cabeÃ§alhos antes)
2. Se usar markdown, use APENAS ```json no inÃ­cio e ``` no final
3. Feche COMPLETAMENTE o JSON antes de qualquer texto explicativo
4. NÃƒO adicione nada DEPOIS do Ãºltimo }}
5. Caracteres especiais devem ser escritos normalmente (ex: "Ã ", "Ã£", "Ã§", "Ã©", "Ã­", "Ã³", "Ã´", "Ãµ", "Ã¼")

OBJETIVO:
- Busque o MÃXIMO de eventos possÃ­vel (objetivo: pelo menos 1 evento)
- INCLUA TODOS os eventos que encontrar com data, horÃ¡rio, local e descriÃ§Ã£o

âš ï¸ REGRAS CRÃTICAS PARA LINKS (leia com atenÃ§Ã£o - links invÃ¡lidos serÃ£o rejeitados):

1. NÃƒO RETORNE HOMEPAGES/SITES INSTITUCIONAIS:
   âŒ NUNCA retornar sites de ARTISTAS (ex: raphaelghanem.com.br, fabriciolins.com.br)
   âŒ NUNCA retornar homepages de VENUES (ex: casadochoro.com.br, teatroopuscitta.com.br)
   âŒ NUNCA retornar homepages de PLATAFORMAS (ex: sympla.com.br, ingresso.com)
   âŒ NUNCA retornar AGREGADORES genÃ©ricos (ex: shazam.com/events, concerts50.com, songkick.com)
   âŒ NUNCA retornar pÃ¡ginas de PROGRAMAÃ‡ÃƒO GERAL (ex: /programacao, /agenda, /calendario)

2. O link DEVE conter IDENTIFICADOR ÃšNICO do evento (um destes formatos):
   - ID numÃ©rico: /evento/nome-do-evento/123456
   - Slug com data: /evento-nome-18-11-2025
   - Hash alfanumÃ©rico: /shows/nome__abc123de/
   - ParÃ¢metro Ãºnico: ?event_id=789 ou ?eve_cod=15246

3. PLATAFORMAS DE BUSCA (nesta ordem de prioridade):
   ğŸ¥‡ PRIORITÃRIAS (sempre buscar primeiro):
   a) Sympla: sympla.com.br/evento/[nome]/[ID-numerico]
   b) Eventbrite: eventbrite.com.br/e/[nome]-tickets-[ID]
   c) Ticketmaster: ticketmaster.com.br/event/[ID]
   d) Fever: feverup.com/rio-de-janeiro/events/[nome-evento]

   ğŸ¥ˆ SECUNDÃRIAS (se prioritÃ¡rias nÃ£o tiverem):
   e) Ingresso.com: ingresso.com/evento/[nome]/[ID]
   f) Bileto: bileto.sympla.com.br/event/[ID]

   ğŸ¥‰ VENUES ESPECÃFICOS (apenas com pÃ¡gina do evento):
   g) Blue Note: bluenoterio.com.br/shows/[nome-show]__[hash]/
   h) Sites oficiais com link ESPECÃFICO do evento (NÃƒO homepage)

âœ… EXEMPLOS DE LINKS VÃLIDOS:
   âœ… https://www.sympla.com.br/evento/raphael-ghanem-stand-up/2345678
   âœ… https://www.eventbrite.com.br/e/quarteto-de-cordas-da-osb-tickets-987654321
   âœ… https://bluenoterio.com.br/shows/irma-you-and-my-guitar__22hz624n/
   âœ… https://www.ingresso.com/evento/caio-martins-segredo-revelado/15246

âŒ EXEMPLOS DE LINKS INVÃLIDOS (NUNCA RETORNAR):
   HOMEPAGES E SITES INSTITUCIONAIS:
   âŒ https://raphaelghanem.com.br (site oficial do artista)
   âŒ https://casadochoro.com.br (homepage do venue)
   âŒ https://teatroopuscitta.com.br (homepage do teatro)
   âŒ https://www.sympla.com.br (homepage da plataforma)

   AGREGADORES GENÃ‰RICOS (nÃ£o vendem ingressos):
   âŒ https://shazam.com/events/rio-de-janeiro (apenas lista eventos)
   âŒ https://concerts50.com/brazil/rio-de-janeiro (agregador de terceiros)

   PÃGINAS DE CATEGORIA/BUSCA/LISTAGEM:
   âŒ https://www.ingresso.com/espetaculos/categorias/stand-up (categoria genÃ©rica)
   âŒ https://www.sympla.com.br/eventos/rio-de-janeiro (listagem por cidade)
   âŒ https://eventbrite.com.br/d/brazil--rio-de-janeiro/events/ (listagem)
   âŒ https://bluenoterio.com.br/shows (listagem de todos os shows - falta ID especÃ­fico)

   PROGRAMAÃ‡ÃƒO GERAL DE VENUES:
   âŒ https://salaceliciameireles.rj.gov.br/programacao (calendÃ¡rio mensal)
   âŒ https://casadochoro.com.br/programacao (agenda geral)

ğŸ“‹ CHECKLIST ANTES DE RETORNAR UM LINK:
   âœ… O link contÃ©m ID/identificador Ãºnico? (numÃ©rico, slug, hash, ou parÃ¢metro)
   âœ… O link Ã© de uma PLATAFORMA de venda (Sympla, Eventbrite, etc) OU pÃ¡gina especÃ­fica do venue?
   âœ… O link aponta para UMA pÃ¡gina especÃ­fica de evento (nÃ£o listagem/categoria)?
   âœ… O link NÃƒO Ã© homepage do artista/venue/plataforma?
   âœ… O link NÃƒO Ã© de agregador genÃ©rico (Shazam, Concerts50, etc)?

   SE TODAS AS RESPOSTAS FOREM âœ… â†’ retornar link
   SE QUALQUER RESPOSTA FOR âŒ â†’ retornar null

4. SE NÃƒO ENCONTRAR link especÃ­fico:
   - Busque em TODAS as plataformas prioritÃ¡rias (Sympla, Eventbrite, Ticketmaster, Fever)
   - Busque em plataformas secundÃ¡rias (Ingresso.com, Bileto)
   - APENAS APÃ“S TENTAR TODAS AS FONTES: retorne null
   - NÃƒO retorne links genÃ©ricos "por garantia" (null Ã© MELHOR que link invÃ¡lido)
"""

        # Montar prompt completo
        return (
            common_header
            + tipos_section
            + keywords_section
            + venues_section
            + sources_section
            + instrucoes_especiais
            + required_fields
            + return_format
        )

    def _get_saturdays_in_period(self, start_date, end_date) -> list[dict]:
        """
        Retorna lista de sÃ¡bados no perÃ­odo de busca.

        Args:
            start_date: Data inicial (datetime)
            end_date: Data final (datetime)

        Returns:
            Lista de dicts com info de cada sÃ¡bado: [{"date": datetime, "date_str": "15/11/2025"}, ...]
        """
        from datetime import timedelta

        saturdays = []
        current = start_date

        # Iterar dia por dia atÃ© end_date
        while current <= end_date:
            # weekday() retorna 5 para sÃ¡bado (0=segunda, 6=domingo)
            if current.weekday() == 5:
                saturdays.append({
                    "date": current,
                    "date_str": current.strftime("%d/%m/%Y")
                })
            current += timedelta(days=1)

        return saturdays


    def _build_prompt_from_config(self, config: dict, context: dict) -> str:
        """
        ConstrÃ³i prompt a partir de configuraÃ§Ã£o YAML.

        Args:
            config: ConfiguraÃ§Ã£o carregada do YAML (categoria ou venue)
            context: Contexto com variÃ¡veis de data (start_date_str, end_date_str, etc)

        Returns:
            Prompt completo formatado
        """
        # Nomes dos campos variam se Ã© categoria ou venue
        # Para categoria: venues_sugeridos
        # Para venue: pode nÃ£o ter venues_sugeridos (usar lista vazia)

        venues_list = config.get("venues_sugeridos", config.get("fontes_prioritarias", []))

        return self._build_focused_prompt(
            categoria=config["nome"],
            tipo_busca=config["tipo_busca"],
            descricao=config["descricao"],
            tipos_evento=config["tipos_evento"],
            palavras_chave=config["palavras_chave"],
            venues_sugeridos=venues_list if isinstance(venues_list, list) else [],
            instrucoes_especiais=config.get("instrucoes_especiais", ""),
            start_date_str=context["start_date_str"],
            end_date_str=context["end_date_str"],
            month_year_str=context["month_year_str"],
            month_str=context["month_str"]
        )

    def _get_search_task(self, prompt: str, search_name: str, config: dict):
        """
        Retorna tarefa de busca (paralela ou sequencial) baseado em configuraÃ§Ã£o.

        Args:
            prompt: Prompt de busca
            search_name: Nome da busca (para logs)
            config: ConfiguraÃ§Ã£o do YAML com campo opcional 'parallel_queries'

        Returns:
            Coroutine para asyncio.gather()
        """
        n_parallel = config.get("parallel_queries", 1)

        if n_parallel <= 1:
            # Busca sequencial
            return self._run_micro_search(prompt, search_name)
        else:
            # Busca paralela
            logger.info(f"   âš¡ Configurando busca paralela para {search_name} ({n_parallel} queries)")
            return self._run_parallel_micro_search(prompt, search_name, n_parallel)

    async def search_all_sources(self) -> dict[str, Any]:
        """Busca eventos usando Perplexity Sonar Pro com 6 micro-searches focadas."""
        logger.info(f"{self.log_prefix} Iniciando busca de eventos com Perplexity Sonar Pro...")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # PRIORIDADE 1: SCRAPERS CUSTOMIZADOS (Blue Note + Sala CecÃ­lia Meireles)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        logger.info(f"{self.log_prefix} ğŸ« Buscando eventos via scrapers customizados...")
        from utils.eventim_scraper import EventimScraper

        # Blue Note
        blue_note_scraped = EventimScraper.scrape_blue_note_events()
        if blue_note_scraped:
            logger.info(f"âœ“ Encontrados {len(blue_note_scraped)} eventos Blue Note no Eventim")
        else:
            logger.warning("âš ï¸  Nenhum evento Blue Note encontrado no scraper")

        # Sala CecÃ­lia Meireles
        cecilia_meireles_scraped = EventimScraper.scrape_cecilia_meireles_events()
        if cecilia_meireles_scraped:
            logger.info(f"âœ“ Encontrados {len(cecilia_meireles_scraped)} eventos Sala CecÃ­lia Meireles")
        else:
            logger.warning("âš ï¸  Nenhum evento Sala CecÃ­lia Meireles encontrado no scraper")

        # CCBB Rio
        ccbb_scraped = EventimScraper.scrape_ccbb_events()
        if ccbb_scraped:
            logger.info(f"âœ“ Encontrados {len(ccbb_scraped)} eventos CCBB")
        else:
            logger.warning("âš ï¸  Nenhum evento CCBB encontrado no scraper")

        # Teatro Municipal (Fever - JSON-LD)
        teatro_municipal_scraped = EventimScraper.scrape_teatro_municipal_fever_events()
        if teatro_municipal_scraped:
            logger.info(f"âœ“ Encontrados {len(teatro_municipal_scraped)} eventos Teatro Municipal (Fever)")
        else:
            logger.warning("âš ï¸  Nenhum evento Teatro Municipal encontrado no scraper Fever")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CARREGAR PROMPTS DO YAML
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Construir contexto de datas para interpolaÃ§Ã£o
        context = self.prompt_loader.build_context(
            SEARCH_CONFIG['start_date'],
            SEARCH_CONFIG['end_date']
        )

        logger.info(f"{self.log_prefix} Criando prompts a partir do YAML...")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CARREGAMENTO DINÃ‚MICO DE CATEGORIAS E VENUES (baseado em config.py)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Importar configuraÃ§Ãµes habilitadas
        categorias_ids = ENABLED_CATEGORIES  # Vem de config.py
        venues_ids = ENABLED_VENUES  # Vem de config.py

        logger.info(f"{self.log_prefix} ConfiguraÃ§Ã£o ativa:")
        logger.info(f"{self.log_prefix}   Categorias habilitadas ({len(categorias_ids)}): {', '.join(categorias_ids) if categorias_ids else 'NENHUMA'}")
        logger.info(f"{self.log_prefix}   Venues habilitados ({len(venues_ids)}): {', '.join(venues_ids) if venues_ids else 'NENHUM'}")

        # Carregar configuraÃ§Ãµes de categorias habilitadas e construir prompts
        prompts_categorias = {}
        configs_categorias = {}

        for cat_id in categorias_ids:
            config = self.prompt_loader.get_categoria(cat_id, context)
            prompts_categorias[cat_id] = self._build_prompt_from_config(config, context)
            configs_categorias[cat_id] = config

        # Carregar configuraÃ§Ãµes de venues habilitados e construir prompts
        prompts_venues = {}
        configs_venues = {}

        for venue_id in venues_ids:
            config = self.prompt_loader.get_venue(venue_id, context)
            prompts_venues[venue_id] = self._build_prompt_from_config(config, context)
            configs_venues[venue_id] = config

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Calcular total de prompts
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Calcular total de prompts (categorias + venues)
        total_categorias = len(categorias_ids)
        total_prompts = total_categorias + len(venues_ids)
        logger.info(f"{self.log_prefix} âœ… {total_prompts} prompts criados com sucesso")

        try:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # EXECUÃ‡ÃƒO PARALELA DAS MICRO-SEARCHES (DINÃ‚MICO)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info(f"{self.log_prefix} Executando {total_prompts} micro-searches em paralelo...")

            # Construir lista de searches dinamicamente com metadados
            searches = []
            search_metadata = []  # Rastreamento de tipo/id/nome para cada busca

            # Adicionar categorias habilitadas (exceto outdoor_parques - tratado separadamente)
            for cat_id in categorias_ids:
                if cat_id == "outdoor_parques":
                    continue  # SubstituÃ­do por buscas de sÃ¡bados

                # Obter display name do YAML
                display_name = configs_categorias[cat_id].get("nome", cat_id)

                searches.append(self._get_search_task(
                    prompts_categorias[cat_id],
                    display_name,
                    configs_categorias[cat_id]
                ))
                search_metadata.append({
                    "type": "category",
                    "id": cat_id,
                    "name": display_name
                })
                logger.debug(f"   âœ“ Adicionada busca de categoria: {display_name}")

            # Adicionar venues habilitados
            for venue_id in venues_ids:
                # Obter display name do YAML
                display_name = configs_venues[venue_id].get("nome", venue_id)

                searches.append(self._get_search_task(
                    prompts_venues[venue_id],
                    display_name,
                    configs_venues[venue_id]
                ))
                search_metadata.append({
                    "type": "venue",
                    "id": venue_id,
                    "name": display_name
                })
                logger.debug(f"   âœ“ Adicionada busca de venue: {display_name}")

            logger.info(f"{self.log_prefix} âœ… {len(searches)} buscas preparadas: {len([m for m in search_metadata if m['type'] == 'category'])} categorias, {len([m for m in search_metadata if m['type'] == 'saturday'])} sÃ¡bados, {len([m for m in search_metadata if m['type'] == 'venue'])} venues")

            # Executar todas as buscas em paralelo
            results = await asyncio.gather(*searches)

            logger.info(f"âœ“ Todas as {total_prompts} micro-searches concluÃ­das")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # PROCESSAR RESULTADOS DINAMICAMENTE (baseado em search_metadata)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Resultados sÃ£o organizados por Ã­ndice seguindo a ordem de search_metadata
            logger.info(f"{self.log_prefix} ğŸ“¦ Processando {len(results)} resultados...")

            # Mapear resultados para seus metadados
            results_map = {}
            for i, metadata in enumerate(search_metadata):
                result_type = metadata["type"]
                result_id = metadata["id"]
                result_name = metadata["name"]

                # Armazenar resultado com seus metadados
                results_map[i] = {
                    "result": results[i],
                    "metadata": metadata
                }

                logger.debug(f"   [{i}] {result_type.upper()}: {result_name}")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # MERGE INTELIGENTE DOS RESULTADOS COM PYDANTIC
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info("ğŸ”— Fazendo merge dos resultados...")

            # Helper function: Filter events by date
            def filter_events_by_date(eventos: list[dict], search_name: str) -> list[dict]:
                """
                Filtra eventos com datas invÃ¡lidas (fora do perÃ­odo).

                CRÃTICO: Perplexity retorna 5-48% de eventos fora do perÃ­odo,
                mesmo com instruÃ§Ãµes explÃ­citas. Este filtro Ã© obrigatÃ³rio.
                """
                if not eventos:
                    return []

                start_date = SEARCH_CONFIG['start_date']
                end_date = SEARCH_CONFIG['end_date']

                valid_events = []
                invalid_events = []

                for evento in eventos:
                    # Tentar mÃºltiplos campos de data
                    date_str = (
                        evento.get('data') or
                        evento.get('data_completa') or
                        evento.get('data_inicio') or
                        ''
                    )

                    if not date_str:
                        invalid_events.append((evento.get('titulo', 'SEM TÃTULO'), 'Sem campo de data'))
                        continue

                    # Validar data
                    validation = DateParser.validate_event_date(date_str, start_date, end_date)

                    if validation['is_valid']:
                        valid_events.append(evento)
                    else:
                        titulo = evento.get('titulo', 'SEM TÃTULO')
                        invalid_events.append((titulo, validation['reason']))

                # Log estatÃ­sticas de filtro
                total = len(eventos)
                filtered = len(invalid_events)

                if filtered > 0:
                    pct = (filtered / total * 100) if total > 0 else 0
                    logger.warning(
                        f"âš ï¸  {search_name}: Filtrados {filtered}/{total} eventos ({pct:.0f}%) "
                        f"com datas invÃ¡lidas"
                    )
                    for titulo, reason in invalid_events[:3]:  # Log primeiros 3
                        logger.debug(f"   â€¢ {titulo}: {reason}")
                    if len(invalid_events) > 3:
                        logger.debug(f"   ... e mais {len(invalid_events) - 3} eventos")
                else:
                    logger.info(f"âœ“ {search_name}: Todos os {total} eventos tÃªm datas vÃ¡lidas")

                return valid_events

            # Helper function: Clean markdown from JSON
            def clean_json_from_markdown(text: str) -> str:
                """Remove markdown code blocks and extra text from JSON responses.

                Handles cases like:
                - # Eventos na Sala CecÃ­lia Meireles\n\nCom base na busca...\n\n```json\n{...}\n```
                - Plain JSON with preamble text
                - Multiple markdown blocks (uses last one)
                """
                if not text or text.strip() == "":
                    return ""

                import re

                # Remove leading/trailing whitespace
                text = text.strip()

                # STEP 1: Try to find JSON within markdown code blocks
                # Pattern: ```json ... ``` or ``` ... ```
                code_block_pattern = r'```(?:json)?\s*([\s\S]*?)\s*```'
                matches = re.findall(code_block_pattern, text)
                if matches:
                    # Use the last match (usually the complete JSON)
                    text = matches[-1].strip()

                # STEP 2: Remove ANYTHING before the first { or [
                # This handles preamble text like headers, explanations, etc.
                json_start_brace = text.find('{')
                json_start_bracket = text.find('[')

                # Find the earliest valid JSON start
                valid_starts = [pos for pos in [json_start_brace, json_start_bracket] if pos != -1]
                if valid_starts:
                    json_start = min(valid_starts)
                    text = text[json_start:]

                # STEP 3: Remove ANYTHING after the last } or ]
                # Find matching closing bracket
                if text.startswith('{'):
                    # Find the last } that matches the structure
                    depth = 0
                    for i, char in enumerate(text):
                        if char == '{':
                            depth += 1
                        elif char == '}':
                            depth -= 1
                            if depth == 0:
                                text = text[:i+1]
                                break
                elif text.startswith('['):
                    # Find the last ] that matches the structure
                    depth = 0
                    for i, char in enumerate(text):
                        if char == '[':
                            depth += 1
                        elif char == ']':
                            depth -= 1
                            if depth == 0:
                                text = text[:i+1]
                                break

                return text.strip()

            # Helper function: Parse categoria com Pydantic
            def safe_parse_categoria(result_str: str, search_name: str) -> list[dict]:
                """Parse categoria usando Pydantic validation."""
                try:
                    # ğŸ” DEBUG: Mostrar detalhes da string recebida
                    logger.info(f"ğŸ” DEBUG [{search_name}] String recebida:")
                    logger.info(f"   â€¢ Tipo: {type(result_str)}")
                    logger.info(f"   â€¢ Length: {len(result_str) if result_str else 'None'}")
                    if result_str:
                        logger.info(f"   â€¢ Primeiros 50 chars (repr): {repr(result_str[:50])}")
                        logger.info(f"   â€¢ Ãšltimos 50 chars (repr): {repr(result_str[-50:])}")
                        logger.info(f"   â€¢ Apenas whitespace? {result_str.isspace()}")
                        logger.info(f"   â€¢ Length apÃ³s strip(): {len(result_str.strip())}")
                    else:
                        logger.info(f"   â€¢ String Ã© None ou vazia")

                    if not result_str or not isinstance(result_str, str) or result_str.strip() == "":
                        logger.warning(f"âš ï¸  Busca {search_name} retornou vazio")
                        return []
                    # Limpar markdown antes de parsear
                    clean_json = clean_json_from_markdown(result_str)
                    if not clean_json:
                        logger.warning(f"âš ï¸  Busca {search_name} retornou JSON vazio apÃ³s limpeza")
                        return []
                    # Use Pydantic para validar e parsear
                    resultado = ResultadoBuscaCategoria.model_validate_json(clean_json)
                    logger.info(f"âœ“ Busca {search_name}: {len(resultado.eventos)} eventos validados")
                    # Converter Pydantic models para dicts
                    eventos = [evento.model_dump() for evento in resultado.eventos]
                    # FILTRO CRÃTICO: Remover eventos com datas invÃ¡lidas
                    eventos_filtrados = filter_events_by_date(eventos, search_name)
                    return eventos_filtrados
                except ValidationError as e:
                    logger.error(f"âŒ Schema invÃ¡lido na busca {search_name}:")
                    for error in e.errors():
                        logger.error(f"   â€¢ {error['loc']}: {error['msg']}")
                    logger.error(f"   ConteÃºdo (primeiros 200 chars): {result_str[:200]}")
                    return []
                except Exception as e:
                    logger.error(f"âŒ Erro inesperado na busca {search_name}: {e}")
                    return []

            # Helper function: Parse venue (formato diferente, mantÃ©m dict)
            def safe_parse_venue(result_str: str, venue_name: str) -> list[dict]:
                """Parse venue usando JSON simples (formato: {venue_name: [eventos]}).

                Inclui fallback com normalizaÃ§Ã£o unicode para lidar com acentuaÃ§Ã£o.
                """
                try:
                    import unicodedata

                    if not result_str or not isinstance(result_str, str) or result_str.strip() == "":
                        logger.warning(f"âš ï¸  Busca {venue_name} retornou vazio")
                        return []
                    # Limpar markdown antes de parsear
                    clean_json = clean_json_from_markdown(result_str)
                    if not clean_json:
                        logger.warning(f"âš ï¸  Busca {venue_name} retornou JSON vazio apÃ³s limpeza")
                        return []
                    data = json.loads(clean_json)

                    # STEP 1: Tentar match exato primeiro
                    eventos = data.get(venue_name, [])

                    # STEP 2: Se nÃ£o encontrou, tentar com normalizaÃ§Ã£o unicode (fallback)
                    if not eventos and venue_name:
                        # Normalizar nome do venue esperado (NFD = decompor acentos)
                        normalized_expected = unicodedata.normalize('NFD', venue_name)

                        # Tentar encontrar chave com normalizaÃ§Ã£o
                        for key in data.keys():
                            normalized_key = unicodedata.normalize('NFD', key)
                            if normalized_key == normalized_expected:
                                eventos = data.get(key, [])
                                logger.info(
                                    f"âš™ï¸  Fallback unicode: '{key}' â†’ '{venue_name}' "
                                    f"({len(eventos)} eventos)"
                                )
                                break

                    if eventos:
                        logger.info(f"âœ“ Busca {venue_name}: {len(eventos)} eventos encontrados")
                        # FILTRO CRÃTICO: Remover eventos com datas invÃ¡lidas
                        eventos_filtrados = filter_events_by_date(eventos, venue_name)
                        return eventos_filtrados
                    else:
                        logger.warning(f"âš ï¸  Nenhum evento encontrado para {venue_name} (chaves disponÃ­veis: {list(data.keys())})")
                        return []
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSON invÃ¡lido na busca {venue_name}: {e}")
                    logger.error(f"   ConteÃºdo (primeiros 200 chars): {result_str[:200]}")
                    return []

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # PARSE DINÃ‚MICO DE RESULTADOS (baseado em search_metadata)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info(f"{self.log_prefix} ğŸ”„ Iniciando parse dinÃ¢mico de {len(results)} resultados...")

            # ColeÃ§Ãµes de eventos por tipo
            eventos_categorias = {}  # {categoria_id: [eventos]}
            eventos_outdoor_saturdays = []  # Lista consolidada de eventos outdoor de sÃ¡bados
            eventos_venues = {}  # {venue_display_name: [eventos]}

            # Iterar sobre todos os resultados usando metadados
            for i, result_data in results_map.items():
                result_str = result_data["result"]
                metadata = result_data["metadata"]
                result_type = metadata["type"]
                result_id = metadata["id"]
                result_name = metadata["name"]

                logger.debug(f"{self.log_prefix} Processando resultado {i}: {result_type}/{result_id}")

                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # CATEGORIAS: Parse com safe_parse_categoria
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                if result_type == "category":
                    eventos_parsed = safe_parse_categoria(result_str, result_name)
                    eventos_categorias[result_id] = eventos_parsed
                    logger.debug(f"   âœ“ Categoria '{result_name}': {len(eventos_parsed)} eventos")

                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # SÃBADOS OUTDOOR: Consolidar todos em uma Ãºnica lista
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                elif result_type == "saturday":
                    eventos_parsed = safe_parse_categoria(result_str, result_name)
                    saturday_date = metadata["saturday_data"]["date_str"]
                    if eventos_parsed:
                        logger.info(f"   âœ“ SÃ¡bado {saturday_date}: {len(eventos_parsed)} eventos outdoor")
                        eventos_outdoor_saturdays.extend(eventos_parsed)
                    else:
                        logger.debug(f"   âš ï¸  SÃ¡bado {saturday_date}: 0 eventos outdoor")

                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # VENUES: Parse com safe_parse_venue
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                elif result_type == "venue":
                    eventos_parsed = safe_parse_venue(result_str, result_name)
                    eventos_venues[result_name] = eventos_parsed
                    logger.debug(f"   âœ“ Venue '{result_name}': {len(eventos_parsed)} eventos")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # SCRAPER PRIORITY: Adicionar eventos de scrapers com prioridade
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Mapeamento de scrapers para categoria/venue
            scraper_mappings = {
                "jazz": ("categoria", "jazz", blue_note_scraped, "Blue Note Rio - Av. AtlÃ¢ntica, 1910, Copacabana, Rio de Janeiro", "Jazz"),
                "sala_cecilia": ("venue", "Sala CecÃ­lia Meireles", cecilia_meireles_scraped, "Sala CecÃ­lia Meireles - Rua da Lapa, 47, Centro, Rio de Janeiro", "Sala CecÃ­lia Meireles"),
                "teatro_municipal": ("venue", "Teatro Municipal do Rio de Janeiro", teatro_municipal_scraped, "Teatro Municipal do Rio de Janeiro - PraÃ§a Floriano, s/n, Centro, Rio de Janeiro", "Teatro Municipal do Rio de Janeiro"),
                "ccbb": ("venue", "CCBB Rio - Centro Cultural Banco do Brasil", ccbb_scraped, "CCBB Rio - Centro Cultural Banco do Brasil - Rua Primeiro de MarÃ§o, 66, Centro, Rio de Janeiro", "CCBB Rio - Centro Cultural Banco do Brasil"),
            }

            for scraper_key, (target_type, target_key, scraped_events, location, field_value) in scraper_mappings.items():
                if not scraped_events:
                    continue

                logger.info(f"ğŸ« [PRIORIDADE] Processando {len(scraped_events)} eventos do scraper {scraper_key}...")

                # Preparar eventos do scraper
                scraper_formatted = []
                for scraped_event in scraped_events:
                    event_dict = {
                        "titulo": scraped_event["titulo"],
                        "data": scraped_event["data"],
                        "horario": scraped_event["horario"],
                        "local": location,
                        "preco": "Consultar link",
                        "link_ingresso": scraped_event["link"],
                        "descricao": None,  # SerÃ¡ enriquecido depois
                        "link_valid": True  # Scraper oficial = link confiÃ¡vel
                    }

                    # Adicionar campo apropriado (categoria ou venue)
                    if target_type == "categoria":
                        event_dict["categoria"] = field_value
                    else:
                        event_dict["venue"] = field_value

                    scraper_formatted.append(event_dict)

                # Adicionar scraped events com prioridade (primeiro na lista)
                if target_type == "categoria":
                    # Categoria: mesclar com eventos Perplexity, removendo duplicatas
                    existing_events = eventos_categorias.get(target_key, [])
                    merged_events = scraper_formatted.copy()

                    # Adicionar eventos Perplexity que nÃ£o sÃ£o duplicatas
                    duplicates_count = 0
                    for perplexity_event in existing_events:
                        if not any(e.get("titulo", "").lower() == perplexity_event.get("titulo", "").lower()
                                   for e in merged_events):
                            merged_events.append(perplexity_event)
                        else:
                            duplicates_count += 1

                    eventos_categorias[target_key] = merged_events
                    logger.info(f"âœ“ Scraper {scraper_key}: {len(scraped_events)} eventos adicionados, {duplicates_count} duplicatas Perplexity removidas")

                else:  # venue
                    # Venue: mesclar com eventos Perplexity, removendo duplicatas
                    existing_events = eventos_venues.get(target_key, [])
                    merged_events = scraper_formatted.copy()

                    # Adicionar eventos Perplexity que nÃ£o sÃ£o duplicatas
                    duplicates_count = 0
                    for perplexity_event in existing_events:
                        if not any(e.get("titulo", "").lower() == perplexity_event.get("titulo", "").lower()
                                   for e in merged_events):
                            merged_events.append(perplexity_event)
                        else:
                            duplicates_count += 1

                    eventos_venues[target_key] = merged_events
                    logger.info(f"âœ“ Scraper {scraper_key}: {len(scraped_events)} eventos adicionados, {duplicates_count} duplicatas Perplexity removidas")

            # Log consolidado de sÃ¡bados outdoor
            if eventos_outdoor_saturdays:
                logger.info(f"âœ“ Total eventos outdoor (todos os sÃ¡bados): {len(eventos_outdoor_saturdays)} eventos")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # CONSOLIDAR EVENTOS OUTDOOR DOS SÃBADOS (adicionar Ã  categoria outdoor_parques)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if eventos_outdoor_saturdays:
                # Se outdoor_parques estÃ¡ habilitado, adicionar eventos dos sÃ¡bados
                if "outdoor_parques" in eventos_categorias:
                    # JÃ¡ existe - deveria estar vazio (outdoor genÃ©rico foi substituÃ­do por sÃ¡bados)
                    eventos_categorias["outdoor_parques"].extend(eventos_outdoor_saturdays)
                else:
                    # Criar categoria outdoor_parques com eventos dos sÃ¡bados
                    eventos_categorias["outdoor_parques"] = eventos_outdoor_saturdays

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # MONTAR ESTRUTURA DE EVENTOS GERAIS (todas as categorias)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            todos_eventos_gerais = []
            for cat_id, eventos_cat in eventos_categorias.items():
                todos_eventos_gerais.extend(eventos_cat)
                logger.debug(f"   Categoria '{cat_id}': {len(eventos_cat)} eventos adicionados ao merge geral")

            # OTIMIZAÃ‡ÃƒO: DeduplicaÃ§Ã£o precoce ANTES de validaÃ§Ã£o/enriquecimento
            eventos_antes_dedup = len(todos_eventos_gerais)
            todos_eventos_gerais = deduplicate_events(todos_eventos_gerais)
            eventos_removidos = eventos_antes_dedup - len(todos_eventos_gerais)
            if eventos_removidos > 0:
                logger.info(
                    f"ğŸ”„ DeduplicaÃ§Ã£o precoce: {eventos_removidos} eventos duplicados removidos "
                    f"({eventos_antes_dedup} â†’ {len(todos_eventos_gerais)})"
                )

            # Estrutura final de eventos gerais
            eventos_gerais_merged = {"eventos": todos_eventos_gerais}

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ESTRUTURA DE EVENTOS DE VENUES (jÃ¡ estÃ¡ pronta)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            eventos_locais_merged = eventos_venues

            total_venues_before = sum(len(v) for v in eventos_locais_merged.values())
            logger.info(
                f"âœ“ Merge dinÃ¢mico concluÃ­do: {len(todos_eventos_gerais)} eventos gerais, "
                f"{total_venues_before} eventos de venues"
            )

            # Normalizar nomes de venues (consolidar CCBB Teatro I/II/III, etc.)
            logger.info(f"ğŸ”— Normalizando nomes de venues...")
            eventos_locais_merged = self._normalize_venue_names(eventos_locais_merged)

            # Aplicar limitaÃ§Ã£o de eventos por venue
            logger.info(f"ğŸ“Š Aplicando limitaÃ§Ã£o de {MAX_EVENTS_PER_VENUE} eventos por venue...")
            eventos_locais_merged = self._limit_events_per_venue(eventos_locais_merged)

            total_venues_after = sum(len(v) for v in eventos_locais_merged.values())
            if total_venues_after < total_venues_before:
                logger.info(
                    f"ğŸ“Š LimitaÃ§Ã£o aplicada: {total_venues_before} eventos â†’ {total_venues_after} eventos "
                    f"({total_venues_before - total_venues_after} removidos)"
                )

            # Retornar no formato compatÃ­vel com o resto do sistema
            try:
                json_geral = json.dumps(eventos_gerais_merged, ensure_ascii=False)
                json_especial = json.dumps(eventos_locais_merged, ensure_ascii=False)

                result = {
                    "perplexity_geral": json_geral,
                    "perplexity_especial": json_especial,
                    "search_timestamp": datetime.now().isoformat(),
                }
                return result
            except Exception as json_error:
                logger.error(f"âŒ Erro na serializaÃ§Ã£o JSON: {json_error}")
                import traceback
                traceback.print_exc()
                raise

        except Exception as e:
            logger.error(f"âŒ ERRO CRÃTICO nas micro-searches: {type(e).__name__}: {e}")
            logger.error("ğŸ“ Local do erro:")
            import traceback
            import sys
            exc_type, exc_value, exc_traceback = sys.exc_info()

            # Logar o traceback completo
            logger.error("=== TRACEBACK COMPLETO ===")
            traceback.print_exc()
            logger.error("=========================")

            # Logar informaÃ§Ãµes sobre onde o erro ocorreu
            if exc_traceback:
                frame = exc_traceback.tb_frame
                lineno = exc_traceback.tb_lineno
                filename = frame.f_code.co_filename
                logger.error(f"Arquivo: {filename}, Linha: {lineno}")
                logger.error(f"FunÃ§Ã£o: {frame.f_code.co_name}")

            # Retornar JSONs vazios como fallback (para nÃ£o quebrar o pipeline)
            logger.warning("âš ï¸  Retornando JSONs vazios como fallback")
            return {
                "perplexity_geral": "{}",
                "perplexity_especial": "{}",
                "search_timestamp": datetime.now().isoformat(),
            }

    def _find_event_ticket_link_batch(self, events_batch: list[dict]) -> dict[str, str]:
        """Busca links de mÃºltiplos eventos em uma Ãºnica chamada (batch)."""
        if not events_batch:
            return {}

        # Construir prompt com lista de eventos
        eventos_texto = []
        for i, event in enumerate(events_batch, 1):
            titulo = event.get("titulo", "")
            data = event.get("data", "")
            local = event.get("local", "")
            eventos_texto.append(f"{i}. {titulo} | Data: {data} | Local: {local}")

        # Usar PromptBuilder para construir prompt estruturado
        prompt = (
            PromptBuilder()
            .add_header(
                "MISSÃƒO CRÃTICA",
                f"Encontrar links ESPECÃFICOS de venda/informaÃ§Ãµes para estes {len(events_batch)} eventos no Rio de Janeiro."
            )
            .add_section("EVENTOS", "\n".join(eventos_texto))
            .add_raw("\nESTRATÃ‰GIA DE BUSCA OBRIGATÃ“RIA (siga esta ordem):\n\nPara CADA evento:")
            .add_numbered_list(
                "",
                [
                    """**PRIORIDADE MÃXIMA - Site Oficial do Venue**:
   - Blue Note Rio â†’ acesse bluenoterio.com e busque na agenda/programaÃ§Ã£o
   - Teatro Municipal â†’ acesse theatromunicipal.rj.gov.br
   - Sala CecÃ­lia Meirelles â†’ acesse salaceliciameireles.com.br
   - Casa do Choro â†’ acesse casadochoro.com.br/agenda
   - Outros venues â†’ busque "[nome venue] agenda programaÃ§Ã£o\"""",
                    """**Plataformas de Ingressos** (use termos EXATOS):
   - Sympla: busque "site:sympla.com.br [titulo evento completo] rio"
   - Ingresso.com: busque "site:ingresso.com [titulo evento completo]"
   - Eventbrite: busque "site:eventbrite.com.br [titulo evento completo]"
   - Bilheteria Digital, Ticket360, Uhuu""",
                    """**Redes Sociais/Instagram** (Ãºltimo recurso):
   - Busque Instagram oficial do venue com link na bio ou stories
   - Posts recentes sobre o evento especÃ­fico"""
                ],
                emoji_prefix=True
            )
            .add_criteria({
                "ACEITE APENAS": [
                    "URLs que levam DIRETAMENTE Ã  pÃ¡gina do evento especÃ­fico",
                    "URLs com ID Ãºnico, slug do evento, ou data na URL",
                    """Exemplos vÃ¡lidos:
     * sympla.com.br/evento/nome-evento-123456
     * bluenoterio.com/shows/artista-data-20250115
     * eventbrite.com.br/e/titulo-evento-tickets-789012"""
                ],
                "REJEITE ABSOLUTAMENTE": [
                    "Homepages: bluenoterio.com, casadochoro.com.br",
                    "PÃ¡ginas de listagem: /agenda, /shows, /eventos, /programacao",
                    "URLs genÃ©ricas sem identificador do evento",
                    "Links de redes sociais (exceto se for o ÃšNICO link disponÃ­vel)"
                ]
            }, title="CRITÃ‰RIOS DE ACEITAÃ‡ÃƒO (seja RIGOROSO)")
            .add_numbered_list(
                "VALIDAÃ‡ÃƒO FINAL:\nAntes de retornar cada link",
                [
                    "Confirme que a URL contÃ©m elemento Ãºnico (ID, nome, data)",
                    "Verifique que nÃ£o Ã© pÃ¡gina genÃ©rica",
                    "Se tiver dÃºvida, retorne null"
                ]
            )
            .add_json_example(
                {
                    "1": "https://url-especifica-evento-1.com/... ou null",
                    "2": "https://url-especifica-evento-2.com/... ou null"
                },
                "FORMATO JSON (sem comentÃ¡rios)"
            )
            .add_raw("âš ï¸ IMPORTANTE: Prefira retornar null do que um link genÃ©rico. Links ruins serÃ£o rejeitados na validaÃ§Ã£o.")
            .build()
        )

        try:
            response = self.search_agent.run(prompt)

            # Usar safe_json_parse para extraÃ§Ã£o consistente
            from utils.json_helpers import safe_json_parse
            links_map = safe_json_parse(
                response.content,
                default={}
            )

            # Converter chaves para int se necessÃ¡rio e validar formato
            result = {}
            for key, value in links_map.items():
                # Validar que o link nÃ£o Ã© genÃ©rico
                if value and value != "null" and isinstance(value, str):
                    # Checar se nÃ£o Ã© link genÃ©rico bÃ¡sico
                    generic_endings = ['/shows/', '/eventos/', '/agenda/', '/programacao/', '/calendar/']
                    is_generic = any(value.rstrip('/').endswith(ending.rstrip('/')) for ending in generic_endings)

                    # TambÃ©m verificar se Ã© apenas homepage (sem path especÃ­fico)
                    from urllib.parse import urlparse
                    parsed = urlparse(value)
                    path = parsed.path.rstrip('/')

                    if is_generic or not path or path == '/':
                        logger.warning(f"   âš ï¸ Link genÃ©rico rejeitado: {value}")
                        result[str(key)] = None
                    else:
                        result[str(key)] = value
                else:
                    result[str(key)] = None

            return result

        except Exception as e:
            logger.error(f"Erro na busca batch de links: {e}")
            return {}

    def _search_missing_links(self, events: list[dict]) -> list[dict]:
        """Busca links para eventos que nÃ£o tÃªm link, processando em batches."""
        # Identificar eventos sem link
        events_without_links = []
        events_indices = []

        for i, event in enumerate(events):
            if not event.get("link_ingresso"):
                events_without_links.append(event)
                events_indices.append(i)

        if not events_without_links:
            logger.info("Todos os eventos jÃ¡ possuem links")
            return events

        logger.info(f"ğŸ”— Buscando links para {len(events_without_links)} eventos sem link...")

        # Processar em batches de 5
        batch_size = 5
        total_found = 0

        for batch_start in range(0, len(events_without_links), batch_size):
            batch_end = min(batch_start + batch_size, len(events_without_links))
            batch = events_without_links[batch_start:batch_end]

            logger.info(f"   Processando batch {batch_start//batch_size + 1} ({len(batch)} eventos)...")

            # Buscar links para este batch
            links_map = self._find_event_ticket_link_batch(batch)

            # Atribuir links encontrados
            for local_idx, event in enumerate(batch):
                batch_key = str(local_idx + 1)
                if batch_key in links_map and links_map[batch_key]:
                    event["link_ingresso"] = links_map[batch_key]
                    event["link_source"] = "busca_complementar_batch"
                    total_found += 1
                    logger.info(f"   âœ“ Link encontrado para: {event.get('titulo')}")

        logger.info(f"âœ“ Busca complementar concluÃ­da: {total_found}/{len(events_without_links)} links encontrados")
        return events

    def _filter_excluded_events(self, events: list[dict], category_name: str = "") -> list[dict]:
        """Filtra eventos que contÃªm palavras de exclusÃ£o no tÃ­tulo ou descriÃ§Ã£o.

        Args:
            events: Lista de eventos para filtrar
            category_name: Nome da categoria/venue (para logging)

        Returns:
            Lista de eventos filtrados (sem eventos que contÃªm keywords de exclusÃ£o)
        """
        from config import EVENT_CATEGORIES, GLOBAL_EXCLUDE_KEYWORDS

        # Iniciar com exclusÃµes GLOBAIS (infantil, LGBTQ+, etc) - aplicadas a TODOS os eventos
        exclude_keywords = list(GLOBAL_EXCLUDE_KEYWORDS)

        # Adicionar exclusÃµes especÃ­ficas de outdoor (shows mainstream) se aplicÃ¡vel
        outdoor_exclude = EVENT_CATEGORIES.get("outdoor", {}).get("exclude", [])
        if outdoor_exclude:
            exclude_keywords.extend(outdoor_exclude)

        if not exclude_keywords:
            return events

        filtered = []
        removed_count = 0

        for event in events:
            titulo = event.get("titulo", "").lower()
            descricao_raw = event.get("descricao", "") or ""  # Handle None values
            descricao = descricao_raw.lower()
            combined_text = f"{titulo} {descricao}"

            # Verificar se contÃ©m alguma palavra de exclusÃ£o
            matched_keyword = None
            for keyword in exclude_keywords:
                if keyword.lower() in combined_text:
                    matched_keyword = keyword
                    break

            if matched_keyword:
                removed_count += 1
                logger.info(f"   âŒ Evento filtrado ({category_name}): '{event.get('titulo')}' [match: '{matched_keyword}']")
            else:
                filtered.append(event)

        if removed_count > 0:
            logger.info(f"âœ“ Filtro de exclusÃ£o aplicado em {category_name}: {removed_count} eventos removidos, {len(filtered)} mantidos")

        return filtered

    def process_with_llm(self, raw_events: dict[str, Any]) -> str:
        """Combina e limpa resultados das duas buscas Perplexity."""
        logger.info("Combinando dados das 2 buscas Perplexity...")

        # Extrair dados das duas buscas
        data_geral = raw_events.get("perplexity_geral", "{}")
        data_especial = raw_events.get("perplexity_especial", "{}")

        # Usar clean_json_response do json_helpers (centralizado)
        from utils.json_helpers import clean_json_response
        data_geral_clean = clean_json_response(data_geral)
        data_especial_clean = clean_json_response(data_especial)

        # Combinar em um Ãºnico JSON
        combined = f'{{"eventos_gerais": {data_geral_clean}, "eventos_locais_especiais": {data_especial_clean}}}'

        logger.info("Dados combinados das 2 buscas")

        # Parsear para aplicar busca complementar de links
        try:
            combined_data = json.loads(combined)

            # Extrair todos os eventos para busca complementar
            all_events = []

            # Eventos gerais
            if "eventos_gerais" in combined_data and "eventos" in combined_data["eventos_gerais"]:
                all_events.extend(combined_data["eventos_gerais"]["eventos"])

            # Eventos de locais especiais
            if "eventos_locais_especiais" in combined_data:
                for local_name, local_events in combined_data["eventos_locais_especiais"].items():
                    if isinstance(local_events, list):
                        all_events.extend([e for e in local_events if isinstance(e, dict)])

            # Aplicar busca complementar de links
            if all_events:
                self._search_missing_links(all_events)

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # APLICAR FILTRO DE EXCLUSÃƒO (remover samba, axÃ©, mainstream)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info("ğŸ” Aplicando filtro de exclusÃ£o...")

            # Filtrar eventos gerais (categorias: Jazz, Teatro-ComÃ©dia, Outdoor-FimDeSemana)
            if "eventos_gerais" in combined_data and "eventos" in combined_data["eventos_gerais"]:
                original_count = len(combined_data["eventos_gerais"]["eventos"])
                combined_data["eventos_gerais"]["eventos"] = self._filter_excluded_events(
                    combined_data["eventos_gerais"]["eventos"],
                    "eventos_gerais"
                )
                final_count = len(combined_data["eventos_gerais"]["eventos"])
                logger.info(f"ğŸ“Š Eventos gerais: {original_count} â†’ {final_count} (removidos: {original_count - final_count})")

            # Filtrar eventos de locais especiais (Casa do Choro, Sala CecÃ­lia, Teatro Municipal, Artemis)
            if "eventos_locais_especiais" in combined_data:
                for local_name, local_events in combined_data["eventos_locais_especiais"].items():
                    if isinstance(local_events, list) and local_events:
                        original_count = len(local_events)
                        combined_data["eventos_locais_especiais"][local_name] = self._filter_excluded_events(
                            local_events,
                            local_name
                        )
                        final_count = len(combined_data["eventos_locais_especiais"][local_name])
                        if original_count != final_count:
                            logger.info(f"ğŸ“Š {local_name}: {original_count} â†’ {final_count} (removidos: {original_count - final_count})")

            logger.info("âœ… Filtro de exclusÃ£o aplicado com sucesso")

            # Retornar JSON atualizado
            return json.dumps(combined_data, ensure_ascii=False, indent=2)

        except json.JSONDecodeError as e:
            logger.error(f"Erro ao parsear JSON combinado: {e}")
            return combined
